# Simple Imputation {#sec-missing-simple}

When dealing with missing data, the first suggestion is often imputation. **Simple imputation** as covered in this chapter refers to the method where for each variable a single value is selected, and all missing values will be replaced by that value. It is important to remind ourselves why we impute missing values. Because the model we are using isn't able to handle missing values natively. Simple imputation is the simplest way to handle missing values.

The way we find this value will depend on the variable type, and within each type of variable we find multiple ways to decide on a value. The main categories are categorical and numeric variables.

For categorical we have 2 main options. We can add replace the value of `NA`  with `"missing"` or some other unused value. This would denote that the value was missing, without trying to pick a value. It would change the cardinality of the predictors as a variables with levels "new", "used", and "old" would go from 3 levels to 4. Which may be undesirable. Especially with ordinal variables as it is unclear where the `"missing"` should go in the ordering.

The simple imputation way to deal with categorical is to find the a value with want to impute with. Typically this will be calculated with the mode, e.i. the most frequent value. It doesn't have to be the most frequent value, you could set up the imputation to pick the 5th most common value, or the least common value. But this is quite uncommon, and the mode is used by far the most. These methods also work for low cardinality integers.

For numeric values we don't have the option to add a new type of value. One option is to manually select the imputed value. If this approach is taken, then it should be done with utmost caution! It would also make it a unlearned imputation method. What is typically done is that some value is picked as the replacement. Be it the median, mean or even mode. 

Datetime variables will be a different story. One could use the mode, or an adjusted mean or median. Another way is to let the value extraction work first, and then apply imputation to the extracted variables. Time series data is different enough that it has its own chapter in @sec-time-series-missing.

One of the main downsides to simple imputation is that it can lead to impossible configurations in the data. Imagine that total square area is missing, but we knows the number of rooms and number of bedrooms. Certain combinations are more likely than others. Below is the classic ames data set

```{r}
library(tidymodels)
ames |>
  count(TotRms_AbvGrd, Bedroom_AbvGr) |>
  ggplot(aes(TotRms_AbvGrd, Bedroom_AbvGr, fill = log(n))) +
  geom_tile() +
  geom_abline(slope = 1, intercept = 0) +
  coord_fixed() +
  theme_minimal() +
  scale_fill_viridis_c()
```

It is not possible for there to be more bedrooms than total number of rooms. And we see that in the data. The average number of bedrooms is `r `round(mean(ames$Bedroom_AbvGr), 2)`, and if we round, then it will be `r round(mean(ames$Bedroom_AbvGr), 0)`. That is perfectly fine for a house with an average number of rooms, but it will be impossible for small houses and quite inadequate for large houses. This is obviously bad, but can be seen as an improvement to the situation where the model didn't fit because a missing value was present. This scenario is part of the motivation to @sec-missing-model.

Other drawbacks of simple include; reducing the variance and standard diviation of the data. This happens because we are adding zero-variance information to the variables. In the same vain, we are changing the distribution of our variables, which can also affect downstream modeling and feature engineering.

Below we see this in effect, as more and more missing data, leads to a larger peak of the mean of the distribution.

```{r}
set.seed(1234)
x <- c(rnorm(1000, mean = 100, sd = 20), rnorm(600, mean = 150, sd = 10))
x <- sample(x)

x_05 <- x
x_05[seq_len(floor(length(x) * 0.05))] <- mean(x)
x_10 <- x
x_10[seq_len(floor(length(x) * 0.10))] <- mean(x)
x_15 <- x
x_15[seq_len(floor(length(x) * 0.15))] <- mean(x)

tibble(
  x = c(x, x_05, x_10, x_15),
  percentage = rep(c("0", "5%", "10%", "15%"), each = 1600)
) |>
  mutate(percentage = factor(percentage, levels = c("0", "5%", "10%", "15%"))) |>
  ggplot(aes(x)) +
  geom_histogram(bins = 50) +
  facet_wrap(~percentage) +
  theme_minimal()
```


TODO: talk about the one case where it isn't a single value, but rather a sampling of the training distribution. 


## Pros and Cons

### Pros

### Cons

- Doesn't preserve relationship between predictors

## R Examples

## Python Examples


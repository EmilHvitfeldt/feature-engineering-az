---
pagetitle: "Feature Engineering A-Z | Percentile Scaling"
---

# Percentile Scaling {#sec-numeric-percentile}

::: {style="visibility: hidden; height: 0px;"}
## Percentile Scaling
:::

**Percentile scaling** (also sometimes called **Rank Scaling** or **quantile scaling**) is a method where we apply a non-linear transformation to our data where each value is the percentile of the training data.

::: callout-note
The words percentile and quantile describe the same things in different ways.
The difference is the representation.
Percentiles are reported as percentages such as 10%, 20%, and 30% and quantiles are reported as decimals such as 0.1, 0.2, and 0.3.
This chapter will use these words interchangeably.
:::

::: callout-caution
# TODO

Add equation
:::

This does a couple of things for us.
It naturally constrains the transformed data into the range $[0, 1]$, and it deals with outlier values nicely in the sense that they don't change the transformation that much.
Moreover, if the testing distribution is close to the training distribution then the transformed distribution would be approximately uniformly distributed between 0 and 1.

::: callout-caution
# TODO

add color-coded pair of distribution, where the color map to the previous location of the data
:::

## Pros and Cons

### Pros

-   Transformation isn't affected much by outliers

### Cons

-   Doesn't allow to exact reverse transformation

-   Isn't ideal if training data doesn't have that many unique values

## R Examples

We will be using the `ames` data set for these examples.

```{r}
#| echo: false
set.seed(1234)
# To avoid changing recipe ID columns
```

```{r}
#| message: false
library(recipes)
library(modeldata)
data("ames")

ames |>
  select(Lot_Area, Wood_Deck_SF, Sale_Price)
```

The {recipes} step to do this transformation is `step_percentile()`.
It defaults to the calculation of 100 percentiles and uses those to transform the data

```{r}
percentile_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>
  step_percentile(Lot_Area) |>
  prep()

percentile_rec |>
  bake(new_data = NULL)
```

We can use the `tidy()` method to pull out what the specific values are for each percentile

```{r}
percentile_rec |>
  tidy(1)
```

You can change the granularity by using the `options` argument.
In this example, we are calculating 500 points evenly spaced between 0 and 1, both inclusive.

```{r}
percentile500_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>
  step_percentile(Lot_Area, options = list(probs = (0:500)/500)) |>
  prep()

percentile500_rec |>
  bake(new_data = NULL)
```

And we can see the more precise numbers.

```{r}
percentile500_rec |>
  tidy(1)
```

Notice how there are only `r nrow(tidy(percentile500_rec, 1))` values in this output.
This is happening because some percentile has been collapsed to save space since if the value for the 10.4 and 10.6 percentile is the same, we just store the 10.6 value.

## Python Examples

```{python}
#| echo: false
import pandas as pd
from sklearn import set_config

set_config(transform_output="pandas")
pd.set_option('display.precision', 3)
```

We are using the `ames` data set for examples.
{sklearn} provided the `QuantileTransformer()` method we can use.
We can use the `n_quantiles` argument to change the number of quantiles to use.

```{python}
from feazdata import ames
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import QuantileTransformer

ct = ColumnTransformer(
    [('Quantile', QuantileTransformer(n_quantiles = 500), ['Lot_Area'])], 
    remainder="passthrough")

ct.fit(ames)
ct.transform(ames)
```
# Binning {#sec-numeric-binning}

**Binning**, is one way to represent a curve that makes it useful in modeling content since it allows us to model non-linear relationships between predictors and outcomes. This is a trained method.

Being able to transform a numeric variable that has a non-linear relationship with the outcome into one or more variables that do have linear relationships with the outcome is of great importance, as many models wouldn't be able to work with these types of variables effectively themselves. Below is a toy example of one such variable

```{r}
#| echo: false
set.seed(1234)
data_toy <- tibble::tibble(
  predictor = rnorm(100) + 1:100
) |>
  dplyr::mutate(outcome = sin(predictor/25) + rnorm(100, sd = 0.1) + 10)
```

```{r}
#| label: fig-binning-predictor-outcome
#| echo: false
#| message: false
#| fig-cap: |
#|   Non-linear relationship between predictor and outcome.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it, but it follows a curve. You would not 
#|   be able to fit a straight line to this data.
library(ggplot2)

data_toy |>
  ggplot(aes(predictor, outcome)) +
  geom_point() +
  theme_minimal()
```

Here we have a non-linear relationship. It is a fairly simple one, the outcome is high when the predictor takes values between 25 and 50, and outside the ranges, it takes over values. Given that this is a toy example, we do not have any expert knowledge regarding what we expect the relationship to be outside this range. The trend could go back up, it could go down or flatten out. We don't know.

Below we see the way each of the new variables activates different areas of the domain. 

```{r}
#| echo: false
#| message: false
library(recipes)

rec_bins <- recipe(outcome ~ predictor, data = data_toy) |>
  step_mutate(predictor1 = predictor) |>
  step_discretize(predictor1, num_breaks = 4) |>
  step_dummy(predictor1, one_hot = TRUE) |>
  prep()

data_bins <- rec_bins |>
  bake(new_data = data_toy) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor1_bin", "Bin ")})
```

```{r}
#| label: fig-binning-curves
#| echo: false
#| message: false
#| fig-cap: |
#|   Binning gives us a series of indicators of different regions
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   Each of the curves is either 0 or 1, with each bin taking up a 4th of the
#|   region where it returns 1 with 0 otherwise.
data_bins |>
  select(-outcome) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name) +
  theme_minimal()
```

What we see in this example is that the regions appear to be evenly spaced over the observed values of the predictor. This is neither and good nor bad thing, but something we need to think about. Ideally, we want the different regions to represent something shared within them. If the cuts are happening at non-ideal locations we don't get the full benefit. See below for such an example 

```{r}
#| label: fig-binning-highlight
#| echo: false
#| message: false
#| fig-cap: |
#|   Each part of the binning detects a part of the data set.
#| fig-alt: |
#|   Facetted scatter chart. Predictor along the x-axis, outcome along the 
#|   y-axis. Each of the facets shows the same non-linear relationship between
#|   predictor and outcome. Color is used to show how each bin highlights a 
#|   different part of the predictor. The highlight goes further to the right 
#|   for each facet.
data_bins |>
  tidyr::pivot_longer(cols = -c(outcome, predictor)) |>
  ggplot(aes(predictor, outcome, color = value)) +
  geom_point() +
  facet_wrap(~name) +
  scale_color_gradient(high = "dark blue", low = "white") +
  theme_minimal()
```

Ideally, we would want the region with the highest values of `outcome` to be in the same region. but It appears to be split between Bin 2 and Bin 3.

Below is a chart of how well using evenly split binning works when using it on our toy example. We notice that using binning doesn't work that well for this type of data. And it makes sense, binning will naturally create a step function like we see below, and if your predictor doesn't have that shape, you are unlikely to see as good performance as with splines @sec-numeric-splines or polynomial expansion @sec-numeric-polynomial.

```{r}
#| label: fig-binning-different-num_breaks
#| echo: false
#| message: false
#| fig-cap: |
#|   Increasing the number of bins makes for a better fit.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it, but it follows a curve. You would not 
#|   be able to fit a straight line to this data.
library(tidymodels)

num_breaks <- c(3, 5, 7, 9)

map(
  num_breaks,
  \(x) {
    workflow(
      recipe(outcome ~ predictor, data = data_toy) |> 
        step_discretize(predictor, num_breaks = x),
      linear_reg()
    ) |>
      fit(data = data_toy) |>
      augment(new_data = arrange(data_toy, predictor))
  }
) |>
  list_rbind(names_to = "degree") |>
  mutate(degree = num_breaks[degree]) |>
  mutate(degree = as.factor(degree)) |>
  ggplot(aes(predictor, .pred)) +
  geom_point(aes(predictor, outcome), data = data_toy) +
  geom_step(aes(color = degree, group = degree)) +
  theme_minimal() +
  scale_color_viridis_d(begin = 0.15, end = 0.85) +
  labs(y = "outcome", color = "deg_free")
```

```{r}
#| echo: false
data_toy_discont <- data_toy |>
  mutate(outcome = (floor(predictor/40) %% 2) + outcome + predictor / 25) |>
  filter(predictor > 0, predictor < 100)
```

Imagine we have a different data set, where the relationship between predictor and outcomes isn't as continuous, but instead has some breaks in it.

```{r}
#| label: fig-discont-binning-different-num_breaks
#| echo: false
#| message: false
#| fig-cap: |
#|   The evenly spaced cuts do not perform well when the breaks in the data
#|   set aren't evenly spaced.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it with two big breaks in the curve. Both 
#|   of the fitted lines doesn't match the data, as the breaks don't align
#|   with the data set.
library(tidymodels)

num_breaks <- c(3, 4)

map(
  num_breaks,
  \(x) {
    workflow(
      recipe(outcome ~ predictor, data = data_toy_discont) |> 
        step_discretize(predictor, num_breaks = x),
      linear_reg()
    ) |>
      fit(data = data_toy_discont) |>
      augment(new_data = arrange(data_toy_discont, predictor))
  }
) |>
  list_rbind(names_to = "degree") |>
  mutate(degree = num_breaks[degree]) |>
  mutate(degree = as.factor(degree)) |>
  ggplot(aes(predictor, .pred)) +
  geom_point(aes(predictor, outcome), data = data_toy_discont) +
  geom_step(aes(color = degree, group = degree)) +
  theme_minimal() +
  scale_color_viridis_d(begin = 0.15, end = 0.85) +
  labs(y = "outcome", color = "deg_free")
```

The evenly spaced cut points don't give us good performance, since they didn't happen to align with the breaks in the data. What we can do instead is to use a more supervised method of finding these splits. Such as using a CART model of xgboost model, to find the optimal breaks in the data. This would make the method supervised and outcome-dependent. Below is an example where we use a CARt model.

```{r}
#| label: fig-discont-binning-different-num_breaks-cart
#| echo: false
#| message: false
#| fig-cap: |
#|   Cuts are played using a fitted CART model. The y are played exactly at the
#|   right places.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it with two big breaks in the curve. The
#|   functions now correctly split the data at the breaks in the data.
library(tidymodels)
library(embed)

num_breaks <- c(1, 2, 3)

map(
  num_breaks,
  \(x) {
    workflow(
      recipe(outcome ~ predictor, data = data_toy_discont) |> 
        step_discretize_cart(predictor, outcome = "outcome", tree_depth = x),
      linear_reg()
    ) |>
      fit(data = data_toy_discont) |>
      augment(new_data = arrange(data_toy_discont, predictor))
  }
) |>
  list_rbind(names_to = "degree") |>
  mutate(degree = num_breaks[degree]) |>
  mutate(degree = as.factor(degree)) |>
  ggplot(aes(predictor, .pred)) +
  geom_point(aes(predictor, outcome), data = data_toy_discont) +
  geom_step(aes(color = degree, group = degree)) +
  theme_minimal() +
  scale_color_viridis_d(begin = 0.15, end = 0.85) +
  labs(y = "outcome", color = "deg_free")
```

While these results look promising, there are a couple of things that are worth remembering. Firstly, this is not a silver bullet, as we still need to tune the method, to find the optimal value for the model we are fitting. Secondly, this type of data is not seen that much in the wild, and if you are seeing them in your application, there is a good chance that you can manually encode this information without having to resort to these methods. 

## Pros and Cons

### Pros

- Works fast computationally
- Behaves predictably outside the range of the predictors
- If cuts are placed well, it can handle sudden changes in distributions
- Interpretable
- doesn't create correlated features

### Cons

- The inherent rounding that happens, can lead to loss of performance and interpretations
- arguably less interpretable than binning
- can produce a lot of variables

## R Examples


```{r}
#| echo: false
#| message: false
library(tidymodels)
data("ames")
```

We will be using the `ames` data set for these examples.

```{r}
#| message: false
library(recipes)
library(modeldata)

ames |>
  select(Lot_Area, Year_Built)
```

{recipes} has the function `step_discretize()` for just this occasion.

```{r}
#| warning: false
bin_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_discretize(Lot_Area, Year_Built)

bin_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

If you don't like the default number of breaks created, you can use the `num_breaks = 6` argument to change it.

```{r}
#| warning: false
bin_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_discretize(Lot_Area, Year_Built, num_breaks = 6)

bin_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

This step technically creates a factor variable, but we can turn it into a series of indicator functions with `step_dummy()`

```{r}
#| warning: false
bin_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_discretize(Lot_Area, Year_Built, num_breaks = 6) |>
  step_dummy(Lot_Area, Year_Built, one_hot = TRUE)

bin_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

## Python Examples

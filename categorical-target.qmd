# Target Encoding {#sec-target-encoding}

**Target encoding** (also called **mean encoding**, **likelihood encoding**, or **impact encoding**) is a method that map the categorical levels to probabilities of your target variable. This method is in some ways quite similar to frequency encoding that we saw in @sec-categorical-frequency. We are taking a single categorical variable, and turning it into a single numeric categorical variable.

This is a trained and supervised method since we are using the outcome of our modeling problem to guide the way this method is estimated. In the most simple formulation, target encoding is done by replacing a categorical variable with the mean of the target variable. The target variable will typically be the outcome, but that is not necessarily a requirement.

Consider the following example data set

```{r}
#| echo: false
animals <- tibble::tibble(
  cuteness = c(1, 5, 9, 3, 2, 4),
  animal = c("dog", "cat", "cat", "cat", "dog", "horse")
)

animals
```

If we where to calculate target encoding on `animal` using `cuteness` as the target, we will first need to calculate the mean of `cuteness` within each

```{r}
#| echo: false
#| message: false
library(dplyr)
animals_means <- animals |>
  summarise(
    math = paste(cuteness, collapse = " + "),
    math = if_else(length(cuteness) == 1, 
                   paste0(math, " / 1"), 
                   paste0("(", math, ") / ", length(cuteness))),
    mean = mean(cuteness), 
    .by = animal
  )

animals_means
```

Taking these means and we can now use them as an encoding

```{r}
#| echo: false
animals |>
  left_join(animals_means, by = join_by(animal)) |>
  select(-animal, -math) |>
  rename(animal = mean)
```

From the above example we notice 3 things. Firstly, once the calculations have been done, applying the encoding to new data is a fairly easy procedure as it amounts to left join. Secondly, some classes have different number of observations associated with them. The `"horse"` class only has 1 observation in this data set, how confident are we that the mean calculated from this value is as valid as the mean that was calculated over the 3 values for the `"cat"` class? Lastly, how will this method handle unseen levels?

Let us think about the unseen levels first. If we have no information about a given class. This could happen in two different ways. Because the level is truly unseen, because the company was just started and wasn't known in the training data set. Or because the known level wasn't observed, e.i. no Sundays in the training data set. Regardless of the reason, we will want to give these levels a baseline number. For this we can use the mean value of the target, across all of the training data set. So for our toy example, we have a mean cuteness of `r mean(animals$cuteness)`, which we will assign to any new animal.

This value is by no means a good value, but it is a educated guess that can be calculated with ease. This also means that regardless of the distribution of the target, this values can be calculated.

The way we handle unseen levels gives us a glue as to how we can deal with low frequency counts. Knowing that the global mean of the target is our baseline when we have no information. We can combine the level mean with the global mean, in accordance to how many observations we observe. If we have a lot of observations in a level, we will let the global mean have little influence, and if there are fewer observations we will let the global mean have a higher influence.

```{r}
#| echo: false
library(ggplot2)

set.seed(1234)
n <- 50
data <- tibble(
  size = ceiling(rlnorm(n, 1.5)),
  value = runif(n, 50, 100)
)

weighted_mean <- weighted.mean(data$value, data$value)

range <- range(data$value) 
range <- range + diff(range) * c(-0.1, 0.1)
```

We can visualize this effect in the following charts. First we have an example of what happens with a smaller amount of smoothing. The points mostly like along the diagonal. Remember that if we didn't do this, all the points would be along the diagonal regardless of their size.

```{r}
#| label: fig-target-smoothness-1
#| echo: false
#| message: false
#| fig-cap: |
#|   With a small amount of smoothing, the adjusted means are close to the 
#|   original means, regardless of the number of observations.
#| fig-alt: |
#|   Scatter chart. A green line with slope 1 and intercept, and a blue line 
#|   with slope 0 and a intercept equal to the global mean. The points are sized
#|   according to the number of observations that where used to calculate the 
#|   value. The points lie along the green line mostly, with some of the smaller
#|   points getting closer to the blue line.
smoothness <- 1

data |>
  mutate(new_value = (size * value + weighted_mean * smoothness) / (size + smoothness)) |>
  ggplot(aes(value, new_value)) +
  geom_abline(slope = 0, intercept = weighted_mean, color ="lightblue") +
  geom_abline(slope = 1, intercept = 0, color = "lightgreen") +
  geom_point(aes(size = size), alpha = 0.2) +
  theme_minimal() +
  xlim(range) +
  ylim(range) +
  labs(x = "Original mean value",
       y = "Smoothed mean value",
       size = "# observations",
       title = "Small amount of smoothing")
```

In this next chart we see the effect of a higher amount of smoothing, now the levels with fewer observatiosn are pulled quite a bit closer to the global mean.

```{r}
#| label: fig-target-smoothness-5
#| echo: false
#| message: false
#| fig-cap: |
#|   With a large amount of smoothing, the adjusted means are not as close to
#|   the original means, with smaller points getting quite close to the global
#|   mean.
#| fig-alt: |
#|   Scatter chart. A green line with slope 1 and intercept, and a blue line 
#|   with slope 0 and a intercept equal to the global mean. The points are sized
#|   according to the number of observations that where used to calculate the 
#|   value. The points lie between the green and blue line, with the smaller
#|   points being close to the blue line than the larger points.
smoothness <- 5

data |>
  mutate(new_value = (size * value + weighted_mean * smoothness) / (size + smoothness)) |>
  ggplot(aes(value, new_value)) +
  geom_abline(slope = 0, intercept = weighted_mean, color ="lightblue") +
  geom_abline(slope = 1, intercept = 0, color = "lightgreen") +
  geom_point(aes(size = size), alpha = 0.2) +
  theme_minimal() +
  xlim(range) +
  ylim(range) +
  labs(x = "Original mean value",
       y = "Smoothed mean value",
       size = "# observations",
       title = "Large amount of smoothing")
```

The exact way this is done will very from method to method, and the strength of this smoothing can and should be tuned as there isn't a emperical best way to choose it. 

tmwr also calls this effect encoding, where as other places call sum-encoding effect encoding https://www.linkedin.com/pulse/encode-categorical-features-revanth-yadama/

We need to talk about the subtleties. There is plain mean encoding, and then there is all the variants,

regularization

https://embed.tidymodels.org/reference/step_lencode_glm.html https://embed.tidymodels.org/reference/step_lencode_mixed.html https://embed.tidymodels.org/reference/step_lencode_mixed.html https://contrib.scikit-learn.org/category_encoders/targetencoder.html

https://dl.acm.org/doi/10.1145/507533.507538

https://www.tmwr.org/categorical.html#using-the-outcome-for-encoding-predictors

<https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/target-encoding.html#:~:text=Target%20encoding%20is%20the%20process,convert%20categorical%20columns%20to%20numeric.>

<https://towardsdatascience.com/dealing-with-categorical-variables-by-using-target-encoder-a0f1733a4c69>

## Pros and Cons

### Pros

### Cons

## R Examples

## Python Examples

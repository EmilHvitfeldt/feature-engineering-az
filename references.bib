@article{knuth84,
  author     = {Knuth, Donald E.},
  title      = {Literate Programming},
  year       = {1984},
  issue_date = {May 1984},
  publisher  = {Oxford University Press, Inc.},
  address    = {USA},
  volume     = {27},
  number     = {2},
  issn       = {0010-4620},
  url        = {https://doi.org/10.1093/comjnl/27.2.97},
  doi        = {10.1093/comjnl/27.2.97},
  journal    = {Comput. J.},
  month      = may,
  pages      = {97–111},
  numpages   = {15}
}

@article{micci2001,
  author     = {Micci-Barreca, Daniele},
  title      = {A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems},
  year       = {2001},
  issue_date = {July 2001},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {3},
  number     = {1},
  issn       = {1931-0145},
  url        = {https://doi.org/10.1145/507533.507538},
  doi        = {10.1145/507533.507538},
  abstract   = {Categorical data fields characterized by a large number of distinct values represent a serious challenge for many classification and regression algorithms that require numerical inputs. On the other hand, these types of data fields are quite common in real-world data mining applications and often contain potentially relevant information that is difficult to represent for modeling purposes.This paper presents a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.While the statistical methods discussed in this paper were first introduced in the mid 1950's, the use of these methods as a preprocessing step for complex models, like neural networks, has not been previously discussed in any literature.},
  journal    = {SIGKDD Explor. Newsl.},
  month      = {jul},
  pages      = {27–32},
  numpages   = {6},
  keywords   = {categorical attributes, empirical bayes, neural networks, hierarchical attributes, predictive models}
}

@book{geron2017hands-on,
  added-at  = {2018-04-06T05:58:31.000+0200},
  address   = {Sebastopol, CA},
  author    = {Géron, Aurélien},
  biburl    = {https://www.bibsonomy.org/bibtex/2a91270a3a516f4edaa5d459c40317fcc/achakraborty},
  interhash = {e2bd4a803c6cba6cca1d926b393806ad},
  intrahash = {a91270a3a516f4edaa5d459c40317fcc},
  isbn      = {978-1491962299},
  keywords  = {2017 book machine-learning oreilly tensorflow textbook},
  publisher = {O'Reilly Media},
  timestamp = {2018-04-06T05:59:31.000+0200},
  title     = {Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems},
  year      = 2017
}

@book{kuhn2022tidy,
  title     = {Tidy Modeling with R},
  author    = {Kuhn, M. and Silge, J.},
  isbn      = {9781492096450},
  url       = {https://books.google.com/books?id=98J6EAAAQBAJ},
  year      = {2022},
  publisher = {O'Reilly Media}
}

@book{thakur2020approaching,
  title     = {Approaching (Almost) Any Machine Learning Problem},
  author    = {Thakur, A.},
  isbn      = {9788269211504},
  url       = {https://books.google.com/books?id=ZbgAEAAAQBAJ},
  year      = {2020},
  publisher = {Amazon Digital Services LLC - Kdp}
}

@book{kuhn2013applied,
  title     = {Applied Predictive Modeling},
  author    = {Kuhn, M. and Johnson, K.},
  isbn      = {9781461468493},
  series    = {SpringerLink : B{\"u}cher},
  url       = {https://books.google.com/books?id=xYRDAAAAQBAJ},
  year      = {2013},
  publisher = {Springer New York}
}

@book{galli2020python,
  title     = {Python Feature Engineering Cookbook: Over 70 recipes for creating, engineering, and transforming features to build machine learning models},
  author    = {Galli, S.},
  isbn      = {9781789807820},
  url       = {https://books.google.com/books?id=2c_LDwAAQBAJ},
  year      = {2020},
  publisher = {Packt Publishing}
}

@book{ozdemir2022feature,
  title     = {Feature Engineering Bookcamp},
  author    = {Ozdemir, S.},
  isbn      = {9781617299797},
  url       = {https://books.google.com/books?id=3n6HEAAAQBAJ},
  year      = {2022},
  publisher = {Manning}
}

@book{kuhn2019feature,
  title     = {Feature Engineering and Selection: A Practical Approach for Predictive Models},
  author    = {Kuhn, M. and Johnson, K.},
  isbn      = {9781351609463},
  series    = {Chapman \& Hall/CRC Data Science Series},
  url       = {https://books.google.com/books?id=q5alDwAAQBAJ},
  year      = {2019},
  publisher = {CRC Press}
}

@article{rubin1976,
  author   = {RUBIN, DONALD B.},
  title    = {{Inference and missing data}},
  journal  = {Biometrika},
  volume   = {63},
  number   = {3},
  pages    = {581-592},
  year     = {1976},
  month    = {12},
  abstract = {{When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random’ and the observed data are ‘observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct’ from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.}},
  issn     = {0006-3444},
  doi      = {10.1093/biomet/63.3.581},
  url      = {https://doi.org/10.1093/biomet/63.3.581},
  eprint   = {https://academic.oup.com/biomet/article-pdf/63/3/581/756166/63-3-581.pdf}
}

@book{van2012flexible,
  title     = {Flexible Imputation of Missing Data},
  author    = {van Buuren, S.},
  isbn      = {9781439868256},
  series    = {Chapman \& Hall/CRC Interdisciplinary Statistics},
  url       = {https://books.google.com/books?id=elDNBQAAQBAJ},
  year      = {2012},
  publisher = {CRC Press}
}

@article{Porter80,
  author  = {Porter, Martin F},
  journal = {Program},
  number  = 3,
  pages   = {130-137},
  title   = {An algorithm for suffix stripping.},
  url     = {https://doi.org/10.1108/eb046814},
  doi     = {10.1108/eb046814},
  volume  = 14,
  year    = 1980
}

@article{Honnibal2020,
  author = {Honnibal, Matthew and Montani, Ines and Van Landeghem, Sofie and Boyd, Adriane},
  doi    = {10.5281/zenodo.1212303},
  title  = {{spaCy: Industrial-strength Natural Language Processing in Python}},
  year   = {2020}
}

@article{luhn1960,
  author   = {Luhn, H. P.},
  title    = {Key word-in-context index for technical literature (kwic index)},
  journal  = {American Documentation},
  volume   = {11},
  number   = {4},
  pages    = {288-295},
  doi      = {https://doi.org/10.1002/asi.5090110403},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.5090110403},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.5090110403},
  abstract = {Abstract A distinction is made between bibliographical indexes for new and past literature based on the willingness of the user to trade perfection for currency. Indexes giving keywords in their context are proposed as suitable for disseminating new information. These can be entirely machine-generated and hence kept up-to-date with the current literature. A compatible coding scheme to identify the indexed documents is also proposed. In it elements are automatically extracted from the usual identifiers of the document so that the coded identifier yields a maximum of information while remaining susceptible to normal methods of ordering.},
  year     = {1960}
}

@article{Lewis2004,
  author  = {Lewis, David D. and Yang, Yiming and Rose, Tony G. and Li, Fan},
  title   = {{RCV1}: A New Benchmark Collection for Text Categorization Research},
  journal = {Journal of Machine Learning Research},
  volume  = {5},
  year    = {2004},
  issn    = {1532-4435},
  pages   = {361--397},
  url     = {https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf}
}

@misc{porter2001snowball,
  title  = {Snowball: A language for stemming algorithms},
  author = {Porter, Martin F},
  year   = {2001},
  url    = {https://snowballstem.org}
}

@inproceedings{nothman2018,
  title     = {Stop Word Lists in Free Open-source Software Packages},
  author    = {Nothman, Joel and Qin, Hanmin and Yurchak, Roman},
  editor    = {Park, Eunjeong L. and Hagiwara, Masato and Milajevs, Dmitrijs and Tan, Liling},
  booktitle = {Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W18-2502},
  doi       = {10.18653/v1/W18-2502},
  pages     = {7--12},
  abstract  = {Open-source software packages for language processing often include stop word lists. Users may apply them without awareness of their surprising omissions (e.g. {``}hasn{'}t{''} but not {``}hadn{'}t{''}) and inclusions ({``}computer{''}), or their incompatibility with a particular tokenizer. Motivated by issues raised about the Scikit-learn stop list, we investigate variation among and consistency within 52 popular English-language stop lists, and propose strategies for mitigating these issues.}
}

@article{spark1972,
  author  = {SPARCK JONES, K},
  title   = {A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL},
  journal = {Journal of Documentation},
  volume  = {28},
  number  = {1},
  pages   = {11-21},
  year    = {1972},
  doi     = {https://doi.org/10.1108/eb026526}
}

@article{robertson2004,
  title     = {Understanding inverse document frequency: on theoretical arguments for IDF},
  author    = {Robertson, Stephen},
  journal   = {Journal of documentation},
  volume    = {60},
  number    = {5},
  pages     = {503--520},
  year      = {2004},
  publisher = {Emerald Group Publishing Limited}
}

@article{lda2003,
  author     = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  title      = {Latent dirichlet allocation},
  year       = {2003},
  issue_date = {3/1/2003},
  publisher  = {JMLR.org},
  volume     = {3},
  number     = {null},
  issn       = {1532-4435},
  abstract   = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  journal    = {J. Mach. Learn. Res.},
  month      = {mar},
  pages      = {993–1022},
  numpages   = {30}
}

@misc{chen2016warpldacacheefficiento1,
  title         = {WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation},
  author        = {Jianfei Chen and Kaiwei Li and Jun Zhu and Wenguang Chen},
  year          = {2016},
  eprint        = {1510.08628},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1510.08628}
}

@inproceedings{devlin-etal-2019-bert,
  title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {Devlin, Jacob  and
               Chang, Ming-Wei  and
               Lee, Kenton  and
               Toutanova, Kristina},
  editor    = {Burstein, Jill  and
               Doran, Christy  and
               Solorio, Thamar},
  booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = jun,
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/N19-1423},
  doi       = {10.18653/v1/N19-1423},
  pages     = {4171--4186},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}
}

@misc{lan2020albertlitebertselfsupervised,
  title         = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author        = {Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  year          = {2020},
  eprint        = {1909.11942},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1909.11942}
}

@misc{sanh2020distilbertdistilledversionbert,
  title         = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author        = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  year          = {2020},
  eprint        = {1910.01108},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1910.01108}
}

@misc{liu2019robertarobustlyoptimizedbert,
  title         = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author        = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year          = {2019},
  eprint        = {1907.11692},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1907.11692}
}

@misc{beltagy2019scibertpretrainedlanguagemodel,
  title         = {SciBERT: A Pretrained Language Model for Scientific Text},
  author        = {Iz Beltagy and Kyle Lo and Arman Cohan},
  year          = {2019},
  eprint        = {1903.10676},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1903.10676}
}

@misc{araci2019finbertfinancialsentimentanalysis,
  title         = {FinBERT: Financial Sentiment Analysis with Pre-trained Language Models},
  author        = {Dogu Araci},
  year          = {2019},
  eprint        = {1908.10063},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1908.10063}
}

@article{Lee_2019,
  title     = {BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  volume    = {36},
  issn      = {1367-4811},
  url       = {http://dx.doi.org/10.1093/bioinformatics/btz682},
  doi       = {10.1093/bioinformatics/btz682},
  number    = {4},
  journal   = {Bioinformatics},
  publisher = {Oxford University Press (OUP)},
  author    = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  editor    = {Wren, Jonathan},
  year      = {2019},
  month     = sep,
  pages     = {1234–1240}
}


@misc{huang2020clinicalbertmodelingclinicalnotes,
  title         = {ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission},
  author        = {Kexin Huang and Jaan Altosaar and Rajesh Ranganath},
  year          = {2020},
  eprint        = {1904.05342},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1904.05342}
}

@misc{lee2019patentbertpatentclassificationfinetuning,
  title         = {PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model},
  author        = {Jieh-Sheng Lee and Jieh Hsiang},
  year          = {2019},
  eprint        = {1906.02124},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1906.02124}
}

@misc{canete2023spanishpretrainedbertmodel,
  title         = {Spanish Pre-trained BERT Model and Evaluation Data},
  author        = {José Cañete and Gabriel Chaperon and Rodrigo Fuentes and Jou-Hui Ho and Hojin Kang and Jorge Pérez},
  year          = {2023},
  eprint        = {2308.02976},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2308.02976}
}

@misc{mikolov2013efficientestimationwordrepresentations,
  title         = {Efficient Estimation of Word Representations in Vector Space},
  author        = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  year          = {2013},
  eprint        = {1301.3781},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1301.3781}
}

@misc{le2014distributedrepresentationssentencesdocuments,
  title         = {Distributed Representations of Sentences and Documents},
  author        = {Quoc V. Le and Tomas Mikolov},
  year          = {2014},
  eprint        = {1405.4053},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1405.4053}
}

@misc{angelov2020top2vecdistributedrepresentationstopics,
  title         = {Top2Vec: Distributed Representations of Topics},
  author        = {Dimo Angelov},
  year          = {2020},
  eprint        = {2008.09470},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2008.09470}
}

@article{10.1371/journal.pone.0141287,
  doi       = {10.1371/journal.pone.0141287},
  author    = {Asgari, Ehsaneddin AND Mofrad, Mohammad R. K.},
  journal   = {PLOS ONE},
  publisher = {Public Library of Science},
  title     = {Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics},
  year      = {2015},
  month     = {11},
  volume    = {10},
  url       = {https://doi.org/10.1371/journal.pone.0141287},
  pages     = {1-15},
  abstract  = {We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93%±0.06% is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0% accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined. Importantly, this model needs to be trained only once and can then be applied to extract a comprehensive set of information regarding proteins of interest. Moreover, this representation can be considered as pre-training for various applications of deep learning in bioinformatics. The related data is available at Life Language Processing Website: http://llp.berkeley.edu and Harvard Dataverse: http://dx.doi.org/10.7910/DVN/JMFHTN.},
  number    = {11}
}

@misc{ng2017dna2vecconsistentvectorrepresentations,
  title         = {dna2vec: Consistent vector representations of variable-length k-mers},
  author        = {Patrick Ng},
  year          = {2017},
  eprint        = {1701.06279},
  archiveprefix = {arXiv},
  primaryclass  = {q-bio.QM},
  url           = {https://arxiv.org/abs/1701.06279}
}

@article{Pargent2022,
  author   = {Pargent, Florian
              and Pfisterer, Florian
              and Thomas, Janek
              and Bischl, Bernd},
  title    = {Regularized target encoding outperforms traditional methods in supervised machine learning with high cardinality features},
  journal  = {Computational Statistics},
  year     = {2022},
  month    = {Nov},
  day      = {01},
  volume   = {37},
  number   = {5},
  pages    = {2671-2692},
  abstract = {Since most machine learning (ML) algorithms are designed for numerical inputs, efficiently encoding categorical variables is a crucial aspect in data analysis. A common problem are high cardinality features, i.e. unordered categorical predictor variables with a high number of levels. We study techniques that yield numeric representations of categorical variables which can then be used in subsequent ML applications. We focus on the impact of these techniques on a subsequent algorithm's predictive performance, and---if possible---derive best practices on when to use which technique. We conducted a large-scale benchmark experiment, where we compared different encoding strategies together with five ML algorithms (lasso, random forest, gradient boosting, k-nearest neighbors, support vector machine) using datasets from regression, binary- and multiclass--classification settings. In our study, regularized versions of target encoding (i.e. using target predictions based on the feature levels in the training set as a new numerical feature) consistently provided the best results. Traditionally widely used encodings that make unreasonable assumptions to map levels to integers (e.g. integer encoding) or to reduce the number of levels (possibly based on target information, e.g. leaf encoding) before creating binary indicator variables (one-hot or dummy encoding) were not as effective in comparison.},
  issn     = {1613-9658},
  doi      = {10.1007/s00180-022-01207-6},
  url      = {https://doi.org/10.1007/s00180-022-01207-6}
}

@misc{prokhorenkova2019catboostunbiasedboostingcategorical,
  title         = {CatBoost: unbiased boosting with categorical features},
  author        = {Liudmila Prokhorenkova and Gleb Gusev and Aleksandr Vorobev and Anna Veronika Dorogush and Andrey Gulin},
  year          = {2019},
  eprint        = {1706.09516},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1706.09516}
}

@inproceedings{quantile2021,
  author    = {Mougan, Carlos and Masip, David and Nin, Jordi and Pujol, Oriol},
  editor    = {Torra, Vicen{\c{c}}
               and Narukawa, Yasuo},
  title     = {Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems},
  booktitle = {Modeling Decisions for Artificial Intelligence},
  year      = {2021},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {168--180},
  isbn      = {978-3-030-85529-1}
}

@article{sparsePCA2006,
  issn      = {10618600},
  url       = {http://www.jstor.org/stable/27594179},
  abstract  = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.},
  author    = {Hui Zou and Trevor Hastie and Robert Tibshirani},
  journal   = {Journal of Computational and Graphical Statistics},
  number    = {2},
  pages     = {265--286},
  publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
  title     = {Sparse Principal Component Analysis},
  urldate   = {2025-06-16},
  volume    = {15},
  year      = {2006}
}

@article{robustpca2011,
  author     = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  title      = {Robust principal component analysis?},
  year       = {2011},
  issue_date = {May 2011},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {58},
  number     = {3},
  issn       = {0004-5411},
  url        = {https://doi.org/10.1145/1970392.1970395},
  doi        = {10.1145/1970392.1970395},
  abstract   = {This article is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the ℓ1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
  journal    = {J. ACM},
  month      = jun,
  articleno  = {11},
  numpages   = {37},
  keywords   = {video surveillance, sparsity, robustness vis-a-vis outliers, nuclear-norm minimization, low-rank matrices, duality, Principal components, ℓ1-norm minimization}
}

@inproceedings{kernalpca1998,
  author    = {Mika, Sebastian and Sch\"{o}lkopf, Bernhard and Smola, Alex and M\"{u}ller, Klaus-Robert and Scholz, Matthias and R\"{a}tsch, Gunnar},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {M. Kearns and S. Solla and D. Cohn},
  pages     = {},
  publisher = {MIT Press},
  title     = {Kernel PCA and De-Noising in Feature Spaces},
  url       = {https://proceedings.neurips.cc/paper_files/paper/1998/file/226d1f15ecd35f784d2a20c3ecf56d7f-Paper.pdf},
  volume    = {11},
  year      = {1998}
}

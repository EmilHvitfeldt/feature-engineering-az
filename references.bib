@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}

@article{micci2001,
author = {Micci-Barreca, Daniele},
title = {A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems},
year = {2001},
issue_date = {July 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1931-0145},
url = {https://doi.org/10.1145/507533.507538},
doi = {10.1145/507533.507538},
abstract = {Categorical data fields characterized by a large number of distinct values represent a serious challenge for many classification and regression algorithms that require numerical inputs. On the other hand, these types of data fields are quite common in real-world data mining applications and often contain potentially relevant information that is difficult to represent for modeling purposes.This paper presents a simple preprocessing scheme for high-cardinality categorical data that allows this class of attributes to be used in predictive models such as neural networks, linear and logistic regression. The proposed method is based on a well-established statistical method (empirical Bayes) that is straightforward to implement as an in-database procedure. Furthermore, for categorical attributes with an inherent hierarchical structure, like ZIP codes, the preprocessing scheme can directly leverage the hierarchy by blending statistics at the various levels of aggregation.While the statistical methods discussed in this paper were first introduced in the mid 1950's, the use of these methods as a preprocessing step for complex models, like neural networks, has not been previously discussed in any literature.},
journal = {SIGKDD Explor. Newsl.},
month = {jul},
pages = {27–32},
numpages = {6},
keywords = {categorical attributes, empirical bayes, neural networks, hierarchical attributes, predictive models}
}

@book{geron2017hands-on,
  added-at = {2018-04-06T05:58:31.000+0200},
  address = {Sebastopol, CA},
  author = {Géron, Aurélien},
  biburl = {https://www.bibsonomy.org/bibtex/2a91270a3a516f4edaa5d459c40317fcc/achakraborty},
  interhash = {e2bd4a803c6cba6cca1d926b393806ad},
  intrahash = {a91270a3a516f4edaa5d459c40317fcc},
  isbn = {978-1491962299},
  keywords = {2017 book machine-learning oreilly tensorflow textbook},
  publisher = {O'Reilly Media},
  timestamp = {2018-04-06T05:59:31.000+0200},
  title = {Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems},
  year = 2017
}

@book{kuhn2022tidy,
  title={Tidy Modeling with R},
  author={Kuhn, M. and Silge, J.},
  isbn={9781492096450},
  url={https://books.google.com/books?id=98J6EAAAQBAJ},
  year={2022},
  publisher={O'Reilly Media}
}

@book{thakur2020approaching,
  title={Approaching (Almost) Any Machine Learning Problem},
  author={Thakur, A.},
  isbn={9788269211504},
  url={https://books.google.com/books?id=ZbgAEAAAQBAJ},
  year={2020},
  publisher={Amazon Digital Services LLC - Kdp}
}

@book{kuhn2013applied,
  title={Applied Predictive Modeling},
  author={Kuhn, M. and Johnson, K.},
  isbn={9781461468493},
  series={SpringerLink : B{\"u}cher},
  url={https://books.google.com/books?id=xYRDAAAAQBAJ},
  year={2013},
  publisher={Springer New York}
}

@book{galli2020python,
  title={Python Feature Engineering Cookbook: Over 70 recipes for creating, engineering, and transforming features to build machine learning models},
  author={Galli, S.},
  isbn={9781789807820},
  url={https://books.google.com/books?id=2c_LDwAAQBAJ},
  year={2020},
  publisher={Packt Publishing}
}

@book{ozdemir2022feature,
  title={Feature Engineering Bookcamp},
  author={Ozdemir, S.},
  isbn={9781617299797},
  url={https://books.google.com/books?id=3n6HEAAAQBAJ},
  year={2022},
  publisher={Manning}
}

@book{kuhn2019feature,
  title={Feature Engineering and Selection: A Practical Approach for Predictive Models},
  author={Kuhn, M. and Johnson, K.},
  isbn={9781351609463},
  series={Chapman \& Hall/CRC Data Science Series},
  url={https://books.google.com/books?id=q5alDwAAQBAJ},
  year={2019},
  publisher={CRC Press}
}

@article{rubin1976,
    author = {RUBIN, DONALD B.},
    title = "{Inference and missing data}",
    journal = {Biometrika},
    volume = {63},
    number = {3},
    pages = {581-592},
    year = {1976},
    month = {12},
    abstract = "{When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are ‘missing at random’ and the observed data are ‘observed at random’, but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is ‘distinct’ from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/63.3.581},
    url = {https://doi.org/10.1093/biomet/63.3.581},
    eprint = {https://academic.oup.com/biomet/article-pdf/63/3/581/756166/63-3-581.pdf},
}




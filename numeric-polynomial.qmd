---
pagetitle: "Feature Engineering A-Z | Polynomial Expansion"
---

# Polynomial Expansion {#sec-numeric-polynomial}

::: {style="visibility: hidden; height: 0px;"}
## Polynomial Expansion
:::

**Polynomial expansion**, is one way to turn a numeric variable into something that can represent a non-linear relationship between two variables.
This is useful in modeling content since it allows us to model non-linear relationships between predictors and outcomes.
This is a trained method.

Being able to transform a numeric variable that has a non-linear relationship with the outcome into one or more variables that do have linear relationships with the outcome is of great importance, as many models wouldn't be able to work with these types of variables effectively themselves.
Below is a toy example of one such variable

```{r}
#| label: data_toy
#| echo: false
#| message: false
library(recipes)
set.seed(1234)
data_toy <- tibble::tibble(
  predictor = rnorm(100) + 1:100
) |>
  dplyr::mutate(outcome = sin(predictor/25) + rnorm(100, sd = 0.1) + 10)
```

```{r}
#| label: fig-polynomial-predictor-outcome
#| echo: false
#| message: false
#| fig-cap: |
#|   Non-linear relationship between predictor and outcome.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it, but it follows a curve. You would not 
#|   be able to fit a straight line to this data.
library(ggplot2)

data_toy |>
  ggplot(aes(predictor, outcome)) +
  geom_point() +
  theme_minimal()
```

Here we have a non-linear relationship.
It is a fairly simple one, the outcome is high when the predictor takes values between 25 and 50, and outside the ranges, it takes over values.
Given that this is a toy example, we do not have any expert knowledge regarding what we expect the relationship to be outside this range.
The trend could go back up, it could go down or flatten out.
We don't know.

As we saw in @sec-numeric-binning, one way to deal with this non-linearity is to chop up the predictor and emit indicators for which region the values take.
While this works, we are losing quite a lot of detail by the rounding that occurs.

We know that we can fit a polynomial function to some data set.
And it would take the following format

$$
\text{poly}(x,\ \text{degree} = n) = a_0 + a_1 x + a_2 x ^ 2 + \cdots + a_n x ^ n
$$

This can then be used to generate features.
Each feature is done by taking the value to a given degree and multiplying it according to the corresponding coefficient.

```{r}
#| label: tbl-poly-raw-values
#| tbl-cap: |
#|   Polynomial values for different values of the predictor, for constants 
#|   equal to 1.
tibble::tibble(
  x = 1:5,
  `x^2` = (1:5) ^ 2,
  `x^3` = (1:5) ^ 3,
  `x^4` = (1:5) ^ 4
) |>
  knitr::kable()
```

In the above table, we see a small example of how this could be done, using a 4th-degree polynomial with coefficients of 1.
If we were to look at the individual functions over the domain of our data we see the following

```{r}
#| label: fig-poly-poly-raw-curves
#| echo: false
#| message: false
#| fig-cap: |
#|   Each part of the spline detects a part of the data set.
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   Each of the curves starts at 0, goes smoothly, and then down to zero.
#|   The highpoint for each curve goes further to the right for each curve 
#|   shown.
recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = TRUE, degree = 6, 
            options = list(raw = TRUE)) |>
  prep() |>
  bake(new_data = data_toy) |>
  dplyr::rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  dplyr::select(-outcome) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  theme_minimal()
```

This is to be expected, but we are noticing that these curves look quite similar, and the values these functions are taking very quickly escalate.
And this is for predictor values between 0 and 100, higher values will get even higher, possibly causing overflow issues.

We also have an issue where the values appear quite correlated since the functions are all increasing.
As we see below, the correlation between the variables is close to 1 for all pairs.

```{r}
#| label: fig-poly-raw-correlation
#| echo: false
#| message: false
#| fig-cap: |
#|   Almost complete correlation is found between variables
#| fig-alt: |
#|   Correlation chart. The polynomial features are lined up one after
#|   another. almost all the correlations between the pairs are close to 1.
recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = FALSE, degree = 6, 
            options = list(raw = TRUE)) |>
  prep() |>
  bake(new_data = data_toy) |>
  dplyr::rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  dplyr::select(-outcome) |>
  corrr::correlate(quiet = TRUE) |>
  autoplot(method = "identity")
```

This is a problem that needs to be dealt with.
The way we can deal with this is by calculating **orthogonal polynomials** instead.
We have that any set polynomial function can be rewritten as a set of orthogonal polynomial functions.

::: callout-caution
# TODO

Add more bath background here
:::

With this, we deal with the two problems we had before.
As seen in the figure below, the functions take smaller values within their ranges

```{r}
#| label: data_poly
#| echo: false
#| message: false
library(recipes)

rec_poly <- recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = TRUE, degree = 6) |>
  prep()

data_poly <- rec_poly |>
  bake(new_data = data_toy) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")})
```

```{r}
#| label: fig-poly-poly-curves
#| echo: false
#| message: false
#| fig-cap: |
#|   Each part of the spline detects a part of the data set.
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   All the curves are between the ranges of -4 and 4.
data_poly |>
  select(-outcome) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name) +
  theme_minimal()
```

And since they are orthogonal by design, we won't have to worry about correlated features.

```{r}
#| label: fig-poly-otho-correlation
#| echo: false
#| message: false
#| fig-cap: |
#|   No correlation to be found
#| fig-alt: |
#|   Correlation chart. The polynomial features are lined up one after another. 
#|   No correlation is found between any of the pairs.
recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = FALSE, degree = 6, 
            options = list(raw = FALSE)) |>
  prep() |>
  bake(new_data = data_toy) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  select(-outcome) |>
  corrr::correlate(quiet = TRUE) |>
  autoplot(method = "identity")
```

The interpretation of these polynomial features is not as easy as with binning in @sec-numeric-binning or splines @sec-numeric-splines, but the calculations are quite fast and versatile.

```{r}
#| label: fig-poly-poly-curves-extrapolate
#| echo: false
#| message: false
#| fig-cap: |
#|   Polynomial features don't handle extrapolation well and values outside
#|   the normal ranges can explode quite fast for higher degree polynomials.
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   Each of the curves has their endpoints go towards infinite or minus 
#|   infinite depending on their degree.
rec_poly |>
  bake(new_data = tibble::tibble(predictor = seq(-500, 500))) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  theme_minimal()
```

Below is a chart of how well using polynomial expansion works when using it on our toy example.
Since the data isn't that complicated, having a `degree` larger than 1 will do the trick.

```{r}
#| label: fig-poly-different-degrees
#| echo: false
#| message: false
#| fig-cap: |
#|   A first degree polynomial isn't enough to fit this data, but second and 
#|   higher does a good job.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it, but it follows a curve. You would not 
#|   be able to fit a straight line to this data. 4 polynomial curves are 
#|   plotted to fit the data. The first degree polynomial doesn't fit the data,
#|   the other ones do.
library(tidymodels)

map(
  1:4,
  \(x) {
    workflow(
      recipe(outcome ~ predictor, data = data_toy) |> 
        step_poly(predictor, degree = x),
      linear_reg()
    ) |>
      fit(data = data_toy) |>
      augment(new_data = arrange(data_toy, predictor))
  }
) |>
  list_rbind(names_to = "degree") |>
  mutate(degree = as.factor(degree)) |>
  ggplot(aes(predictor, .pred)) +
  geom_point(aes(predictor, outcome), data = data_toy) +
  geom_line(aes(color = degree, group = degree)) +
  theme_minimal() +
  scale_color_viridis_d() +
  labs(y = "outcome")
```

## Pros and Cons

### Pros

-   Works fast computationally
-   Good performance compared to binning
-   Doesn't create correlated features
-   is good at handling continuous changes in predictors

### Cons

-   arguably less interpretable than binning and splines
-   can produce a lot of variables
-   have a hard time modeling sudden changes in distributions

## R Examples

```{r}
#| label: ames
#| echo: false
#| message: false
library(tidymodels)
data("ames")
```

We will be using the `ames` data set for these examples.

```{r}
#| label: show-data
library(recipes)
library(modeldata)

ames |>
  select(Lot_Area, Year_Built)
```

{recipes} has the function `step_poly()` for just this occasion.

```{r}
#| label: step_poly
poly_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_poly(Lot_Area, Year_Built)

poly_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

If you don't like the default number of features created, you can use the `degree` argument to change it.

```{r}
#| label: step_poly-degree-5
poly_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_poly(Lot_Area, Year_Built, degree = 5)

poly_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

while you properly shouldn't, you can turn off the orthogonal polynomials by setting `options = list(raw = TRUE)`.

```{r}
#| label: step_poly-raw
poly_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_poly(Lot_Area, Year_Built, options = list(raw = TRUE))

poly_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

## Python Examples

```{python}
#| label: python-setup
#| echo: false
import pandas as pd
from sklearn import set_config

set_config(transform_output="pandas")
pd.set_option('display.precision', 3)
```

We are using the `ames` data set for examples.
{sklearn} provided the `PolynomialFeatures()`.
We can use it out of the box.

```{python}
#| label: polynomialfeatures
from feazdata import ames
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import PolynomialFeatures

ct = ColumnTransformer(
    [('polynomial', PolynomialFeatures(), ['Lot_Area'])], 
    remainder="passthrough")

ct.fit(ames)
ct.transform(ames)
```

Or we can change the degree using the `degree` argument

```{python}
#| label: polynomialfeatures-degree
ct = ColumnTransformer(
    [('polynomial', PolynomialFeatures(degree = 4), ['Lot_Area'])], 
    remainder="passthrough")

ct.fit(ames)
ct.transform(ames)
```

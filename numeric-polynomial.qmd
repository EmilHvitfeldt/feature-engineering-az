# Polynomial {#sec-polynomial}

```{r}
#| echo: false
library(recipes)
set.seed(1234)
data_toy <- tibble::tibble(
  predictor = rnorm(100) + 1:100
) |>
  dplyr::mutate(outcome = sin(predictor/25) + rnorm(100, sd = 0.1) + 10)
```

```{r}
#| label: fig-polynomial-predictor-outcome
#| echo: false
#| message: false
#| fig-cap: |
#|   Non-linear relationship between predictor and outcome.
#| fig-alt: |
#|   Scatter chart. Predictor along the x-axis and outcome along the y-axis.
#|   The data has some wiggliness to it, but it follows a curve. You would not 
#|   be able to fit a straight line to this data.
library(ggplot2)

data_toy |>
  ggplot(aes(predictor, outcome)) +
  geom_point() +
  theme_minimal()
```

We know that we can fit a polynomial function to some data set. And it would take the following format

$$
\text{poly}(x,\ \text{degree} = n) = a_0 + a_1 x + a_2 x ^ 2 + \cdots + a_n x ^ n
$$

This can then be used to to generate features. Such that each feature is done taking the value to a given degree and multiplying according with the corresponding coefficient.

```{r}
#| label: tbl-poly-raw-values
#| tbl-cap: |
#|   Polynomial values for different values of the predictor, for constants 
#|   equal to 1.
tibble::tibble(
  x = 1:5,
  `x^2` = (1:5) ^ 2,
  `x^3` = (1:5) ^ 3,
  `x^4` = (1:5) ^ 4
) |>
  knitr::kable()
```

In the above table we are seeing a small example of how this could be done, using a 4th degree polynomial with coefficients of 1. If we were to look at the individual functions over the domain of our data we are seing the following

```{r}
#| label: fig-poly-poly-raw-curves
#| echo: false
#| message: false
#| fig-cap: |
#|   Each part of the spline detects a part of the data set.
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   Each of the curves start at 0, goes to smoothly, and then down to zero.
#|   The highpoint for each curve goes further to the right for each curve 
#|   shown.
recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = TRUE, degree = 6, 
            options = list(raw = TRUE)) |>
  prep() |>
  bake(new_data = data_toy) |>
  dplyr::rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  dplyr::select(-outcome) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  theme_minimal()
```

This is to be expected, but we are noticing that these curve look quite similar, and the values these functions are taking very quickly escalate. And this is for predictor values between 0 and 100, higher values will get even higher, possibly causing overflow issues.

We also have an issue where the values appear quite correlated, since the functions are all increasing. As we see below, the correlation between the variables are close to 1 for all pairs.

```{r}
#| label: fig-poly-raw-correlation
#| echo: false
#| message: false
#| fig-cap: |
#|   Almost complete correlation is found between variables
#| fig-alt: |
#|   Correlation chart. The polynomial features are lined up one after
#|   another. almost all the correlation between the pairs are close to 1.
recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = FALSE, degree = 6, 
            options = list(raw = TRUE)) |>
  prep() |>
  bake(new_data = data_toy) |>
  dplyr::rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  dplyr::select(-outcome) |>
  corrr::correlate(quiet = TRUE) |>
  autoplot(method = "identity")
```

This is a problem that needs to be dealt with. The way we can deal with this is by calculation **orthogonal polynomials** instead. We have that any set polynomial function can be rewritten as a set of orthogonal polynomial functions.

TODO Add more bath background here

With this, we deal with the two problems we had before. As seen in the figure below, the functions takes smaller values within their ranges

```{r}
#| echo: false
#| message: false
library(recipes)

rec_poly <- recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = TRUE, degree = 6) |>
  prep()

data_poly <- rec_poly |>
  bake(new_data = data_toy) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")})
```

```{r}
#| label: fig-poly-poly-curves
#| echo: false
#| message: false
#| fig-cap: |
#|   Each part of the spline detects a part of the data set.
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   All the curves are betwwen the ranges of -4 and 4.
data_poly |>
  select(-outcome) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name) +
  theme_minimal()
```

And since they are orthogonal by design, we won't have to worry about correlated features.

```{r}
#| label: fig-poly-otho-correlation
#| echo: false
#| message: false
#| fig-cap: |
#|   No correlation to be found
#| fig-alt: |
#|   Correlation chart. The polynomial features are lined up one after another. 
#|   No correlation is found between any of the pairs.
recipe(outcome ~ predictor, data = data_toy) |>
  step_poly(predictor, keep_original_cols = FALSE, degree = 6, 
            options = list(raw = FALSE)) |>
  prep() |>
  bake(new_data = data_toy) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  select(-outcome) |>
  corrr::correlate(quiet = TRUE) |>
  autoplot(method = "identity")
```

The interpretation of these polynomial features are not as easy as with binning in @sec-numeric-binning or splines @sec-splines, but the calculations are quite fast and versatile.

```{r}
#| label: fig-poly-poly-curves-extrapolate
#| echo: false
#| message: false
#| fig-cap: |
#|   Polynomial features don't handle extrapolation well and values outside
#|   the normal ranges can explode quite fast for higher degree polynomials.
#| fig-alt: |
#|   Facetted line chart. Predictor along the x-axis, value along the y-axis.
#|   Each of the curves have their endpoints go towards infinite or minus 
#|   infinite depending on their degree.
rec_poly |>
  bake(new_data = tibble::tibble(predictor = seq(-500, 500))) |>
  rename_all(\(x) {stringr::str_replace(x, "predictor_poly_", "Polynomial Feature ")}) |>
  tidyr::pivot_longer(cols = -predictor) |>
  ggplot(aes(predictor, value)) +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  theme_minimal()
```

## Pros and Cons

### Pros

- Works fast computationally
- Good performance compared to binning
- Doesn't create correlated features

### Cons

- arguably less interpretable than binning and splines
- can produce a lot of variables

## R Examples

```{r}
#| echo: false
#| message: false
library(tidymodels)
data("ames")
```

We will be using the `ames` data set for these examples.

```{r}
ames |>
  select(Lot_Area, Year_Built)
```

{recipes} provides a number of steps to perform spline operations, each of them starts with `step_spline_`. Let us use a B-spline and a M-spline as examples here:

```{r}
log_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_spline_b(Lot_Area) |>
  step_spline_monotone(Year_Built)

log_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

We can set the `deg_free` argument to specify how many spline features we want for each of the splines.

```{r}
log_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_spline_b(Lot_Area, deg_free = 3) |>
  step_spline_monotone(Year_Built, deg_free = 4)

log_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

These steps have more arguments, so we can change other things. The B-splines created by `step_spline_b()` defaults to cubic splines, but we can change that by specifying which polynomial degree with want with the `degree` argument.

```{r}
log_rec <- recipe(~ Lot_Area + Year_Built, data = ames) |>
  step_spline_b(Lot_Area, deg_free = 3, degree = 1) |>
  step_spline_monotone(Year_Built, deg_free = 4)

log_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

## Python Examples

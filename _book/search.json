[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feature Engineering A-Z",
    "section": "",
    "text": "Preface\nWelcome to “Feature Engineering A-Z”! This book is written to be used as a reference guide to the different techniques. This is reflected in the chapter structure. Any question a practitioner is having should be answered by looking at the index and finding the right chapter.\nEach section tries to be as comprehensive as possible with the number of different methods/solutions that are presented. A section on dimensionality reduction should list all the practical methods that could be used, as well as a comparison between the methods to help the reader decide what would be most appropriate. This does not mean that all methods are recommended to use. A number of these methods have little and narrow use cases.\nWhenever possible each method will be accompanied by simple mathematical formulas and visualizations to illustrate the mechanics of the method.\nLastly, each section will include code snippets in showcasing how to implement the methods. Preferable in R and Python, Keras/PyTorch. This book is a methods book first, a coding book second."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "It is commonly said that feature engineering, much like machine learning, is an art rather then a science. This wants to reemphasize this point. But being an art doesn’t mean we can’t thoroughly explain the tools and techniques. This is the main goal of this book. Giving you the knowledge of the different techniques you are likely to see and work with, and enough of a base, that any future methods won’t be too intimidating."
  },
  {
    "objectID": "motivation.html",
    "href": "motivation.html",
    "title": "2  Motivation",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "3  Where does feature engineering fit into the modeling workflow?",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "how-to-deal-with.html#sec-terminology",
    "href": "how-to-deal-with.html#sec-terminology",
    "title": "How to Deal With …",
    "section": "Terminology",
    "text": "Terminology\nBelow are some terms we use throughout the book, that we want to make sure are clear. Some of these might differ from other books, and that is fine. This is why we have this section. Some of the methods described in this book are known under multiple names. When that is the case, it will be listed at the beginning of the chapter. The index will like wise point you to the right chapter regardless of which name you use.\n\nObservations\nThis books will mostly be working with rectangular data. In this context, each observation is defined as a row, with the columns holding the specific characteristics for each observation.\nThe observational unit can change depending on the data. If we were looking at a data set of restaurant health code inspections, you are likely to see the data with one row per inspection. However, depending on your problem statement or hypothesis, you might want to think of each restaurant as a observation. If you are thinking from a planning perspective you could think of each day/week as an observation.\nReading this book will not tell you how to think of your data as we don’t know what you are trying to do. Once you have your data in the right format and order, we can show you what is possible.\n\n\nLearned\nSome methods requires information to be transformed, that we are not able to supply beforehand. In the case of centering of numeric variables described in Chapter 10. To be able to do this transformation, you need to know the mean value of the training data set. This mean is the sufficient information needed to perform the calculations, and is the reason why the method is a learned method.\nOn the other hand, taking the square root of a variable as described in Chapter 6 isn’t a learned method as there isn’t any sufficient information needed. The method can be applied right away.\n\n\nSupervised / Unsupervised\nSome method use the outcome to guide the calculations. If the outcome is used, the method is said to be supervised. Most methods are unsupervised.\n\n\nLevels\nVariables that contain non-numeric information are typically called qualitative or categorical variables. This can be things such as eye color, street names, names, grades, car model and subscription types. Where there is finite known set of values a categorical variable can take, we call these values the levels of that variables. So the levels of the variables containing weekdays are “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, and “Sunday”. But the names of our subscribers doesn’t have levels as we don’t know all of them.\nWe will sometimes bend this definition, as it is sometimes useful to pretend that a variable has a finite known set of values, even if it doesn’t.\n\n\nLinear models\nWe talk about linear models as models that are specified as a linear combination of features. These models tend to simple, and fast to use, but having the limitation of “linear combination of features” means that struggle if the data has non-linear effects."
  },
  {
    "objectID": "numeric.html#distributional-problems",
    "href": "numeric.html#distributional-problems",
    "title": "4  Overview",
    "section": "4.1 Distributional problems",
    "text": "4.1 Distributional problems\n\nlogarithm\nsqrt\nBoxCox\nYeo-Johnson\nPercentile"
  },
  {
    "objectID": "numeric.html#sec-numeric-scaling-issues",
    "href": "numeric.html#sec-numeric-scaling-issues",
    "title": "4  Overview",
    "section": "4.2 Scaling issues",
    "text": "4.2 Scaling issues\nThe topic of feature scaling is used important and used widely in all of machine learning. This chapter will go over what feature scaling is and why we want to use it. The following chapters will each go over a different method of feature scaling.\n\n\n\n\n\n\nNote\n\n\n\nThere is some disagreement about the naming of these topics. These types of methods are called feature scaling and scaling in different fields. This book will call this general class of methods feature scaling and will make notes for each specific method and what other names they go by.\n\n\nIn this book, we will define feature scaling as an operation that modified variables using multiplication and addition. While broadly defined, the methods typically reduce to the following form:\n\\[\nX_{scaled} = \\dfrac{X - a}{b}\n\\tag{4.1}\\]\nThe main difference between the methods is how \\(a\\) and \\(b\\) are calculated. These methods are learned transformation. So we use the training data to derive the right values of \\(a\\) and \\(b\\), and then these values are used to perform the transformations when applied to new data. The different methods might differ on what property is desired for the transformed variables, same range or same spread, but they never change the distribution itself. The power transformations we saw in Chapter 7 and Chapter 8, distort the transformations, where these feature scalings essentially perform a “zooming” effect.\n\n\nTable 4.1: All feature scaling methods\n\n\n\n\n\n\nMethod\nDefinition\n\n\n\n\nCentering\n\\(X_{scaled} = X - \\text{mean}(X)\\)\n\n\nScaling\n\\(X_{scaled} = \\dfrac{X}{\\text{sd}(X)}\\)\n\n\nMax-Abs\n\\(X_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\\)\n\n\nNormalization\n\\(X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\)\n\n\nMin-Max\n\\(X_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\)\n\n\nRobust\n\\(X_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)}\\)\n\n\n\n\nWe see here that all the methods in Table 4.1 follows the equation Equation 4.1. Sometimes \\(a\\) and \\(b\\) takes a value of 0, which is perfectly fine. Centering and scaling when used together is equal to normalization. They are kept separate in the table since they are sometimes used independently. Centering, scaling, and normalization will all be discussed in Chapter 10.\nThere are 2 main reasons why we want to perform feature scaling. Firstly, many different types of models take the magnitude of the variables into account when fitting the models, so having variables on different scales can be disadvantageous because some variables have high priorities. In turn, we get that the other variables have low priority. Models that work using Euclidean distances like KNN models are affected by this change. Regularized models such as lasso and ridge regression also need to be scaled since the regularization depends on the magnitude of the estimates. Secondly, some algorithms simply converge much faster when all the variables are on the same scale. These types of models produce the same fit, just at a slower pace than if you don’t scale the variables. Any algorithms using Gradient Descent fits into this category.\nTODO: Have KNN diagram show why this is important\nList which types of models need feature scaling. Should be a 2 column list. Left=name, right=comment %in% c(no effect, different fit, slow down)"
  },
  {
    "objectID": "numeric.html#non-linear-effect",
    "href": "numeric.html#non-linear-effect",
    "title": "4  Overview",
    "section": "4.3 Non-linear effect",
    "text": "4.3 Non-linear effect\n\nbinning\nsplines\npolynomial\n\nTODO: Show different distributions, and how well the different methods does at dealing with them"
  },
  {
    "objectID": "numeric.html#sec-numeric-outliers-issues",
    "href": "numeric.html#sec-numeric-outliers-issues",
    "title": "4  Overview",
    "section": "4.4 Outliers",
    "text": "4.4 Outliers"
  },
  {
    "objectID": "numeric.html#other",
    "href": "numeric.html#other",
    "title": "4  Overview",
    "section": "4.5 Other",
    "text": "4.5 Other\nThere are any number of transformations we can apply to numeric data, other functions include:\n\nhyperbolic\nRelu\ninverse\ninverse logit\nlogit"
  },
  {
    "objectID": "numeric-logarithms.html#pros-and-cons",
    "href": "numeric-logarithms.html#pros-and-cons",
    "title": "5  Logarithms",
    "section": "5.1 Pros and Cons",
    "text": "5.1 Pros and Cons\n\n5.1.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data set alike\n\n\n\n5.1.2 Cons\n\nNeeds offset to deal with negative data\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn’t skewed. See the effect below on 10,000 uniformly distributed values"
  },
  {
    "objectID": "numeric-logarithms.html#r-examples",
    "href": "numeric-logarithms.html#r-examples",
    "title": "5  Logarithms",
    "section": "5.2 R Examples",
    "text": "5.2 R Examples\nWe will be using the ames data set for these examples.\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nlog_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_log(Lot_Area)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    10.4      215000\n 2     9.36     105000\n 3     9.57     172000\n 4     9.32     244000\n 5     9.53     189900\n 6     9.21     195500\n 7     8.50     213500\n 8     8.52     191500\n 9     8.59     236500\n10     8.92     189000\n# ℹ 2,920 more rows\n\n\nThe base can be changed by setting the base argument.\n\nlog_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_log(Lot_Area, base = 2)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     15.0     215000\n 2     13.5     105000\n 3     13.8     172000\n 4     13.4     244000\n 5     13.8     189900\n 6     13.3     195500\n 7     12.3     213500\n 8     12.3     191500\n 9     12.4     236500\n10     12.9     189000\n# ℹ 2,920 more rows\n\n\nIf we have non-positive values, which we do in the Wood_Deck_SF variable because it has quite a lot of zeroes, we get -Inf which isn’t going to work.\n\nlog_rec &lt;- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |&gt;\n  step_log(Wood_Deck_SF)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Wood_Deck_SF Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1         5.35     215000\n 2         4.94     105000\n 3         5.97     172000\n 4      -Inf        244000\n 5         5.36     189900\n 6         5.89     195500\n 7      -Inf        213500\n 8      -Inf        191500\n 9         5.47     236500\n10         4.94     189000\n# ℹ 2,920 more rows\n\n\nSetting the offset argument helps us to deal with that problem.\n\nlog_rec &lt;- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |&gt;\n  step_log(Wood_Deck_SF, offset = 0.5)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Wood_Deck_SF Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1        5.35      215000\n 2        4.95      105000\n 3        5.98      172000\n 4       -0.693     244000\n 5        5.36      189900\n 6        5.89      195500\n 7       -0.693     213500\n 8       -0.693     191500\n 9        5.47      236500\n10        4.95      189000\n# ℹ 2,920 more rows"
  },
  {
    "objectID": "numeric-logarithms.html#python-examples",
    "href": "numeric-logarithms.html#python-examples",
    "title": "5  Logarithms",
    "section": "5.3 Python Examples",
    "text": "5.3 Python Examples\nTODO"
  },
  {
    "objectID": "numeric-sqrt.html#pros-and-cons",
    "href": "numeric-sqrt.html#pros-and-cons",
    "title": "6  Square Root",
    "section": "6.1 Pros and Cons",
    "text": "6.1 Pros and Cons\n\n6.1.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data set alike\nCan be applied to all numbers, not just non-negative values\n\n\n\n6.1.2 Cons\n\nIt will leave regression coefficients virtually uninterpretable\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn’t skewed"
  },
  {
    "objectID": "numeric-sqrt.html#r-examples",
    "href": "numeric-sqrt.html#r-examples",
    "title": "6  Square Root",
    "section": "6.2 R Examples",
    "text": "6.2 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\nhotel_bookings |&gt;\n  select(lead_time, adr)\n\n# A tibble: 119,390 × 2\n   lead_time   adr\n       &lt;dbl&gt; &lt;dbl&gt;\n 1       342    0 \n 2       737    0 \n 3         7   75 \n 4        13   75 \n 5        14   98 \n 6        14   98 \n 7         0  107 \n 8         9  103 \n 9        85   82 \n10        75  106.\n# ℹ 119,380 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\n# TODO use signed sqrt\nsqrt_rec &lt;- recipe(lead_time ~ adr, data = hotel_bookings) |&gt;\n  step_sqrt(adr)\n\nsqrt_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nWarning in sqrt(new_data[[col_name]]): NaNs produced\n\n\n# A tibble: 119,390 × 2\n     adr lead_time\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  0          342\n 2  0          737\n 3  8.66         7\n 4  8.66        13\n 5  9.90        14\n 6  9.90        14\n 7 10.3          0\n 8 10.1          9\n 9  9.06        85\n10 10.3         75\n# ℹ 119,380 more rows"
  },
  {
    "objectID": "numeric-sqrt.html#python-examples",
    "href": "numeric-sqrt.html#python-examples",
    "title": "6  Square Root",
    "section": "6.3 Python Examples",
    "text": "6.3 Python Examples"
  },
  {
    "objectID": "numeric-boxcox.html#pros-and-cons",
    "href": "numeric-boxcox.html#pros-and-cons",
    "title": "7  Box-Cox",
    "section": "7.1 Pros and Cons",
    "text": "7.1 Pros and Cons\n\n7.1.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\n\n\n\n7.1.2 Cons\n\nDoesn’t work with negative values\nIsn’t a universal fix"
  },
  {
    "objectID": "numeric-boxcox.html#r-examples",
    "href": "numeric-boxcox.html#r-examples",
    "title": "7  Box-Cox",
    "section": "7.2 R Examples",
    "text": "7.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform Box-Cox transformations.\n\nboxcox_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_BoxCox(Lot_Area) |&gt;\n  prep()\n\nboxcox_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# ℹ 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nboxcox_rec |&gt;\n  tidy(1)\n\n# A tibble: 1 × 3\n  terms    value id          \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 Lot_Area 0.129 BoxCox_3gJXR"
  },
  {
    "objectID": "numeric-boxcox.html#python-examples",
    "href": "numeric-boxcox.html#python-examples",
    "title": "7  Box-Cox",
    "section": "7.3 Python Examples",
    "text": "7.3 Python Examples"
  },
  {
    "objectID": "numeric-yeojohnson.html#pros-and-cons",
    "href": "numeric-yeojohnson.html#pros-and-cons",
    "title": "8  Yeo-Johnson",
    "section": "8.1 Pros and Cons",
    "text": "8.1 Pros and Cons\n\n8.1.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\nCan handle negative values\n\n\n\n8.1.2 Cons\n\nIsn’t a universal fix"
  },
  {
    "objectID": "numeric-yeojohnson.html#r-examples",
    "href": "numeric-yeojohnson.html#r-examples",
    "title": "8  Yeo-Johnson",
    "section": "8.2 R Examples",
    "text": "8.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform Yeo-Johnson transformations, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nyeojohnson_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_YeoJohnson(Lot_Area) |&gt;\n  prep()\n\nyeojohnson_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# ℹ 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nyeojohnson_rec |&gt;\n  tidy(1)\n\n# A tibble: 1 × 3\n  terms    value id              \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1 Lot_Area 0.129 YeoJohnson_3gJXR"
  },
  {
    "objectID": "numeric-yeojohnson.html#python-examples",
    "href": "numeric-yeojohnson.html#python-examples",
    "title": "8  Yeo-Johnson",
    "section": "8.3 Python Examples",
    "text": "8.3 Python Examples"
  },
  {
    "objectID": "numeric-percentile.html#pros-and-cons",
    "href": "numeric-percentile.html#pros-and-cons",
    "title": "9  Percentile",
    "section": "9.1 Pros and Cons",
    "text": "9.1 Pros and Cons\n\n9.1.1 Pros\n\nTransformation isn’t affected much by outliers\n\n\n\n9.1.2 Cons\n\nDoesn’t allow to exact reverse transformation\nIsn’t ideal if training data doesn’t have that many unique values"
  },
  {
    "objectID": "numeric-percentile.html#r-examples",
    "href": "numeric-percentile.html#r-examples",
    "title": "9  Percentile",
    "section": "9.2 R Examples",
    "text": "9.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\nThe {recipes} step to do this transformation is step_percentile(). It defaults to calculation 100 percentiles and using those to transform the data\n\npercentile_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_percentile(Lot_Area) |&gt;\n  prep()\n\npercentile_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    0.989     215000\n 2    0.756     105000\n 3    0.898     172000\n 4    0.717     244000\n 5    0.883     189900\n 6    0.580     195500\n 7    0.104     213500\n 8    0.106     191500\n 9    0.120     236500\n10    0.259     189000\n# ℹ 2,920 more rows\n\n\nWe cna use the tidy() method to pull out what the specific values are for each percentile\n\npercentile_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 × 4\n   term     value percentile id              \n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n 1 Lot_Area 1300           0 percentile_Bp5vK\n 2 Lot_Area 1680           1 percentile_Bp5vK\n 3 Lot_Area 2040.          2 percentile_Bp5vK\n 4 Lot_Area 2362.          3 percentile_Bp5vK\n 5 Lot_Area 2779.          4 percentile_Bp5vK\n 6 Lot_Area 3188.          5 percentile_Bp5vK\n 7 Lot_Area 3674.          6 percentile_Bp5vK\n 8 Lot_Area 3901.          7 percentile_Bp5vK\n 9 Lot_Area 4122.          8 percentile_Bp5vK\n10 Lot_Area 4435           9 percentile_Bp5vK\n# ℹ 89 more rows\n\n\nYou are able to change the granularity by using the options argument. In this example we are calculation 500 points evenly spaced between 0 and 1, both inclusive.\n\npercentile500_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_percentile(Lot_Area, options = list(probs = (0:500)/500)) |&gt;\n  prep()\n\npercentile500_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    0.989     215000\n 2    0.755     105000\n 3    0.899     172000\n 4    0.717     244000\n 5    0.884     189900\n 6    0.580     195500\n 7    0.103     213500\n 8    0.106     191500\n 9    0.118     236500\n10    0.254     189000\n# ℹ 2,920 more rows\n\n\nAnd we can see the more precise numbers.\n\npercentile500_rec |&gt;\n  tidy(1)\n\n# A tibble: 457 × 4\n   term     value percentile id              \n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n 1 Lot_Area 1300         0   percentile_RUieL\n 2 Lot_Area 1487.        0.2 percentile_RUieL\n 3 Lot_Area 1531.        0.4 percentile_RUieL\n 4 Lot_Area 1605.        0.6 percentile_RUieL\n 5 Lot_Area 1680         0.8 percentile_RUieL\n 6 Lot_Area 1879.        1.4 percentile_RUieL\n 7 Lot_Area 1890         1.6 percentile_RUieL\n 8 Lot_Area 1946.        1.8 percentile_RUieL\n 9 Lot_Area 2040.        2   percentile_RUieL\n10 Lot_Area 2136.        2.2 percentile_RUieL\n# ℹ 447 more rows\n\n\nNotice how there are only 457 values in this output. This is happening because some percentile have been collapsed to save space since if the value for the 10.4 and 10.6 percentile is the same, we just store the 10.6 value."
  },
  {
    "objectID": "numeric-percentile.html#python-examples",
    "href": "numeric-percentile.html#python-examples",
    "title": "9  Percentile",
    "section": "9.3 Python Examples",
    "text": "9.3 Python Examples"
  },
  {
    "objectID": "numeric-normalization.html#pros-and-cons",
    "href": "numeric-normalization.html#pros-and-cons",
    "title": "10  Normalization",
    "section": "10.1 Pros and Cons",
    "text": "10.1 Pros and Cons\n\n10.1.1 Pros\n\nIf you don’t have any severe outliers then you will rarely see any downsides to applying normalization\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n10.1.2 Cons\n\nNot all software solutions will not be helpful when applying this transformation to a constant variable. A division by 0 error is likely what you will see\nCannot be used with sparse data as it isn’t preserved because of the centering that is happening. If you only scale the data you don’t have a problem\nThis transformation is highly affected by outliers, as they affect the mean and standard deviation quite a lot\n\nBelow is the figure Figure 10.2 is an illustration of the effect by having a single high value. In this case, a single observation with the value 10000 moved the transformed distribution much tighter around zero. And all but removed the variance of the non-outliers.\n\n\n\n\n\nFigure 10.2: Outliers can have a big effect on the resulting distribution when applying normalization."
  },
  {
    "objectID": "numeric-normalization.html#r-examples",
    "href": "numeric-normalization.html#r-examples",
    "title": "10  Normalization",
    "section": "10.2 R Examples",
    "text": "10.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform scaling, centering, and normalization. They are called step_scale(), step_center() and step_normalize() respectively.\nBelow is an example using step_scale()\n\nscale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  prep()\n\nscale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    4.03          1.66        0.627\n 2     105000    1.47          1.11        0    \n 3     172000    1.81          3.11        0.605\n 4     244000    1.42          0           0    \n 5     189900    1.76          1.68        0    \n 6     195500    1.27          2.85        0.112\n 7     213500    0.624         0           0    \n 8     191500    0.635         0           0    \n 9     236500    0.684         1.88        0    \n10     189000    0.952         1.11        0    \n# ℹ 2,920 more rows\n\n\nWe can also pull out the value of the standard deviation for each variable that was affected using tidy()\n\nscale_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 3\n   terms            value id         \n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage     33.5  scale_FGmgk\n 2 Lot_Area       7880.   scale_FGmgk\n 3 Year_Built       30.2  scale_FGmgk\n 4 Year_Remod_Add   20.9  scale_FGmgk\n 5 Mas_Vnr_Area    179.   scale_FGmgk\n 6 BsmtFin_SF_1      2.23 scale_FGmgk\n 7 BsmtFin_SF_2    169.   scale_FGmgk\n 8 Bsmt_Unf_SF     440.   scale_FGmgk\n 9 Total_Bsmt_SF   441.   scale_FGmgk\n10 First_Flr_SF    392.   scale_FGmgk\n# ℹ 23 more rows\n\n\nWe could also have used step_center() and step_scale() together in one recipe\n\ncenter_scale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  prep()\n\ncenter_scale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# ℹ 2,920 more rows\n\n\nUsing tidy() we can see information about each step\n\ncenter_scale_rec |&gt;\n  tidy()\n\n# A tibble: 2 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_tSRk5\n2      2 step      scale  TRUE    FALSE scale_kjP2v \n\n\nAnd we can pull out the means using tidy(1)\n\ncenter_scale_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 3\n   terms             value id          \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage      57.6  center_tSRk5\n 2 Lot_Area       10148.   center_tSRk5\n 3 Year_Built      1971.   center_tSRk5\n 4 Year_Remod_Add  1984.   center_tSRk5\n 5 Mas_Vnr_Area     101.   center_tSRk5\n 6 BsmtFin_SF_1       4.18 center_tSRk5\n 7 BsmtFin_SF_2      49.7  center_tSRk5\n 8 Bsmt_Unf_SF      559.   center_tSRk5\n 9 Total_Bsmt_SF   1051.   center_tSRk5\n10 First_Flr_SF    1160.   center_tSRk5\n# ℹ 23 more rows\n\n\nand the standard deviation using tidy(2)\n\ncenter_scale_rec |&gt;\n  tidy(2)\n\n# A tibble: 33 × 3\n   terms            value id         \n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage     33.5  scale_kjP2v\n 2 Lot_Area       7880.   scale_kjP2v\n 3 Year_Built       30.2  scale_kjP2v\n 4 Year_Remod_Add   20.9  scale_kjP2v\n 5 Mas_Vnr_Area    179.   scale_kjP2v\n 6 BsmtFin_SF_1      2.23 scale_kjP2v\n 7 BsmtFin_SF_2    169.   scale_kjP2v\n 8 Bsmt_Unf_SF     440.   scale_kjP2v\n 9 Total_Bsmt_SF   441.   scale_kjP2v\n10 First_Flr_SF    392.   scale_kjP2v\n# ℹ 23 more rows\n\n\nSince these steps often follow each other, we often use the step_normalize() as a shortcut to do both operations in one step\n\nscale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  prep()\n\nscale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# ℹ 2,920 more rows\n\n\nAnd we can still pull out the means and standard deviations using tidy()\n\nscale_rec |&gt;\n  tidy(1) |&gt;\n  filter(terms %in% c(\"Lot_Area\", \"Wood_Deck_SF\", \"Mas_Vnr_Area\"))\n\n# A tibble: 6 × 4\n  terms        statistic   value id             \n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          \n1 Lot_Area     mean      10148.  normalize_ucdPw\n2 Mas_Vnr_Area mean        101.  normalize_ucdPw\n3 Wood_Deck_SF mean         93.8 normalize_ucdPw\n4 Lot_Area     sd         7880.  normalize_ucdPw\n5 Mas_Vnr_Area sd          179.  normalize_ucdPw\n6 Wood_Deck_SF sd          126.  normalize_ucdPw"
  },
  {
    "objectID": "numeric-normalization.html#python-examples",
    "href": "numeric-normalization.html#python-examples",
    "title": "10  Normalization",
    "section": "10.3 Python Examples",
    "text": "10.3 Python Examples"
  },
  {
    "objectID": "numeric-range.html#pros-and-cons",
    "href": "numeric-range.html#pros-and-cons",
    "title": "11  Range",
    "section": "11.1 Pros and Cons",
    "text": "11.1 Pros and Cons\n\n11.1.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale, provided that clipping wasn’t turned on\n\n\n\n11.1.2 Cons\n\nTurning on clipping diminishes the effect of outliers by rounding them up/down\nDoesn’t work with zero variance data as max(x) - min(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn’t preserved"
  },
  {
    "objectID": "numeric-range.html#r-examples",
    "href": "numeric-range.html#r-examples",
    "title": "11  Range",
    "section": "11.2 R Examples",
    "text": "11.2 R Examples\nstep_range() clips. Does allow user to specify range step_minmax() doesn’t clip. doens’t allow user to specify range. A PR is planned to allow users to turn off clipping in step_range()\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\nWe will be using the step_range() step for this\n\nrange_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_range(all_numeric_predictors()) |&gt;\n  prep()\n\nrange_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   0.142        0.147        0.07  \n 2     105000   0.0482       0.0983       0     \n 3     172000   0.0606       0.276        0.0675\n 4     244000   0.0461       0            0     \n 5     189900   0.0586       0.149        0     \n 6     195500   0.0406       0.253        0.0125\n 7     213500   0.0169       0            0     \n 8     191500   0.0173       0            0     \n 9     236500   0.0191       0.166        0     \n10     189000   0.0290       0.0983       0     \n# ℹ 2,920 more rows\n\n\nWe can also pull out what the min and max value were for each variable using tidy()\n\nrange_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 4\n   terms            min    max id         \n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage       0    313 range_FGmgk\n 2 Lot_Area        1300 215245 range_FGmgk\n 3 Year_Built      1872   2010 range_FGmgk\n 4 Year_Remod_Add  1950   2010 range_FGmgk\n 5 Mas_Vnr_Area       0   1600 range_FGmgk\n 6 BsmtFin_SF_1       0      7 range_FGmgk\n 7 BsmtFin_SF_2       0   1526 range_FGmgk\n 8 Bsmt_Unf_SF        0   2336 range_FGmgk\n 9 Total_Bsmt_SF      0   6110 range_FGmgk\n10 First_Flr_SF     334   5095 range_FGmgk\n# ℹ 23 more rows\n\n\nusing the min and max argument we can set different ranges\n\nrange_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_range(all_numeric_predictors(), min = -2, max = 2) |&gt;\n  prep()\n\nrange_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    -1.43       -1.41         -1.72\n 2     105000    -1.81       -1.61         -2   \n 3     172000    -1.76       -0.896        -1.73\n 4     244000    -1.82       -2            -2   \n 5     189900    -1.77       -1.40         -2   \n 6     195500    -1.84       -0.989        -1.95\n 7     213500    -1.93       -2            -2   \n 8     191500    -1.93       -2            -2   \n 9     236500    -1.92       -1.33         -2   \n10     189000    -1.88       -1.61         -2   \n# ℹ 2,920 more rows"
  },
  {
    "objectID": "numeric-range.html#python-examples",
    "href": "numeric-range.html#python-examples",
    "title": "11  Range",
    "section": "11.3 Python Examples",
    "text": "11.3 Python Examples\nMinMaxScaler() doesn’t clip by default. Allows user to specify range."
  },
  {
    "objectID": "numeric-maxabs.html#pros-and-cons",
    "href": "numeric-maxabs.html#pros-and-cons",
    "title": "12  Max Abs",
    "section": "12.1 Pros and Cons",
    "text": "12.1 Pros and Cons\n\n12.1.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\nDoesn’t affect sparsity\nCan be used on a zero variance variable. Doesn’t matter such since you likely should get rid of it\n\n\n\n12.1.2 Cons\n\nIs highly affected by outliers"
  },
  {
    "objectID": "numeric-maxabs.html#r-examples",
    "href": "numeric-maxabs.html#r-examples",
    "title": "12  Max Abs",
    "section": "12.2 R Examples",
    "text": "12.2 R Examples\nWe will be using the ames data set for these examples.\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\nWe will be using the step_maxabs() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_maxabs(all_numeric_predictors()) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   0.148        0.147        0.07  \n 2     105000   0.0540       0.0983       0     \n 3     172000   0.0663       0.276        0.0675\n 4     244000   0.0518       0            0     \n 5     189900   0.0643       0.149        0     \n 6     195500   0.0464       0.253        0.0125\n 7     213500   0.0229       0            0     \n 8     191500   0.0233       0            0     \n 9     236500   0.0250       0.166        0     \n10     189000   0.0348       0.0983       0     \n# ℹ 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   max          313 maxabs_Bp5vK\n 2 Lot_Area       max       215245 maxabs_Bp5vK\n 3 Year_Built     max         2010 maxabs_Bp5vK\n 4 Year_Remod_Add max         2010 maxabs_Bp5vK\n 5 Mas_Vnr_Area   max         1600 maxabs_Bp5vK\n 6 BsmtFin_SF_1   max            7 maxabs_Bp5vK\n 7 BsmtFin_SF_2   max         1526 maxabs_Bp5vK\n 8 Bsmt_Unf_SF    max         2336 maxabs_Bp5vK\n 9 Total_Bsmt_SF  max         6110 maxabs_Bp5vK\n10 First_Flr_SF   max         5095 maxabs_Bp5vK\n# ℹ 23 more rows"
  },
  {
    "objectID": "numeric-maxabs.html#python-examples",
    "href": "numeric-maxabs.html#python-examples",
    "title": "12  Max Abs",
    "section": "12.3 Python Examples",
    "text": "12.3 Python Examples"
  },
  {
    "objectID": "numeric-robust.html#pros-and-cons",
    "href": "numeric-robust.html#pros-and-cons",
    "title": "13  Robust Scaling",
    "section": "13.1 Pros and Cons",
    "text": "13.1 Pros and Cons\n\n13.1.1 Pros\n\nIsn’t affected by outliers\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n13.1.2 Cons\n\nCompletely ignores part of the data outside the quantile ranges\nDoesn’t work with near zero variance data as Q1(x) - Q3(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn’t preserved"
  },
  {
    "objectID": "numeric-robust.html#r-examples",
    "href": "numeric-robust.html#r-examples",
    "title": "13  Robust Scaling",
    "section": "13.2 R Examples",
    "text": "13.2 R Examples\nWe will be using the ames data set for these examples.\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\nWe will be using the step_robust() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_robust(all_numeric_predictors()) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    5.43         1.25         0.688\n 2     105000    0.531        0.833        0    \n 3     172000    1.17         2.34         0.664\n 4     244000    0.419        0            0    \n 5     189900    1.07         1.26         0    \n 6     195500    0.132        2.14         0.123\n 7     213500   -1.10         0            0    \n 8     191500   -1.08         0            0    \n 9     236500   -0.984        1.41         0    \n10     189000   -0.471        0.833        0    \n# ℹ 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   lower        43  robust_Bp5vK\n 2 Lot_Frontage   median       63  robust_Bp5vK\n 3 Lot_Frontage   higher       78  robust_Bp5vK\n 4 Lot_Area       lower      7440. robust_Bp5vK\n 5 Lot_Area       median     9436. robust_Bp5vK\n 6 Lot_Area       higher    11555. robust_Bp5vK\n 7 Year_Built     lower      1954  robust_Bp5vK\n 8 Year_Built     median     1973  robust_Bp5vK\n 9 Year_Built     higher     2001  robust_Bp5vK\n10 Year_Remod_Add lower      1965  robust_Bp5vK\n# ℹ 89 more rows\n\n\nWe can also can\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_robust(all_numeric_predictors(), range = c(0.1, 0.9)) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.35          0.820       0.350 \n 2     105000   0.230         0.547       0     \n 3     172000   0.509         1.53        0.337 \n 4     244000   0.181         0           0     \n 5     189900   0.463         0.828       0     \n 6     195500   0.0570        1.41        0.0625\n 7     213500  -0.475         0           0     \n 8     191500  -0.467         0           0     \n 9     236500  -0.426         0.925       0     \n10     189000  -0.204         0.547       0     \n# ℹ 2,920 more rows\n\n\nwhen we pull out the ranges, we see that they are wider\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   lower         0  robust_RUieL\n 2 Lot_Frontage   median       63  robust_RUieL\n 3 Lot_Frontage   higher       91  robust_RUieL\n 4 Lot_Area       lower      4800  robust_RUieL\n 5 Lot_Area       median     9436. robust_RUieL\n 6 Lot_Area       higher    14299. robust_RUieL\n 7 Year_Built     lower      1925. robust_RUieL\n 8 Year_Built     median     1973  robust_RUieL\n 9 Year_Built     higher     2006  robust_RUieL\n10 Year_Remod_Add lower      1950  robust_RUieL\n# ℹ 89 more rows"
  },
  {
    "objectID": "numeric-robust.html#python-examples",
    "href": "numeric-robust.html#python-examples",
    "title": "13  Robust Scaling",
    "section": "13.3 Python Examples",
    "text": "13.3 Python Examples"
  },
  {
    "objectID": "numeric-binning.html#pros-and-cons",
    "href": "numeric-binning.html#pros-and-cons",
    "title": "14  Binning",
    "section": "14.1 Pros and Cons",
    "text": "14.1 Pros and Cons\n\n14.1.1 Pros\n\nWorks fast computationally\nBehaves predictably outside the range of the predictors\nIf cuts are playes well, it can handle sudden changes in distributions\nInterpretable\ndoens’t create correlated features\n\n\n\n14.1.2 Cons\n\nThe inherent rounding that happens, can lead to loss of performance and interpretations\narguably less interpretable than binning\ncan produce a lot of variables"
  },
  {
    "objectID": "numeric-binning.html#r-examples",
    "href": "numeric-binning.html#r-examples",
    "title": "14  Binning",
    "section": "14.2 R Examples",
    "text": "14.2 R Examples\nWe will be using the ames data set for these examples.\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 × 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ℹ 2,920 more rows\n\n\n{recipes} has the function step_discretize() for just this occasion.\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nWarning: Note that the options `prefix` and `labels` will be applied to all\nvariables\n\n\nRows: 2,930\nColumns: 2\n$ Lot_Area   &lt;fct&gt; bin4, bin4, bin4, bin3, bin4, bin3, bin1, bin1, bin1, bin2,…\n$ Year_Built &lt;fct&gt; bin2, bin2, bin2, bin2, bin3, bin3, bin3, bin3, bin3, bin3,…\n\n\nIf you don’t like the default number of breaks created, you can use the num_breaks = 6 argument to change it.\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built, num_breaks = 6)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nWarning: Note that the options `prefix` and `labels` will be applied to all\nvariables\n\n\nRows: 2,930\nColumns: 2\n$ Lot_Area   &lt;fct&gt; bin6, bin5, bin6, bin5, bin6, bin4, bin1, bin1, bin1, bin2,…\n$ Year_Built &lt;fct&gt; bin2, bin3, bin2, bin3, bin5, bin5, bin5, bin4, bin4, bin5,…\n\n\nThis step technically creates a factor variable, but we can turn it into a series of indicator functions with step_dummy()\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built, num_breaks = 6) |&gt;\n  step_dummy(Lot_Area, Year_Built, one_hot = TRUE)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nWarning: Note that the options `prefix` and `labels` will be applied to all\nvariables\n\n\nRows: 2,930\nColumns: 12\n$ Lot_Area_bin1   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Lot_Area_bin2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, …\n$ Lot_Area_bin3   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ Lot_Area_bin4   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, …\n$ Lot_Area_bin5   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ Lot_Area_bin6   &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ Year_Built_bin1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year_Built_bin2 &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year_Built_bin3 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year_Built_bin4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ Year_Built_bin5 &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, …\n$ Year_Built_bin6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …"
  },
  {
    "objectID": "numeric-binning.html#python-examples",
    "href": "numeric-binning.html#python-examples",
    "title": "14  Binning",
    "section": "14.3 Python Examples",
    "text": "14.3 Python Examples"
  },
  {
    "objectID": "numeric-splines.html#pros-and-cons",
    "href": "numeric-splines.html#pros-and-cons",
    "title": "15  Splines",
    "section": "15.1 Pros and Cons",
    "text": "15.1 Pros and Cons\n\n15.1.1 Pros\n\nWorks fast computationally\nGood performance compared to binning\nis good at handling continuous changes in predictors\n\n\n\n15.1.2 Cons\n\narguably less interpretable than binning\ncreates correlated features\ncan produce a lot of variables\nhave a hard time modeling sudden changes in distributions"
  },
  {
    "objectID": "numeric-splines.html#r-examples",
    "href": "numeric-splines.html#r-examples",
    "title": "15  Splines",
    "section": "15.2 R Examples",
    "text": "15.2 R Examples\nWe will be using the ames data set for these examples.\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 × 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ℹ 2,920 more rows\n\n\n{recipes} provides a number of steps to perform spline operations, each of them starts with step_spline_. Let us use a B-spline and a M-spline as examples here:\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area) |&gt;\n  step_spline_monotone(Year_Built)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 20\n$ Lot_Area_01   &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.00…\n$ Lot_Area_02   &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, …\n$ Lot_Area_03   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0…\n$ Lot_Area_04   &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, …\n$ Lot_Area_05   &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0078526499, …\n$ Lot_Area_06   &lt;dbl&gt; 0.000000000, 0.283603274, 0.000000000, 0.507327055, 0.00…\n$ Lot_Area_07   &lt;dbl&gt; 0.73399934, 0.71382012, 0.96474057, 0.48414256, 0.971047…\n$ Lot_Area_08   &lt;dbl&gt; 2.408258e-01, 2.576602e-03, 3.503161e-02, 6.777374e-04, …\n$ Lot_Area_09   &lt;dbl&gt; 2.444735e-02, 3.441535e-09, 2.277849e-04, 0.000000e+00, …\n$ Lot_Area_10   &lt;dbl&gt; 7.274651e-04, 0.000000e+00, 3.035128e-08, 0.000000e+00, …\n$ Year_Built_01 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Year_Built_02 &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1…\n$ Year_Built_03 &lt;dbl&gt; 0.9991483, 0.9995281, 0.9977527, 1.0000000, 1.0000000, 1…\n$ Year_Built_04 &lt;dbl&gt; 0.9041892, 0.9210803, 0.8649607, 0.9884577, 1.0000000, 1…\n$ Year_Built_05 &lt;dbl&gt; 0.20672563, 0.24041792, 0.14882796, 0.54043388, 0.999999…\n$ Year_Built_06 &lt;dbl&gt; 4.792231e-04, 1.169978e-03, 2.995144e-05, 3.881537e-02, …\n$ Year_Built_07 &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 4.491099e-07, …\n$ Year_Built_08 &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.221125…\n$ Year_Built_09 &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, …\n$ Year_Built_10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n\n\nWe can set the deg_free argument to specify how many spline features we want for each of the splines.\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area, deg_free = 3) |&gt;\n  step_spline_monotone(Year_Built, deg_free = 4)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 7\n$ Lot_Area_1   &lt;dbl&gt; 0.31422525, 0.13110895, 0.16045431, 0.12580964, 0.1557218…\n$ Lot_Area_2   &lt;dbl&gt; 0.0521839123, 0.0066461383, 0.0103524317, 0.0060782666, 0…\n$ Lot_Area_3   &lt;dbl&gt; 2.888757e-03, 1.123014e-04, 2.226446e-04, 9.788684e-05, 2…\n$ Year_Built_1 &lt;dbl&gt; 0.9827669, 0.9841047, 0.9798397, 0.9914201, 0.9999212, 0.…\n$ Year_Built_2 &lt;dbl&gt; 0.8614458, 0.8686207, 0.8464715, 0.9129756, 0.9968924, 0.…\n$ Year_Built_3 &lt;dbl&gt; 0.5411581, 0.5539857, 0.5156159, 0.6440229, 0.9532064, 0.…\n$ Year_Built_4 &lt;dbl&gt; 0.1653539, 0.1729990, 0.1508264, 0.2341901, 0.6731684, 0.…\n\n\nThese steps have more arguments, so we can change other things. The B-splines created by step_spline_b() defaults to cubic splines, but we can change that by specifying which polynomial degree with want with the degree argument.\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area, deg_free = 3, degree = 1) |&gt;\n  step_spline_monotone(Year_Built, deg_free = 4)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 7\n$ Lot_Area_1   &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.0000000…\n$ Lot_Area_2   &lt;dbl&gt; 0.89690903, 0.99540159, 0.98247163, 0.99766006, 0.9846078…\n$ Lot_Area_3   &lt;dbl&gt; 0.103090969, 0.004598405, 0.017528365, 0.002339940, 0.015…\n$ Year_Built_1 &lt;dbl&gt; 0.9827669, 0.9841047, 0.9798397, 0.9914201, 0.9999212, 0.…\n$ Year_Built_2 &lt;dbl&gt; 0.8614458, 0.8686207, 0.8464715, 0.9129756, 0.9968924, 0.…\n$ Year_Built_3 &lt;dbl&gt; 0.5411581, 0.5539857, 0.5156159, 0.6440229, 0.9532064, 0.…\n$ Year_Built_4 &lt;dbl&gt; 0.1653539, 0.1729990, 0.1508264, 0.2341901, 0.6731684, 0.…"
  },
  {
    "objectID": "numeric-splines.html#python-examples",
    "href": "numeric-splines.html#python-examples",
    "title": "15  Splines",
    "section": "15.3 Python Examples",
    "text": "15.3 Python Examples"
  },
  {
    "objectID": "numeric-polynomial.html#pros-and-cons",
    "href": "numeric-polynomial.html#pros-and-cons",
    "title": "16  Polynomial",
    "section": "16.1 Pros and Cons",
    "text": "16.1 Pros and Cons\n\n16.1.1 Pros\n\nWorks fast computationally\nGood performance compared to binning\nDoesn’t create correlated features\nis good at handling continuous changes in predictors\n\n\n\n16.1.2 Cons\n\narguably less interpretable than binning and splines\ncan produce a lot of variables\nhave a hard time modeling sudden changes in distributions"
  },
  {
    "objectID": "numeric-polynomial.html#r-examples",
    "href": "numeric-polynomial.html#r-examples",
    "title": "16  Polynomial",
    "section": "16.2 R Examples",
    "text": "16.2 R Examples\nWe will be using the ames data set for these examples.\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 × 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ℹ 2,920 more rows\n\n\n{recipes} has the function step_poly() for just this occasion.\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built)\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 4\n$ Lot_Area_poly_1   &lt;dbl&gt; 5.070030e-02, 3.456477e-03, 9.658577e-03, 2.373161e-…\n$ Lot_Area_poly_2   &lt;dbl&gt; -0.052288355, -0.006139895, -0.013560043, -0.0048015…\n$ Year_Built_poly_1 &lt;dbl&gt; -0.0069377547, -0.0063268386, -0.0081595868, -0.0020…\n$ Year_Built_poly_2 &lt;dbl&gt; -0.0188536923, -0.0189190631, -0.0186090288, -0.0183…\n\n\nIf you don’t like the default number of features created, you can use the degree argument to change it.\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built, degree = 5)\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 10\n$ Lot_Area_poly_1   &lt;dbl&gt; 5.070030e-02, 3.456477e-03, 9.658577e-03, 2.373161e-…\n$ Lot_Area_poly_2   &lt;dbl&gt; -0.052288355, -0.006139895, -0.013560043, -0.0048015…\n$ Lot_Area_poly_3   &lt;dbl&gt; 0.0024951091, 0.0067956902, 0.0110336270, 0.00588901…\n$ Lot_Area_poly_4   &lt;dbl&gt; 0.0390305341, -0.0078110499, -0.0092519823, -0.00723…\n$ Lot_Area_poly_5   &lt;dbl&gt; -0.0649379780, 0.0051370320, 0.0004088393, 0.0055404…\n$ Year_Built_poly_1 &lt;dbl&gt; -0.0069377547, -0.0063268386, -0.0081595868, -0.0020…\n$ Year_Built_poly_2 &lt;dbl&gt; -0.0188536923, -0.0189190631, -0.0186090288, -0.0183…\n$ Year_Built_poly_3 &lt;dbl&gt; -0.0031709327, -0.0044248985, -0.0006208212, -0.0124…\n$ Year_Built_poly_4 &lt;dbl&gt; 1.420211e-02, 1.311711e-02, 1.609112e-02, 3.358699e-…\n$ Year_Built_poly_5 &lt;dbl&gt; 0.009938840, 0.011096007, 0.007277173, 0.015173692, …\n\n\nwhile you properly shouldn’t, you can turn off the orthogonal polynomials by setting options = list(raw = TRUE).\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built, options = list(raw = TRUE))\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 4\n$ Lot_Area_poly_1   &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005,…\n$ Lot_Area_poly_2   &lt;dbl&gt; 1009332900, 135070884, 203547289, 124545600, 1912689…\n$ Year_Built_poly_1 &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995…\n$ Year_Built_poly_2 &lt;dbl&gt; 3841600, 3845521, 3833764, 3873024, 3988009, 3992004…"
  },
  {
    "objectID": "numeric-polynomial.html#python-examples",
    "href": "numeric-polynomial.html#python-examples",
    "title": "16  Polynomial",
    "section": "16.3 Python Examples",
    "text": "16.3 Python Examples"
  },
  {
    "objectID": "categorical.html#categorical-to-categorical",
    "href": "categorical.html#categorical-to-categorical",
    "title": "17  Overview",
    "section": "17.1 Categorical to Categorical",
    "text": "17.1 Categorical to Categorical\nThese methods take a categorical variable and improve them. Whether it means cleaning levels, collapsing levels, or making sure it handles new levels correctly. These Tasks as not always needed depending on the method you are using but they are generally helpful to apply. One method that would have been located here if it wasn’t for the fact that it has a whole section by itself is dealing with missing values as seen in Chapter 63."
  },
  {
    "objectID": "categorical.html#categorical-to-numerical",
    "href": "categorical.html#categorical-to-numerical",
    "title": "17  Overview",
    "section": "17.2 Categorical to Numerical",
    "text": "17.2 Categorical to Numerical\nThe vast majority of the chapters in these chapters concern methods that take a categorical variable and produce one or more numerical variables suitable for modeling. There are quite a lot of different methods, all have upsides and downsides and they will all be explored in the remaining chapters."
  },
  {
    "objectID": "categorical-cleaning.html#r-examples",
    "href": "categorical-cleaning.html#r-examples",
    "title": "18  Cleaning",
    "section": "18.1 R examples",
    "text": "18.1 R examples\nTODO find good data set for these examples\nUse janitor\ntextrecipes::step_clean_levels()"
  },
  {
    "objectID": "categorical-cleaning.html#python-examples",
    "href": "categorical-cleaning.html#python-examples",
    "title": "18  Cleaning",
    "section": "18.2 Python Examples",
    "text": "18.2 Python Examples"
  },
  {
    "objectID": "categorical-unseen.html#r-examples",
    "href": "categorical-unseen.html#r-examples",
    "title": "19  Unseen Levels",
    "section": "19.1 R Examples",
    "text": "19.1 R Examples\nWe will be using the nycflights13 data set. We are downsampling just a bit to only work on the first day and doing a test-train split.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(rsample)\nlibrary(nycflights13)\n\nflights &lt;- flights |&gt;\n  filter(year == 2013, month == 1, day == 1)\n\nset.seed(13630)\nflights_split &lt;- initial_split(flights)\nflights_train &lt;- training(flights_split)\nflights_test &lt;- testing(flights_split)\n\nNow we are doing the cardinal sin by looking at the testing data. But in this case, it is okay because we are doing it for educational purposes.\n\nflights_train |&gt; pull(carrier) |&gt; unique() |&gt; sort()\n\n [1] \"9E\" \"AA\" \"B6\" \"DL\" \"EV\" \"F9\" \"FL\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\nflights_test |&gt; pull(carrier) |&gt; unique() |&gt; sort()\n\n [1] \"9E\" \"AA\" \"AS\" \"B6\" \"DL\" \"EV\" \"FL\" \"HA\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\n\nNotice that the testing data includes the carrier \"AS\" and \"HA\" but the training data doesn’t know that. Let us see what would happen if we were to calculate dummy variables without doing any adjusting.\n\ndummy_spec &lt;- recipe(arr_delay ~ carrier, data = flights_train) |&gt;\n  step_dummy(carrier)\n\ndummy_spec_prepped &lt;- prep(dummy_spec)\n\nbake(dummy_spec_prepped, new_data = flights_test)\n\nWarning: There are new levels in a factor: AS, HA\n\n\n# A tibble: 211 × 12\n   arr_delay carrier_AA carrier_B6 carrier_DL carrier_EV carrier_F9 carrier_FL\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        12          0          0          0          0          0          0\n 2         8          1          0          0          0          0          0\n 3       -14          0          0          0          0          0          0\n 4        -6          0          1          0          0          0          0\n 5        -3          1          0          0          0          0          0\n 6       -33          0          0          1          0          0          0\n 7        -7          1          0          0          0          0          0\n 8         5          0          1          0          0          0          0\n 9        31          1          0          0          0          0          0\n10       -10         NA         NA         NA         NA         NA         NA\n# ℹ 201 more rows\n# ℹ 5 more variables: carrier_MQ &lt;dbl&gt;, carrier_UA &lt;dbl&gt;, carrier_US &lt;dbl&gt;,\n#   carrier_VX &lt;dbl&gt;, carrier_WN &lt;dbl&gt;\n\n\nWe get a warning, and if you look at the rows that were affected we see that it produces NAs. Let us now use the function step_novel() that implements the above-described method.\n\nnovel_spec &lt;- recipe(arr_delay ~ carrier, data = flights_train) |&gt;\n  step_novel(carrier) |&gt;\n  step_dummy(carrier) \n\nnovel_spec_prepped &lt;- prep(novel_spec)\n\nbake(novel_spec_prepped, new_data = flights_test)\n\n# A tibble: 211 × 13\n   arr_delay carrier_AA carrier_B6 carrier_DL carrier_EV carrier_F9 carrier_FL\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        12          0          0          0          0          0          0\n 2         8          1          0          0          0          0          0\n 3       -14          0          0          0          0          0          0\n 4        -6          0          1          0          0          0          0\n 5        -3          1          0          0          0          0          0\n 6       -33          0          0          1          0          0          0\n 7        -7          1          0          0          0          0          0\n 8         5          0          1          0          0          0          0\n 9        31          1          0          0          0          0          0\n10       -10          0          0          0          0          0          0\n# ℹ 201 more rows\n# ℹ 6 more variables: carrier_MQ &lt;dbl&gt;, carrier_UA &lt;dbl&gt;, carrier_US &lt;dbl&gt;,\n#   carrier_VX &lt;dbl&gt;, carrier_WN &lt;dbl&gt;, carrier_new &lt;dbl&gt;\n\n\nAnd we see that we get no error or anything."
  },
  {
    "objectID": "categorical-unseen.html#python-examples",
    "href": "categorical-unseen.html#python-examples",
    "title": "19  Unseen Levels",
    "section": "19.2 Python Examples",
    "text": "19.2 Python Examples\nTODO"
  },
  {
    "objectID": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "href": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "title": "20  Dummy Encoding",
    "section": "20.1 Dummy or one-hot encoding",
    "text": "20.1 Dummy or one-hot encoding\nTODO add diagram\nThe terms dummy encoding and one-hot encoding get thrown around interchangeably, but they do have different and distinct meanings. One-hot encoding is when you return k variables when you have k different levels. Like we have shown above\n\n\n     cat dog horse\n[1,]   0   1     0\n[2,]   1   0     0\n[3,]   0   0     1\n[4,]   0   1     0\n[5,]   1   0     0\n\n\nDummy encoding on the other hand returns k-1 variables, where the excluded one typically is the first one.\n\n\n     dog horse\n[1,]   1     0\n[2,]   0     0\n[3,]   0     1\n[4,]   1     0\n[5,]   0     0\n\n\nThese two encodings store the exact same information, even though the dummy encoding has 1 less column. Because we are able to deduce which observations are cat by finding the rows with all zeros. The main reason why one would use dummy variables is because of what some people call the dummy variable trap. When you use one-hot encoding, you are increasing the likelihood that you run into a collinearity problem. With the above example, if you included an intercept in your model you have that intercept = cat + dog + horse which gives perfect collinearity and would cause some models to error as they aren’t able to handle that.\n\n\n\n\n\n\nNote\n\n\n\nAn intercept is a variable that takes the value 1 for all entries.\n\n\nEven if you don’t include an intercept you could still run into collinearity. Imagine that in addition to the animal variable also creates a one-hot encoding of the home variable taking the two values \"house\" and \"apartment\", you would get the following indicator variables\n\n\n     cat dog horse house apartment\n[1,]   0   1     0     0         1\n[2,]   1   0     0     1         0\n[3,]   0   0     1     0         1\n[4,]   0   1     0     0         1\n[5,]   1   0     0     1         0\n\n\nAnd in this case, we have that house = cat + dog + horse - apartment which again is an example of perfect collinearity. Unless you have a reason to do otherwise I would suggest that you use dummy encoding in your models. Additionally, this leads to slightly smaller models as each categorical variable produces 1 less variable. It is worth noting that the choice between dummy encoding and one-hot encoding does matter for some models such as decision trees. Depending on what types of rules they are able to use. Being able to write animal == \"cat\" is easier then saying animal != \"dog\" & animal != \"horse\". This is unlikely to be an issue as many tree-based models can work with categorical variables directly without the need for encoding."
  },
  {
    "objectID": "categorical-dummy.html#ordered-factors",
    "href": "categorical-dummy.html#ordered-factors",
    "title": "20  Dummy Encoding",
    "section": "20.2 Ordered factors",
    "text": "20.2 Ordered factors\nTODO"
  },
  {
    "objectID": "categorical-dummy.html#contrasts",
    "href": "categorical-dummy.html#contrasts",
    "title": "20  Dummy Encoding",
    "section": "20.3 Contrasts",
    "text": "20.3 Contrasts\nTODO"
  },
  {
    "objectID": "categorical-dummy.html#pros-and-cons",
    "href": "categorical-dummy.html#pros-and-cons",
    "title": "20  Dummy Encoding",
    "section": "20.4 Pros and Cons",
    "text": "20.4 Pros and Cons\n\n20.4.1 Pros\n\nVersatile and commonly used\nEasy interpretation\nWill rarely lead to a decrease in performance\n\n\n\n20.4.2 Cons\n\nDoes require fairly clean categorical levels\nCan be quite memory intensive if you have many levels in your categorical variables and you are unable to use sparse representation\nProvides a complete, but not necessarily compact set of variables"
  },
  {
    "objectID": "categorical-dummy.html#r-examples",
    "href": "categorical-dummy.html#r-examples",
    "title": "20  Dummy Encoding",
    "section": "20.5 R Examples",
    "text": "20.5 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nAnd since MS_SubClass is a factor, we can verify that they match and that all the levels are observed\n\names |&gt; pull(MS_SubClass) |&gt; levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_dummy() step for this, which defaults to creating dummy variables\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 21\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1…\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; 0, 1, 0, 0, 0, 0…\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 1, 0, 1, 1, 1, 1…\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_C_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_I_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n\n\nWe can pull the factor levels for each variable by using tidy(). If a character vector was present in the data set, it would record the observed variables.\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 243 × 3\n   terms       columns                                id         \n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;      \n 1 MS_SubClass One_Story_1945_and_Older               dummy_Bp5vK\n 2 MS_SubClass One_Story_with_Finished_Attic_All_Ages dummy_Bp5vK\n 3 MS_SubClass One_and_Half_Story_Unfinished_All_Ages dummy_Bp5vK\n 4 MS_SubClass One_and_Half_Story_Finished_All_Ages   dummy_Bp5vK\n 5 MS_SubClass Two_Story_1946_and_Newer               dummy_Bp5vK\n 6 MS_SubClass Two_Story_1945_and_Older               dummy_Bp5vK\n 7 MS_SubClass Two_and_Half_Story_All_Ages            dummy_Bp5vK\n 8 MS_SubClass Split_or_Multilevel                    dummy_Bp5vK\n 9 MS_SubClass Split_Foyer                            dummy_Bp5vK\n10 MS_SubClass Duplex_All_Styles_and_Ages             dummy_Bp5vK\n# ℹ 233 more rows\n\n\nsetting one_hot = TRUE gives us the complete one-hot encoding results.\n\nonehot_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  prep()\n\nonehot_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 23\n$ MS_SubClass_One_Story_1946_and_Newer_All_Styles       &lt;dbl&gt; 1, 1, 1, 1, 0, 0…\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1…\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Floating_Village_Residential                &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; 0, 1, 0, 0, 0, 0…\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 1, 0, 1, 1, 1, 1…\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_C_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_I_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "categorical-dummy.html#python-examples",
    "href": "categorical-dummy.html#python-examples",
    "title": "20  Dummy Encoding",
    "section": "20.6 Python Examples",
    "text": "20.6 Python Examples\nTODO"
  },
  {
    "objectID": "categorical-label.html#pros-and-cons",
    "href": "categorical-label.html#pros-and-cons",
    "title": "21  Label Encoding",
    "section": "21.1 Pros and Cons",
    "text": "21.1 Pros and Cons\n\n21.1.1 Pros\n\nOnly produces a single numeric variable for each categorical variable\nHas a way to handle unseen levels, although poorly\n\n\n\n21.1.2 Cons\n\nOrdering of the levels matters a lot!\nWill very often give inferior performance compared to other methods."
  },
  {
    "objectID": "categorical-label.html#r-examples",
    "href": "categorical-label.html#r-examples",
    "title": "21  Label Encoding",
    "section": "21.2 R Examples",
    "text": "21.2 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nLooking at the levels of MS_SubClass we see that levels are set in a specific way. It isn’t alphabetical, but there isn’t one clear order. No clarification of the ordering can be done in the data documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt.\n\names |&gt; pull(MS_SubClass) |&gt; levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_integer() step for this, which defaults to 1-based indexing\n\nlabel_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_integer(all_nominal_predictors()) |&gt;\n  prep()\n\nlabel_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\"))\n\n# A tibble: 2,930 × 2\n   MS_SubClass MS_Zoning\n         &lt;int&gt;     &lt;int&gt;\n 1           1         3\n 2           1         2\n 3           1         3\n 4           1         3\n 5           6         3\n 6           6         3\n 7          12         3\n 8          12         3\n 9          12         3\n10           6         3\n# ℹ 2,920 more rows"
  },
  {
    "objectID": "categorical-label.html#python-examples",
    "href": "categorical-label.html#python-examples",
    "title": "21  Label Encoding",
    "section": "21.3 Python Examples",
    "text": "21.3 Python Examples"
  },
  {
    "objectID": "categorical-ordinal.html#pros-and-cons",
    "href": "categorical-ordinal.html#pros-and-cons",
    "title": "22  Ordinal Encoding",
    "section": "22.1 Pros and Cons",
    "text": "22.1 Pros and Cons\n\n22.1.1 Pros\n\nOnly produces a single numeric variable for each categorical variable\nPreserves the natural ordering of ordered values\n\n\n\n22.1.2 Cons\n\nWill very often give inferior performance compared to other methods\nUnseen levels need to be manually specified"
  },
  {
    "objectID": "categorical-ordinal.html#r-examples",
    "href": "categorical-ordinal.html#r-examples",
    "title": "22  Ordinal Encoding",
    "section": "22.2 R Examples",
    "text": "22.2 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Shape, Land_Slope)\n\n# A tibble: 2,930 × 2\n   Lot_Shape          Land_Slope\n   &lt;fct&gt;              &lt;fct&gt;     \n 1 Slightly_Irregular Gtl       \n 2 Regular            Gtl       \n 3 Slightly_Irregular Gtl       \n 4 Regular            Gtl       \n 5 Slightly_Irregular Gtl       \n 6 Slightly_Irregular Gtl       \n 7 Regular            Gtl       \n 8 Slightly_Irregular Gtl       \n 9 Slightly_Irregular Gtl       \n10 Regular            Gtl       \n# ℹ 2,920 more rows\n\n\nLooking at the levels of Lot_Shape and Land_Slope we see that they match the levels in the documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt. Furthermore, these variables are listed as ordinal, they just aren’t denoted like this in this data set.\n\names |&gt; pull(Lot_Shape) |&gt; levels()\n\n[1] \"Regular\"              \"Slightly_Irregular\"   \"Moderately_Irregular\"\n[4] \"Irregular\"           \n\names |&gt; pull(Land_Slope) |&gt; levels()\n\n[1] \"Gtl\" \"Mod\" \"Sev\"\n\n\nWe will fix that by turning them into ordered factors.\n\names &lt;- ames |&gt;\n  mutate(across(.cols = c(Lot_Shape, Land_Slope), .fns = as.ordered))\n\nto perform ordinal encoding we will use the step_ordinalscore() step. This defaults to giving each level values between 1 and n, much like step_integer().\n\nordinal_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_ordinalscore(Lot_Shape, Land_Slope) |&gt;\n  prep()\n\nordinal_rec |&gt;\n  bake(new_data = NULL, starts_with(\"Lot_Shape\"), starts_with(\"Land_Slope\"))\n\n# A tibble: 2,930 × 2\n   Lot_Shape Land_Slope\n       &lt;int&gt;      &lt;int&gt;\n 1         2          1\n 2         1          1\n 3         2          1\n 4         1          1\n 5         2          1\n 6         2          1\n 7         1          1\n 8         2          1\n 9         2          1\n10         1          1\n# ℹ 2,920 more rows\n\n\nWhat we can do is define a special transformation function for each of the steps. One way is to use the case_when() function\n\nLot_Shape_transformer &lt;- function(x) {\n  case_when(\n    x == \"Regular\" ~ 0, \n    x == \"Slightly_Irregular\" ~ -1,\n    x == \"Moderately_Irregular\" ~ -5,\n    x == \"Irregular\" ~ -10\n  )\n}\n\nIf you have the values for each of the levels as a vector or data, you can write the function to use that information as well.\n\nLand_Slope_values &lt;- c(Gtl = 0, Mod = 1, Sev = 5)\n\nLand_Slope_transformer &lt;- function(x) {\n  Land_Slope_values[x]\n}\n\nWith these functions, we can now apply them to the respective columns by using the convert argument.\n\nordinal_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_ordinalscore(Lot_Shape, convert = Lot_Shape_transformer) |&gt;\n  step_ordinalscore(Land_Slope, convert = Land_Slope_transformer) |&gt;\n  prep()\n\nordinal_rec |&gt;\n  bake(new_data = NULL, starts_with(\"Lot_Shape\"), starts_with(\"Land_Slope\")) |&gt;\n  distinct()\n\n# A tibble: 11 × 2\n   Lot_Shape Land_Slope\n       &lt;int&gt;      &lt;int&gt;\n 1        -1          0\n 2         0          0\n 3        -5          1\n 4        -1          1\n 5         0          1\n 6        -5          0\n 7         0          5\n 8        -1          5\n 9       -10          0\n10        -5          5\n11       -10          5"
  },
  {
    "objectID": "categorical-ordinal.html#python-examples",
    "href": "categorical-ordinal.html#python-examples",
    "title": "22  Ordinal Encoding",
    "section": "22.3 Python Examples",
    "text": "22.3 Python Examples"
  },
  {
    "objectID": "categorical-binary.html#pros-and-cons",
    "href": "categorical-binary.html#pros-and-cons",
    "title": "23  Binary Encoding",
    "section": "23.1 Pros and Cons",
    "text": "23.1 Pros and Cons\n\n23.1.1 Pros\n\nuses fewer variables to store the same information as dummy encoding\n\n\n\n23.1.2 Cons\n\nLess interpretability compared to dummy variables"
  },
  {
    "objectID": "categorical-binary.html#r-examples",
    "href": "categorical-binary.html#r-examples",
    "title": "23  Binary Encoding",
    "section": "23.2 R Examples",
    "text": "23.2 R Examples\nWe will be using the ames data set for these examples. The step_encoding_binary() function from the extrasteps package allows us to perform binary encoding.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nWe can then apply binary encoding using step_encoding_binary(). Notice how we only get 1 numeric variable for each categorical variable\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_encoding_binary(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 9\n$ MS_SubClass_1  &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1…\n$ MS_SubClass_2  &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0…\n$ MS_SubClass_4  &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0…\n$ MS_SubClass_8  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ MS_SubClass_16 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_1    &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ MS_Zoning_2    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ MS_Zoning_4    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_8    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nWe can pull the number of distinct levels of each variable by using tidy().\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 40 × 3\n   terms        value id                   \n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;                \n 1 MS_SubClass     16 encoding_binary_Bp5vK\n 2 MS_Zoning        7 encoding_binary_Bp5vK\n 3 Street           2 encoding_binary_Bp5vK\n 4 Alley            3 encoding_binary_Bp5vK\n 5 Lot_Shape        4 encoding_binary_Bp5vK\n 6 Land_Contour     4 encoding_binary_Bp5vK\n 7 Utilities        3 encoding_binary_Bp5vK\n 8 Lot_Config       5 encoding_binary_Bp5vK\n 9 Land_Slope       3 encoding_binary_Bp5vK\n10 Neighborhood    29 encoding_binary_Bp5vK\n# ℹ 30 more rows"
  },
  {
    "objectID": "categorical-binary.html#python-examples",
    "href": "categorical-binary.html#python-examples",
    "title": "23  Binary Encoding",
    "section": "23.3 Python Examples",
    "text": "23.3 Python Examples\nhttp://contrib.scikit-learn.org/category_encoders/binary.html"
  },
  {
    "objectID": "categorical-frequency.html#pros-and-cons",
    "href": "categorical-frequency.html#pros-and-cons",
    "title": "24  Frequency Encoding",
    "section": "24.1 Pros and Cons",
    "text": "24.1 Pros and Cons\n\n24.1.1 Pros\n\nPowerful and simple when used correctly\nHigh interpretability\n\n\n\n24.1.2 Cons\n\nIs not able to distinguish between two levels that have the same frequency\nMay not add predictive power"
  },
  {
    "objectID": "categorical-frequency.html#r-examples",
    "href": "categorical-frequency.html#r-examples",
    "title": "24  Frequency Encoding",
    "section": "24.2 R Examples",
    "text": "24.2 R Examples\nWe will be using the ames data set for these examples. The step_encoding_frequency() function from the extrasteps package allows us to perform frequency encoding.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nWe can then apply frequency encoding using step_encoding_frequency(). Notice how we only get 1 numeric variable for each categorical variable\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_encoding_frequency(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 2\n$ MS_SubClass &lt;dbl&gt; 0.36825939, 0.36825939, 0.36825939, 0.36825939, 0.19624573…\n$ MS_Zoning   &lt;dbl&gt; 0.775767918, 0.009215017, 0.775767918, 0.775767918, 0.7757…\n\n\nWe can pull the frequencies for each level of each variable by using tidy().\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 283 × 4\n   terms       level                                  frequency id              \n   &lt;chr&gt;       &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;           \n 1 MS_SubClass One_Story_1946_and_Newer_All_Styles      0.368   encoding_freque…\n 2 MS_SubClass One_Story_1945_and_Older                 0.0474  encoding_freque…\n 3 MS_SubClass One_Story_with_Finished_Attic_All_Ages   0.00205 encoding_freque…\n 4 MS_SubClass One_and_Half_Story_Unfinished_All_Ages   0.00614 encoding_freque…\n 5 MS_SubClass One_and_Half_Story_Finished_All_Ages     0.0980  encoding_freque…\n 6 MS_SubClass Two_Story_1946_and_Newer                 0.196   encoding_freque…\n 7 MS_SubClass Two_Story_1945_and_Older                 0.0437  encoding_freque…\n 8 MS_SubClass Two_and_Half_Story_All_Ages              0.00785 encoding_freque…\n 9 MS_SubClass Split_or_Multilevel                      0.0403  encoding_freque…\n10 MS_SubClass Split_Foyer                              0.0164  encoding_freque…\n# ℹ 273 more rows"
  },
  {
    "objectID": "categorical-frequency.html#python-examples",
    "href": "categorical-frequency.html#python-examples",
    "title": "24  Frequency Encoding",
    "section": "24.3 Python Examples",
    "text": "24.3 Python Examples"
  },
  {
    "objectID": "categorical-target.html#pros-and-cons",
    "href": "categorical-target.html#pros-and-cons",
    "title": "25  Target Encoding",
    "section": "25.1 Pros and Cons",
    "text": "25.1 Pros and Cons\n\n25.1.1 Pros\n\n\n25.1.2 Cons"
  },
  {
    "objectID": "categorical-target.html#r-examples",
    "href": "categorical-target.html#r-examples",
    "title": "25  Target Encoding",
    "section": "25.2 R Examples",
    "text": "25.2 R Examples"
  },
  {
    "objectID": "categorical-target.html#python-examples",
    "href": "categorical-target.html#python-examples",
    "title": "25  Target Encoding",
    "section": "25.3 Python Examples",
    "text": "25.3 Python Examples\n\n\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.” SIGKDD Explor. Newsl. 3 (1): 27–32. https://doi.org/10.1145/507533.507538."
  },
  {
    "objectID": "categorical-hashing.html#pros-and-cons",
    "href": "categorical-hashing.html#pros-and-cons",
    "title": "26  Hashing Encoding",
    "section": "26.1 Pros and Cons",
    "text": "26.1 Pros and Cons\n\n26.1.1 Pros\n\nComputationally fast\nAllows for fixed number of output columns\ngives less sparse output than dummy encoding\n\n\n\n26.1.2 Cons\n\nLoss of interpretability\nStill gives quite sparse output"
  },
  {
    "objectID": "categorical-hashing.html#r-examples",
    "href": "categorical-hashing.html#r-examples",
    "title": "26  Hashing Encoding",
    "section": "26.2 R Examples",
    "text": "26.2 R Examples\nTODO: find higher cardionality data set for this\nWe will be using the ames data set for these examples. The step_dummy_hash() function from the textrecipes package allows us to perform hashing encoding.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(textrecipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Exterior_1st)\n\n# A tibble: 2,930 × 2\n   Sale_Price Exterior_1st\n        &lt;int&gt; &lt;fct&gt;       \n 1     215000 BrkFace     \n 2     105000 VinylSd     \n 3     172000 Wd Sdng     \n 4     244000 BrkFace     \n 5     189900 VinylSd     \n 6     195500 VinylSd     \n 7     213500 CemntBd     \n 8     191500 HdBoard     \n 9     236500 CemntBd     \n10     189000 VinylSd     \n# ℹ 2,920 more rows\n\n\nWe will be using the step_dummy_hash() step for this. For illustrative purposes we will be creating 8 columns, where in practice you would likely want this value higher.\n\ndummy_rec &lt;- recipe(Sale_Price ~ Exterior_1st, data = ames) |&gt;\n  step_dummy_hash(Exterior_1st, num_terms = 8) |&gt;\n  prep()\n\n'as(&lt;dgTMatrix&gt;, \"dgCMatrix\")' is deprecated.\nUse 'as(., \"CsparseMatrix\")' instead.\nSee help(\"Deprecated\") and help(\"Matrix-deprecated\").\n\ndummy_rec |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 9\n$ Sale_Price               &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 19550…\n$ dummyhash_Exterior_1st_1 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1,…\n$ dummyhash_Exterior_1st_2 &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_3 &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_4 &lt;int&gt; -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ dummyhash_Exterior_1st_5 &lt;int&gt; 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0…\n$ dummyhash_Exterior_1st_6 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_7 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_8 &lt;int&gt; 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0…"
  },
  {
    "objectID": "categorical-hashing.html#python-examples",
    "href": "categorical-hashing.html#python-examples",
    "title": "26  Hashing Encoding",
    "section": "26.3 Python Examples",
    "text": "26.3 Python Examples"
  },
  {
    "objectID": "categorical-leaveoneout.html#pros-and-cons",
    "href": "categorical-leaveoneout.html#pros-and-cons",
    "title": "27  Leave One Out Encoding",
    "section": "27.1 Pros and Cons",
    "text": "27.1 Pros and Cons\n\n27.1.1 Pros\n\n\n27.1.2 Cons"
  },
  {
    "objectID": "categorical-leaveoneout.html#r-examples",
    "href": "categorical-leaveoneout.html#r-examples",
    "title": "27  Leave One Out Encoding",
    "section": "27.2 R Examples",
    "text": "27.2 R Examples"
  },
  {
    "objectID": "categorical-leaveoneout.html#python-examples",
    "href": "categorical-leaveoneout.html#python-examples",
    "title": "27  Leave One Out Encoding",
    "section": "27.3 Python Examples",
    "text": "27.3 Python Examples"
  },
  {
    "objectID": "categorical-leaf.html#pros-and-cons",
    "href": "categorical-leaf.html#pros-and-cons",
    "title": "28  Leaf Encoding",
    "section": "28.1 Pros and Cons",
    "text": "28.1 Pros and Cons\n\n28.1.1 Pros\n\n\n28.1.2 Cons"
  },
  {
    "objectID": "categorical-leaf.html#r-examples",
    "href": "categorical-leaf.html#r-examples",
    "title": "28  Leaf Encoding",
    "section": "28.2 R Examples",
    "text": "28.2 R Examples"
  },
  {
    "objectID": "categorical-leaf.html#python-examples",
    "href": "categorical-leaf.html#python-examples",
    "title": "28  Leaf Encoding",
    "section": "28.3 Python Examples",
    "text": "28.3 Python Examples"
  },
  {
    "objectID": "categorical-glmm.html#pros-and-cons",
    "href": "categorical-glmm.html#pros-and-cons",
    "title": "29  GLMM Encoding",
    "section": "29.1 Pros and Cons",
    "text": "29.1 Pros and Cons\n\n29.1.1 Pros\n\n\n29.1.2 Cons"
  },
  {
    "objectID": "categorical-glmm.html#r-examples",
    "href": "categorical-glmm.html#r-examples",
    "title": "29  GLMM Encoding",
    "section": "29.2 R Examples",
    "text": "29.2 R Examples"
  },
  {
    "objectID": "categorical-glmm.html#python-examples",
    "href": "categorical-glmm.html#python-examples",
    "title": "29  GLMM Encoding",
    "section": "29.3 Python Examples",
    "text": "29.3 Python Examples"
  },
  {
    "objectID": "categorical-catboost.html#pros-and-cons",
    "href": "categorical-catboost.html#pros-and-cons",
    "title": "30  Catboost Encoding",
    "section": "30.1 Pros and Cons",
    "text": "30.1 Pros and Cons\n\n30.1.1 Pros\n\n\n30.1.2 Cons"
  },
  {
    "objectID": "categorical-catboost.html#r-examples",
    "href": "categorical-catboost.html#r-examples",
    "title": "30  Catboost Encoding",
    "section": "30.2 R Examples",
    "text": "30.2 R Examples"
  },
  {
    "objectID": "categorical-catboost.html#python-examples",
    "href": "categorical-catboost.html#python-examples",
    "title": "30  Catboost Encoding",
    "section": "30.3 Python Examples",
    "text": "30.3 Python Examples"
  },
  {
    "objectID": "categorical-woe.html#pros-and-cons",
    "href": "categorical-woe.html#pros-and-cons",
    "title": "31  Weight of Evidence Encoding",
    "section": "31.1 Pros and Cons",
    "text": "31.1 Pros and Cons\n\n31.1.1 Pros\n\n\n31.1.2 Cons"
  },
  {
    "objectID": "categorical-woe.html#r-examples",
    "href": "categorical-woe.html#r-examples",
    "title": "31  Weight of Evidence Encoding",
    "section": "31.2 R Examples",
    "text": "31.2 R Examples"
  },
  {
    "objectID": "categorical-woe.html#python-examples",
    "href": "categorical-woe.html#python-examples",
    "title": "31  Weight of Evidence Encoding",
    "section": "31.3 Python Examples",
    "text": "31.3 Python Examples"
  },
  {
    "objectID": "categorical-jamesstein.html#pros-and-cons",
    "href": "categorical-jamesstein.html#pros-and-cons",
    "title": "32  James-Stein Encoding",
    "section": "32.1 Pros and Cons",
    "text": "32.1 Pros and Cons\n\n32.1.1 Pros\n\n\n32.1.2 Cons"
  },
  {
    "objectID": "categorical-jamesstein.html#r-examples",
    "href": "categorical-jamesstein.html#r-examples",
    "title": "32  James-Stein Encoding",
    "section": "32.2 R Examples",
    "text": "32.2 R Examples"
  },
  {
    "objectID": "categorical-jamesstein.html#python-examples",
    "href": "categorical-jamesstein.html#python-examples",
    "title": "32  James-Stein Encoding",
    "section": "32.3 Python Examples",
    "text": "32.3 Python Examples"
  },
  {
    "objectID": "categorical-mestimator.html#pros-and-cons",
    "href": "categorical-mestimator.html#pros-and-cons",
    "title": "33  M-Estimator Encoding",
    "section": "33.1 Pros and Cons",
    "text": "33.1 Pros and Cons\n\n33.1.1 Pros\n\n\n33.1.2 Cons"
  },
  {
    "objectID": "categorical-mestimator.html#r-examples",
    "href": "categorical-mestimator.html#r-examples",
    "title": "33  M-Estimator Encoding",
    "section": "33.2 R Examples",
    "text": "33.2 R Examples"
  },
  {
    "objectID": "categorical-mestimator.html#python-examples",
    "href": "categorical-mestimator.html#python-examples",
    "title": "33  M-Estimator Encoding",
    "section": "33.3 Python Examples",
    "text": "33.3 Python Examples"
  },
  {
    "objectID": "categorical-thermometer.html#pros-and-cons",
    "href": "categorical-thermometer.html#pros-and-cons",
    "title": "34  Thermometer Encoding",
    "section": "34.1 Pros and Cons",
    "text": "34.1 Pros and Cons\n\n34.1.1 Pros\n\n\n34.1.2 Cons"
  },
  {
    "objectID": "categorical-thermometer.html#r-examples",
    "href": "categorical-thermometer.html#r-examples",
    "title": "34  Thermometer Encoding",
    "section": "34.2 R Examples",
    "text": "34.2 R Examples"
  },
  {
    "objectID": "categorical-thermometer.html#python-examples",
    "href": "categorical-thermometer.html#python-examples",
    "title": "34  Thermometer Encoding",
    "section": "34.3 Python Examples",
    "text": "34.3 Python Examples"
  },
  {
    "objectID": "categorical-quantile.html#pros-and-cons",
    "href": "categorical-quantile.html#pros-and-cons",
    "title": "35  Quantile Encoding",
    "section": "35.1 Pros and Cons",
    "text": "35.1 Pros and Cons\n\n35.1.1 Pros\n\n\n35.1.2 Cons"
  },
  {
    "objectID": "categorical-quantile.html#r-examples",
    "href": "categorical-quantile.html#r-examples",
    "title": "35  Quantile Encoding",
    "section": "35.2 R Examples",
    "text": "35.2 R Examples"
  },
  {
    "objectID": "categorical-quantile.html#python-examples",
    "href": "categorical-quantile.html#python-examples",
    "title": "35  Quantile Encoding",
    "section": "35.3 Python Examples",
    "text": "35.3 Python Examples"
  },
  {
    "objectID": "categorical-summary.html#pros-and-cons",
    "href": "categorical-summary.html#pros-and-cons",
    "title": "36  Summary Encoding",
    "section": "36.1 Pros and Cons",
    "text": "36.1 Pros and Cons\n\n36.1.1 Pros\n\n\n36.1.2 Cons"
  },
  {
    "objectID": "categorical-summary.html#r-examples",
    "href": "categorical-summary.html#r-examples",
    "title": "36  Summary Encoding",
    "section": "36.2 R Examples",
    "text": "36.2 R Examples"
  },
  {
    "objectID": "categorical-summary.html#python-examples",
    "href": "categorical-summary.html#python-examples",
    "title": "36  Summary Encoding",
    "section": "36.3 Python Examples",
    "text": "36.3 Python Examples"
  },
  {
    "objectID": "categorical-multi-dummy.html#pros-and-cons",
    "href": "categorical-multi-dummy.html#pros-and-cons",
    "title": "37  Multi-Dummy encoding",
    "section": "37.1 Pros and Cons",
    "text": "37.1 Pros and Cons\n\n37.1.1 Pros\n\n\n37.1.2 Cons"
  },
  {
    "objectID": "categorical-multi-dummy.html#r-examples",
    "href": "categorical-multi-dummy.html#r-examples",
    "title": "37  Multi-Dummy encoding",
    "section": "37.2 R Examples",
    "text": "37.2 R Examples"
  },
  {
    "objectID": "categorical-multi-dummy.html#python-examples",
    "href": "categorical-multi-dummy.html#python-examples",
    "title": "37  Multi-Dummy encoding",
    "section": "37.3 Python Examples",
    "text": "37.3 Python Examples"
  },
  {
    "objectID": "categorical-collapse.html#pros-and-cons",
    "href": "categorical-collapse.html#pros-and-cons",
    "title": "38  Collapsing Categories",
    "section": "38.1 Pros and Cons",
    "text": "38.1 Pros and Cons\n\n38.1.1 Pros\n\n\n38.1.2 Cons"
  },
  {
    "objectID": "categorical-collapse.html#r-examples",
    "href": "categorical-collapse.html#r-examples",
    "title": "38  Collapsing Categories",
    "section": "38.2 R Examples",
    "text": "38.2 R Examples"
  },
  {
    "objectID": "categorical-collapse.html#python-examples",
    "href": "categorical-collapse.html#python-examples",
    "title": "38  Collapsing Categories",
    "section": "38.3 Python Examples",
    "text": "38.3 Python Examples"
  },
  {
    "objectID": "categorical-combination.html#pros-and-cons",
    "href": "categorical-combination.html#pros-and-cons",
    "title": "39  Combination",
    "section": "39.1 Pros and Cons",
    "text": "39.1 Pros and Cons\n\n39.1.1 Pros\n\n\n39.1.2 Cons"
  },
  {
    "objectID": "categorical-combination.html#r-examples",
    "href": "categorical-combination.html#r-examples",
    "title": "39  Combination",
    "section": "39.2 R Examples",
    "text": "39.2 R Examples"
  },
  {
    "objectID": "categorical-combination.html#python-examples",
    "href": "categorical-combination.html#python-examples",
    "title": "39  Combination",
    "section": "39.3 Python Examples",
    "text": "39.3 Python Examples"
  },
  {
    "objectID": "datetime.html",
    "href": "datetime.html",
    "title": "40  Overview",
    "section": "",
    "text": "Date and datetime variables are another type of data that can be quite common. This is different than time as we talked about in Chapter 114, as when we talk about time series data, it is typically a series of data points that are related, and we use that inherent structure of the data as the basis for the modeling problem. Here we are talking about predictors, that happen be to expressed as a date or datetime field.\nThese types of data could be the day where the sale took place, or when the taxi started and ended its trip. In some of these cases, it wouldn’t make sense to treat it as a time-series model, but we still wants to be able to pull out valuable information. In many cases, date and datetime variables will be treated as text fields if they are unparsed, and as fancy integer representation. When encoded they typically use integers to denote time since a specific reference point.\nIf date and datetime variables were used directly in a modeling function it will at best use the underlying integer representation, which is unlikely to be useful since it just denotes chronological time. At worst the modeling function will error and complain.\nThe chapters in this section is going to assume that you have parsed the date and datetime variables, and that we are working with those directly.\nWhen we talk about extraction in Chapter 41, what we will be doing is extracting components of the data. So this can be things like year, month, day, hour, minutes and seconds. There are more complicated things like “is this a holiday” or “is it a weekend”. After that, we will go over some more complicated features in Chapter 42. These features will mostly be based on the extraction features from earlier. But it can be things like “closest holiday” and “how long since last Monday”. Lastly we will talk about how many of these features work in a very circular way in Chapter 43. Naturally if we were to model using hours of the day, 1 hour before midnight and 1 hour after midnight are close on the clock but not numerically."
  },
  {
    "objectID": "datetime-extraction.html#pros-and-cons",
    "href": "datetime-extraction.html#pros-and-cons",
    "title": "41  Value extraction",
    "section": "41.1 Pros and Cons",
    "text": "41.1 Pros and Cons\n\n41.1.1 Pros\n\nFast and easy computations\nCan provide good results\n\n\n\n41.1.2 Cons\n\nThe numerical features generated are all increasing with time linearly\nThere are a lot of extractions, and they correlate quite a bit"
  },
  {
    "objectID": "datetime-extraction.html#r-examples",
    "href": "datetime-extraction.html#r-examples",
    "title": "41  Value extraction",
    "section": "41.2 R Examples",
    "text": "41.2 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n✔ broom        1.0.5          ✔ recipes      1.0.8.9000\n✔ dials        1.2.0          ✔ rsample      1.1.1     \n✔ dplyr        1.1.2          ✔ tibble       3.2.1     \n✔ ggplot2      3.4.2          ✔ tidyr        1.3.0     \n✔ infer        1.0.4          ✔ tune         1.1.1     \n✔ modeldata    1.1.0          ✔ workflows    1.1.3     \n✔ parsnip      1.1.0          ✔ workflowsets 1.0.1     \n✔ purrr        1.0.2          ✔ yardstick    1.2.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\n\nRows: 119390 Columns: 32\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): hotel, arrival_date_month, meal, country, market_segment, distrib...\ndbl  (18): is_canceled, lead_time, arrival_date_year, arrival_date_week_numb...\ndate  (1): reservation_status_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nhotel_bookings |&gt;\n  select(reservation_status_date)\n\n# A tibble: 119,390 × 1\n   reservation_status_date\n   &lt;date&gt;                 \n 1 2015-07-01             \n 2 2015-07-01             \n 3 2015-07-02             \n 4 2015-07-02             \n 5 2015-07-03             \n 6 2015-07-03             \n 7 2015-07-03             \n 8 2015-07-03             \n 9 2015-05-06             \n10 2015-04-22             \n# ℹ 119,380 more rows\n\n\n{recipes} provide two steps for date time extraction. step_date() handles dates, and step_time() handles the sub-day time features. The steps work the same way, so we will only show how step_date() works here. A couple of features are selected by default,\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date)\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 5\n$ reservation_status_date       &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, 201…\n$ is_canceled                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0…\n$ reservation_status_date_dow   &lt;fct&gt; Wed, Wed, Thu, Thu, Fri, Fri, Fri, Fri, …\n$ reservation_status_date_month &lt;fct&gt; Jul, Jul, Jul, Jul, Jul, Jul, Jul, Jul, …\n$ reservation_status_date_year  &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015…\n\n\nBut you can use the features argument to specify other types as well\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date, \n            features = c( \"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                          \"quarter\", \"dow\", \"month\"))\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 10\n$ reservation_status_date          &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, …\n$ is_canceled                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0…\n$ reservation_status_date_year     &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ reservation_status_date_doy      &lt;int&gt; 182, 182, 183, 183, 184, 184, 184, 18…\n$ reservation_status_date_week     &lt;int&gt; 26, 26, 27, 27, 27, 27, 27, 27, 18, 1…\n$ reservation_status_date_decimal  &lt;dbl&gt; 2015.496, 2015.496, 2015.499, 2015.49…\n$ reservation_status_date_semester &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2…\n$ reservation_status_date_quarter  &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3…\n$ reservation_status_date_dow      &lt;fct&gt; Wed, Wed, Thu, Thu, Fri, Fri, Fri, Fr…\n$ reservation_status_date_month    &lt;fct&gt; Jul, Jul, Jul, Jul, Jul, Jul, Jul, Ju…\n\n\nfeatures that can be categorical will be so by default, but can be turned off by setting label = FALSE.\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date, \n            features = c( \"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                          \"quarter\", \"dow\", \"month\"), label = FALSE)\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 10\n$ reservation_status_date          &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, …\n$ is_canceled                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0…\n$ reservation_status_date_year     &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ reservation_status_date_doy      &lt;int&gt; 182, 182, 183, 183, 184, 184, 184, 18…\n$ reservation_status_date_week     &lt;int&gt; 26, 26, 27, 27, 27, 27, 27, 27, 18, 1…\n$ reservation_status_date_decimal  &lt;dbl&gt; 2015.496, 2015.496, 2015.499, 2015.49…\n$ reservation_status_date_semester &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2…\n$ reservation_status_date_quarter  &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3…\n$ reservation_status_date_dow      &lt;int&gt; 4, 4, 5, 5, 6, 6, 6, 6, 4, 4, 3, 1, 1…\n$ reservation_status_date_month    &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 5, 4, 6, 7, 7…\n\n\nIf we want to extract holiday features, we can use the step_holiday() function, that uses the {timeDate} library. With known holiday listed in timeDate::listHolidays().\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_holiday(reservation_status_date, \n               holidays = c(\"BoxingDay\", \"CAFamilyDay\", \"JPConstitutionDay\"))\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 5\n$ reservation_status_date                   &lt;date&gt; 2015-07-01, 2015-07-01, 201…\n$ is_canceled                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ reservation_status_date_BoxingDay         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reservation_status_date_CAFamilyDay       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reservation_status_date_JPConstitutionDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "datetime-extraction.html#python-examples",
    "href": "datetime-extraction.html#python-examples",
    "title": "41  Value extraction",
    "section": "41.3 Python Examples",
    "text": "41.3 Python Examples"
  },
  {
    "objectID": "datetime-advanced.html#pros-and-cons",
    "href": "datetime-advanced.html#pros-and-cons",
    "title": "42  Advanced features",
    "section": "42.1 Pros and Cons",
    "text": "42.1 Pros and Cons\n\n42.1.1 Pros\n\n\n42.1.2 Cons"
  },
  {
    "objectID": "datetime-advanced.html#r-examples",
    "href": "datetime-advanced.html#r-examples",
    "title": "42  Advanced features",
    "section": "42.2 R Examples",
    "text": "42.2 R Examples"
  },
  {
    "objectID": "datetime-advanced.html#python-examples",
    "href": "datetime-advanced.html#python-examples",
    "title": "42  Advanced features",
    "section": "42.3 Python Examples",
    "text": "42.3 Python Examples"
  },
  {
    "objectID": "datetime-circular.html#pros-and-cons",
    "href": "datetime-circular.html#pros-and-cons",
    "title": "43  Circular",
    "section": "43.1 Pros and Cons",
    "text": "43.1 Pros and Cons\n\n43.1.1 Pros\n\n\n43.1.2 Cons"
  },
  {
    "objectID": "datetime-circular.html#r-examples",
    "href": "datetime-circular.html#r-examples",
    "title": "43  Circular",
    "section": "43.2 R Examples",
    "text": "43.2 R Examples"
  },
  {
    "objectID": "datetime-circular.html#python-examples",
    "href": "datetime-circular.html#python-examples",
    "title": "43  Circular",
    "section": "43.3 Python Examples",
    "text": "43.3 Python Examples"
  },
  {
    "objectID": "text.html#text-cleaning",
    "href": "text.html#text-cleaning",
    "title": "44  Overview",
    "section": "44.1 Text cleaning",
    "text": "44.1 Text cleaning\nIn Chapter 46, we will look over the ways we take raw text and get it ready for later tasks. This work deals with encoding issues, standardization, cases and sometimes you need to get rid of a lot of unwanted chunks."
  },
  {
    "objectID": "text.html#tokenization",
    "href": "text.html#tokenization",
    "title": "44  Overview",
    "section": "44.2 Tokenization",
    "text": "44.2 Tokenization\nOnce the text is cleaned, we need to split it into a smaller unit of information such that we can count it, this is called tokenization and we will visit that in Chapter 47."
  },
  {
    "objectID": "text.html#modifying-tokens",
    "href": "text.html#modifying-tokens",
    "title": "44  Overview",
    "section": "44.3 Modifying tokens",
    "text": "44.3 Modifying tokens\nOnce you have the data as tokens, once of the things you might want to do is modifying them in various ways. This could things like changing the endings to words or changing the words entirely. We see examples of this in Chapter 48, and Chapter 49."
  },
  {
    "objectID": "text.html#filtering-tokens",
    "href": "text.html#filtering-tokens",
    "title": "44  Overview",
    "section": "44.4 Filtering tokens",
    "text": "44.4 Filtering tokens\nThe tokens you are created might not all be of the same quality. Depending on your choice of tokenizer, there will be reasons for you to remove some of the tokens you have created. We see examples of this in Chapter 50 and Chapter 51."
  },
  {
    "objectID": "text.html#counting-tokens",
    "href": "text.html#counting-tokens",
    "title": "44  Overview",
    "section": "44.5 Counting tokens",
    "text": "44.5 Counting tokens\nWe have gotten to the end of the line and we are ready to turn the tokens into numeric variables we can use. There are many different ways are we look at them in Chapter 52, Chapter 53, Chapter 54, Chapter 55, and Chapter 56."
  },
  {
    "objectID": "text.html#embeddings",
    "href": "text.html#embeddings",
    "title": "44  Overview",
    "section": "44.6 Embeddings",
    "text": "44.6 Embeddings\nAnother way to use text is to work with embeddings, this is another powerful tool that can give you good performance. We look at some of them in Chapter 57 and Chapter 58."
  },
  {
    "objectID": "text-manual.html#pros-and-cons",
    "href": "text-manual.html#pros-and-cons",
    "title": "45  Manual Text Features",
    "section": "45.1 Pros and Cons",
    "text": "45.1 Pros and Cons\n\n45.1.1 Pros\n\n\n45.1.2 Cons"
  },
  {
    "objectID": "text-manual.html#r-examples",
    "href": "text-manual.html#r-examples",
    "title": "45  Manual Text Features",
    "section": "45.2 R Examples",
    "text": "45.2 R Examples"
  },
  {
    "objectID": "text-manual.html#python-examples",
    "href": "text-manual.html#python-examples",
    "title": "45  Manual Text Features",
    "section": "45.3 Python Examples",
    "text": "45.3 Python Examples"
  },
  {
    "objectID": "text-cleaning.html#pros-and-cons",
    "href": "text-cleaning.html#pros-and-cons",
    "title": "46  Text cleaning",
    "section": "46.1 Pros and Cons",
    "text": "46.1 Pros and Cons\n\n46.1.1 Pros\n\n\n46.1.2 Cons"
  },
  {
    "objectID": "text-cleaning.html#r-examples",
    "href": "text-cleaning.html#r-examples",
    "title": "46  Text cleaning",
    "section": "46.2 R Examples",
    "text": "46.2 R Examples"
  },
  {
    "objectID": "text-cleaning.html#python-examples",
    "href": "text-cleaning.html#python-examples",
    "title": "46  Text cleaning",
    "section": "46.3 Python Examples",
    "text": "46.3 Python Examples"
  },
  {
    "objectID": "text-tokenization.html#pros-and-cons",
    "href": "text-tokenization.html#pros-and-cons",
    "title": "47  Tokenization",
    "section": "47.1 Pros and Cons",
    "text": "47.1 Pros and Cons\n\n47.1.1 Pros\n\n\n47.1.2 Cons"
  },
  {
    "objectID": "text-tokenization.html#r-examples",
    "href": "text-tokenization.html#r-examples",
    "title": "47  Tokenization",
    "section": "47.2 R Examples",
    "text": "47.2 R Examples"
  },
  {
    "objectID": "text-tokenization.html#python-examples",
    "href": "text-tokenization.html#python-examples",
    "title": "47  Tokenization",
    "section": "47.3 Python Examples",
    "text": "47.3 Python Examples"
  },
  {
    "objectID": "text-stemming.html#pros-and-cons",
    "href": "text-stemming.html#pros-and-cons",
    "title": "48  Stemming",
    "section": "48.1 Pros and Cons",
    "text": "48.1 Pros and Cons\n\n48.1.1 Pros\n\n\n48.1.2 Cons"
  },
  {
    "objectID": "text-stemming.html#r-examples",
    "href": "text-stemming.html#r-examples",
    "title": "48  Stemming",
    "section": "48.2 R Examples",
    "text": "48.2 R Examples"
  },
  {
    "objectID": "text-stemming.html#python-examples",
    "href": "text-stemming.html#python-examples",
    "title": "48  Stemming",
    "section": "48.3 Python Examples",
    "text": "48.3 Python Examples"
  },
  {
    "objectID": "text-ngrams.html#pros-and-cons",
    "href": "text-ngrams.html#pros-and-cons",
    "title": "49  N-grams",
    "section": "49.1 Pros and Cons",
    "text": "49.1 Pros and Cons\n\n49.1.1 Pros\n\n\n49.1.2 Cons"
  },
  {
    "objectID": "text-ngrams.html#r-examples",
    "href": "text-ngrams.html#r-examples",
    "title": "49  N-grams",
    "section": "49.2 R Examples",
    "text": "49.2 R Examples"
  },
  {
    "objectID": "text-ngrams.html#python-examples",
    "href": "text-ngrams.html#python-examples",
    "title": "49  N-grams",
    "section": "49.3 Python Examples",
    "text": "49.3 Python Examples"
  },
  {
    "objectID": "text-stopwords.html#pros-and-cons",
    "href": "text-stopwords.html#pros-and-cons",
    "title": "50  Stop words",
    "section": "50.1 Pros and Cons",
    "text": "50.1 Pros and Cons\n\n50.1.1 Pros\n\n\n50.1.2 Cons"
  },
  {
    "objectID": "text-stopwords.html#r-examples",
    "href": "text-stopwords.html#r-examples",
    "title": "50  Stop words",
    "section": "50.2 R Examples",
    "text": "50.2 R Examples"
  },
  {
    "objectID": "text-stopwords.html#python-examples",
    "href": "text-stopwords.html#python-examples",
    "title": "50  Stop words",
    "section": "50.3 Python Examples",
    "text": "50.3 Python Examples"
  },
  {
    "objectID": "text-filter.html#pros-and-cons",
    "href": "text-filter.html#pros-and-cons",
    "title": "51  Token filter",
    "section": "51.1 Pros and Cons",
    "text": "51.1 Pros and Cons\n\n51.1.1 Pros\n\n\n51.1.2 Cons"
  },
  {
    "objectID": "text-filter.html#r-examples",
    "href": "text-filter.html#r-examples",
    "title": "51  Token filter",
    "section": "51.2 R Examples",
    "text": "51.2 R Examples"
  },
  {
    "objectID": "text-filter.html#python-examples",
    "href": "text-filter.html#python-examples",
    "title": "51  Token filter",
    "section": "51.3 Python Examples",
    "text": "51.3 Python Examples"
  },
  {
    "objectID": "text-tf.html#pros-and-cons",
    "href": "text-tf.html#pros-and-cons",
    "title": "52  Term Frequency",
    "section": "52.1 Pros and Cons",
    "text": "52.1 Pros and Cons\n\n52.1.1 Pros\n\n\n52.1.2 Cons"
  },
  {
    "objectID": "text-tf.html#r-examples",
    "href": "text-tf.html#r-examples",
    "title": "52  Term Frequency",
    "section": "52.2 R Examples",
    "text": "52.2 R Examples"
  },
  {
    "objectID": "text-tf.html#python-examples",
    "href": "text-tf.html#python-examples",
    "title": "52  Term Frequency",
    "section": "52.3 Python Examples",
    "text": "52.3 Python Examples"
  },
  {
    "objectID": "text-tfidf.html#pros-and-cons",
    "href": "text-tfidf.html#pros-and-cons",
    "title": "53  TF-IDF",
    "section": "53.1 Pros and Cons",
    "text": "53.1 Pros and Cons\n\n53.1.1 Pros\n\n\n53.1.2 Cons"
  },
  {
    "objectID": "text-tfidf.html#r-examples",
    "href": "text-tfidf.html#r-examples",
    "title": "53  TF-IDF",
    "section": "53.2 R Examples",
    "text": "53.2 R Examples"
  },
  {
    "objectID": "text-tfidf.html#python-examples",
    "href": "text-tfidf.html#python-examples",
    "title": "53  TF-IDF",
    "section": "53.3 Python Examples",
    "text": "53.3 Python Examples"
  },
  {
    "objectID": "text-hashing.html#pros-and-cons",
    "href": "text-hashing.html#pros-and-cons",
    "title": "54  Token Hashing",
    "section": "54.1 Pros and Cons",
    "text": "54.1 Pros and Cons\n\n54.1.1 Pros\n\n\n54.1.2 Cons"
  },
  {
    "objectID": "text-hashing.html#r-examples",
    "href": "text-hashing.html#r-examples",
    "title": "54  Token Hashing",
    "section": "54.2 R Examples",
    "text": "54.2 R Examples"
  },
  {
    "objectID": "text-hashing.html#python-examples",
    "href": "text-hashing.html#python-examples",
    "title": "54  Token Hashing",
    "section": "54.3 Python Examples",
    "text": "54.3 Python Examples"
  },
  {
    "objectID": "text-onehot.html#pros-and-cons",
    "href": "text-onehot.html#pros-and-cons",
    "title": "55  Sequence Encoding",
    "section": "55.1 Pros and Cons",
    "text": "55.1 Pros and Cons\n\n55.1.1 Pros\n\n\n55.1.2 Cons"
  },
  {
    "objectID": "text-onehot.html#r-examples",
    "href": "text-onehot.html#r-examples",
    "title": "55  Sequence Encoding",
    "section": "55.2 R Examples",
    "text": "55.2 R Examples"
  },
  {
    "objectID": "text-onehot.html#python-examples",
    "href": "text-onehot.html#python-examples",
    "title": "55  Sequence Encoding",
    "section": "55.3 Python Examples",
    "text": "55.3 Python Examples"
  },
  {
    "objectID": "text-lda.html#pros-and-cons",
    "href": "text-lda.html#pros-and-cons",
    "title": "56  LDA",
    "section": "56.1 Pros and Cons",
    "text": "56.1 Pros and Cons\n\n56.1.1 Pros\n\n\n56.1.2 Cons"
  },
  {
    "objectID": "text-lda.html#r-examples",
    "href": "text-lda.html#r-examples",
    "title": "56  LDA",
    "section": "56.2 R Examples",
    "text": "56.2 R Examples"
  },
  {
    "objectID": "text-lda.html#python-examples",
    "href": "text-lda.html#python-examples",
    "title": "56  LDA",
    "section": "56.3 Python Examples",
    "text": "56.3 Python Examples"
  },
  {
    "objectID": "text-word2vec.html#pros-and-cons",
    "href": "text-word2vec.html#pros-and-cons",
    "title": "57  word2vec",
    "section": "57.1 Pros and Cons",
    "text": "57.1 Pros and Cons\n\n57.1.1 Pros\n\n\n57.1.2 Cons"
  },
  {
    "objectID": "text-word2vec.html#r-examples",
    "href": "text-word2vec.html#r-examples",
    "title": "57  word2vec",
    "section": "57.2 R Examples",
    "text": "57.2 R Examples"
  },
  {
    "objectID": "text-word2vec.html#python-examples",
    "href": "text-word2vec.html#python-examples",
    "title": "57  word2vec",
    "section": "57.3 Python Examples",
    "text": "57.3 Python Examples"
  },
  {
    "objectID": "text-bert.html#pros-and-cons",
    "href": "text-bert.html#pros-and-cons",
    "title": "58  BERT",
    "section": "58.1 Pros and Cons",
    "text": "58.1 Pros and Cons\n\n58.1.1 Pros\n\n\n58.1.2 Cons"
  },
  {
    "objectID": "text-bert.html#r-examples",
    "href": "text-bert.html#r-examples",
    "title": "58  BERT",
    "section": "58.2 R Examples",
    "text": "58.2 R Examples"
  },
  {
    "objectID": "text-bert.html#python-examples",
    "href": "text-bert.html#python-examples",
    "title": "58  BERT",
    "section": "58.3 Python Examples",
    "text": "58.3 Python Examples"
  },
  {
    "objectID": "circular.html",
    "href": "circular.html",
    "title": "59  Overview",
    "section": "",
    "text": "When working with numeric variables, you see many different kind of relationship. Positive, negative, non-linear. But a special type of relationship is the circular type. It is in essence a non-linear relationship, but specifically it relies on the assumption that the begining of the domain behaves the same way as the end. Another assumption in this type of data is that the domain of values are restricted on the left and right side.\nTypical examples of this are type based, such as time of day, day of the week, day of the month, day of the year. If we have an effect, we would image that the end and beginning are similar. Another examples are directions, 1° of a circle is very close to 359° in reality. And the goal of the chapters in this section is to use transformations to make sure they are close numerically in your model.\nTODO find good example of this\nThere are 2 main ways we can handle this. Harmonic calculations using trigonometric functions will be showcased in Chapter 60, Essentially what happens is that we are mapping the features to the unit circle in 2 dimensions. Another intriguing type of methods are using periodic indicators such as splines or other methods. It doesn’t have to be splines, but it if you carefully set the range of the indicators, you can get good performance. Splines are covered in Chapter 61, and the other more general methods are covered in Chapter 62."
  },
  {
    "objectID": "circular-trig.html#pros-and-cons",
    "href": "circular-trig.html#pros-and-cons",
    "title": "60  Trigonometric",
    "section": "60.1 Pros and Cons",
    "text": "60.1 Pros and Cons\n\n60.1.1 Pros\n\n\n60.1.2 Cons"
  },
  {
    "objectID": "circular-trig.html#r-examples",
    "href": "circular-trig.html#r-examples",
    "title": "60  Trigonometric",
    "section": "60.2 R Examples",
    "text": "60.2 R Examples"
  },
  {
    "objectID": "circular-trig.html#python-examples",
    "href": "circular-trig.html#python-examples",
    "title": "60  Trigonometric",
    "section": "60.3 Python Examples",
    "text": "60.3 Python Examples"
  },
  {
    "objectID": "circular-splines.html#pros-and-cons",
    "href": "circular-splines.html#pros-and-cons",
    "title": "61  Period Splines",
    "section": "61.1 Pros and Cons",
    "text": "61.1 Pros and Cons\n\n61.1.1 Pros\n\n\n61.1.2 Cons"
  },
  {
    "objectID": "circular-splines.html#r-examples",
    "href": "circular-splines.html#r-examples",
    "title": "61  Period Splines",
    "section": "61.2 R Examples",
    "text": "61.2 R Examples"
  },
  {
    "objectID": "circular-splines.html#python-examples",
    "href": "circular-splines.html#python-examples",
    "title": "61  Period Splines",
    "section": "61.3 Python Examples",
    "text": "61.3 Python Examples"
  },
  {
    "objectID": "circular-indicators.html#pros-and-cons",
    "href": "circular-indicators.html#pros-and-cons",
    "title": "62  Periodic Indicators",
    "section": "62.1 Pros and Cons",
    "text": "62.1 Pros and Cons\n\n62.1.1 Pros\n\n\n62.1.2 Cons"
  },
  {
    "objectID": "circular-indicators.html#r-examples",
    "href": "circular-indicators.html#r-examples",
    "title": "62  Periodic Indicators",
    "section": "62.2 R Examples",
    "text": "62.2 R Examples"
  },
  {
    "objectID": "circular-indicators.html#python-examples",
    "href": "circular-indicators.html#python-examples",
    "title": "62  Periodic Indicators",
    "section": "62.3 Python Examples",
    "text": "62.3 Python Examples"
  },
  {
    "objectID": "missing.html#imputation",
    "href": "missing.html#imputation",
    "title": "63  Overview",
    "section": "63.1 Imputation",
    "text": "63.1 Imputation\nOne of the most common way of dealing with missing values, is to fill them in with some values. The types of methods that do this can be split into two groups. Simple imputation in Chapter 64 is when you uses the values in the variable to impute its own missing values, which is where mean and mode imputation are found. Anything more complicated than this will be found in Chapter 65. This is where multiple columns are used to determine the type of imputation needed."
  },
  {
    "objectID": "missing.html#indication",
    "href": "missing.html#indication",
    "title": "63  Overview",
    "section": "63.2 Indication",
    "text": "63.2 Indication\nIf you suspect that the data is not missing at random, it might be worthwhile to include the missingness as a indicator in your data. We will see how we can do that in Chapter 66."
  },
  {
    "objectID": "missing.html#removal",
    "href": "missing.html#removal",
    "title": "63  Overview",
    "section": "63.3 Removal",
    "text": "63.3 Removal\nAt a last resort, you might want to remove variables or rows with missing data, we will see how that is done in Chapter 67. This chapter is put last in this section, as it is generally not the preferred action, and all other avenues should be considered before removal is done."
  },
  {
    "objectID": "missing-simple.html#pros-and-cons",
    "href": "missing-simple.html#pros-and-cons",
    "title": "64  Simple Imputation",
    "section": "64.1 Pros and Cons",
    "text": "64.1 Pros and Cons\n\n64.1.1 Pros\n\n\n64.1.2 Cons"
  },
  {
    "objectID": "missing-simple.html#r-examples",
    "href": "missing-simple.html#r-examples",
    "title": "64  Simple Imputation",
    "section": "64.2 R Examples",
    "text": "64.2 R Examples"
  },
  {
    "objectID": "missing-simple.html#python-examples",
    "href": "missing-simple.html#python-examples",
    "title": "64  Simple Imputation",
    "section": "64.3 Python Examples",
    "text": "64.3 Python Examples"
  },
  {
    "objectID": "missing-model.html#pros-and-cons",
    "href": "missing-model.html#pros-and-cons",
    "title": "65  Model Based Imputation",
    "section": "65.1 Pros and Cons",
    "text": "65.1 Pros and Cons\n\n65.1.1 Pros\n\n\n65.1.2 Cons"
  },
  {
    "objectID": "missing-model.html#r-examples",
    "href": "missing-model.html#r-examples",
    "title": "65  Model Based Imputation",
    "section": "65.2 R Examples",
    "text": "65.2 R Examples"
  },
  {
    "objectID": "missing-model.html#python-examples",
    "href": "missing-model.html#python-examples",
    "title": "65  Model Based Imputation",
    "section": "65.3 Python Examples",
    "text": "65.3 Python Examples"
  },
  {
    "objectID": "missing-indicator.html#pros-and-cons",
    "href": "missing-indicator.html#pros-and-cons",
    "title": "66  Remove Missing Values",
    "section": "66.1 Pros and Cons",
    "text": "66.1 Pros and Cons\n\n66.1.1 Pros\n\n\n66.1.2 Cons"
  },
  {
    "objectID": "missing-indicator.html#r-examples",
    "href": "missing-indicator.html#r-examples",
    "title": "66  Remove Missing Values",
    "section": "66.2 R Examples",
    "text": "66.2 R Examples"
  },
  {
    "objectID": "missing-indicator.html#python-examples",
    "href": "missing-indicator.html#python-examples",
    "title": "66  Remove Missing Values",
    "section": "66.3 Python Examples",
    "text": "66.3 Python Examples"
  },
  {
    "objectID": "missing-remove.html#pros-and-cons",
    "href": "missing-remove.html#pros-and-cons",
    "title": "67  Remove Missing Values",
    "section": "67.1 Pros and Cons",
    "text": "67.1 Pros and Cons\n\n67.1.1 Pros\n\n\n67.1.2 Cons"
  },
  {
    "objectID": "missing-remove.html#r-examples",
    "href": "missing-remove.html#r-examples",
    "title": "67  Remove Missing Values",
    "section": "67.2 R Examples",
    "text": "67.2 R Examples"
  },
  {
    "objectID": "missing-remove.html#python-examples",
    "href": "missing-remove.html#python-examples",
    "title": "67  Remove Missing Values",
    "section": "67.3 Python Examples",
    "text": "67.3 Python Examples"
  },
  {
    "objectID": "too-many.html#non-zero-variance-filtering",
    "href": "too-many.html#non-zero-variance-filtering",
    "title": "68  Overview",
    "section": "68.1 Non-zero Variance filtering",
    "text": "68.1 Non-zero Variance filtering\nThese types of methods are quite simple, we remove variables that takes a few number of values. If the value is always 1 then it doesn’t have any information in it and we should remove. It the variables are almost always the same we might want to remove them. We look at these methods in Chapter 69."
  },
  {
    "objectID": "too-many.html#dimensionality-reduction",
    "href": "too-many.html#dimensionality-reduction",
    "title": "68  Overview",
    "section": "68.2 Dimensionality reduction",
    "text": "68.2 Dimensionality reduction\nThe bulk of the chapter will be in this category. This books categorizes dimensionality reduction methods as methods where a calculation is done on a number of features, with the same or fewer features being returned. Remember that we only look at methods that can be used in predictive settings, hence we won’t be talking about t-distributed stochastic neighbor embedding (t-SNE) (https://stats.stackexchange.com/a/584327).\n\nPrincipal Component Analysis (PCA) Chapter 70\nPCA variants Chapter 71\nIndependent Component Analysis (ICA) Chapter 72\nNon-negative matrix factorization (NMF) Chapter 73\nLinear discriminant analysis (LDA) Chapter 74\nGeneralized discriminant analysis (GDA) Chapter 75\nAutoencoders Chapter 76\nUniform Manifold Approximation and Projection (UMAP) Chapter 77\nISOMAP Chapter 78"
  },
  {
    "objectID": "too-many.html#feature-selection",
    "href": "too-many.html#feature-selection",
    "title": "68  Overview",
    "section": "68.3 Feature selection",
    "text": "68.3 Feature selection\nFeature selection on the other hand finds which variables to keep or remove. And then you act on that. This can be done in a couple of different ways. Filter based approaches are covered in Chapter 79, these methods give each feature a score or rank, and then you use this information to select variables. There are many different ways to get these rankings and many will be covered in the chapter. Wrapper based approaches are covered in Chapter 80. These methods iteratively look at subsets of data to try to find the best set. Their main downside is they tend to add a lot of computational overhead as you need to fit your model many times. Lastly we have embedded methods which will be covered in Chapter 81. These methods uses more advanced method, sometimes other models, to do the feature selection."
  },
  {
    "objectID": "too-many-zv.html",
    "href": "too-many-zv.html",
    "title": "69  Zero Variance Filter",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-pca.html",
    "href": "too-many-pca.html",
    "title": "70  Principal Component Analysis",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-pca-variants.html",
    "href": "too-many-pca-variants.html",
    "title": "71  Principal Component Analysis Variants",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-ica.html",
    "href": "too-many-ica.html",
    "title": "72  Independent Component Analysis",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-nmf.html",
    "href": "too-many-nmf.html",
    "title": "73  Non-negative matrix factorization",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-lda.html",
    "href": "too-many-lda.html",
    "title": "74  Linear discriminant analysis",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-gda.html",
    "href": "too-many-gda.html",
    "title": "75  Generalized discriminant analysis",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-autoencoder.html",
    "href": "too-many-autoencoder.html",
    "title": "76  Autoencoders",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-umap.html",
    "href": "too-many-umap.html",
    "title": "77  Uniform Manifold Approximation and Projection",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-isomap.html",
    "href": "too-many-isomap.html",
    "title": "78  ISOMAP",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-filter.html",
    "href": "too-many-filter.html",
    "title": "79  Filter based feature selection",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-wrapper.html",
    "href": "too-many-wrapper.html",
    "title": "80  Wrapper based feature selection",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-embedded.html",
    "href": "too-many-embedded.html",
    "title": "81  embedded based feature selection",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "correlated.html",
    "href": "correlated.html",
    "title": "82  Overview",
    "section": "",
    "text": "Correlation happens when two or more variables contain similar information. We typically refer to correlation when we talk about predictors. This can be a problem for some machine learning models as they don’t perform well with correlated predictors. There are many different ways to calculate the degree of correlation. And those details aren’t going to matter much right now. The important thing is that it can happen. Below we see such examples\n\n\n\n\n\nFigure 82.1: uncorrelated features and correlated feature\n\n\n\n\nThe reason why correlated features are bad for our models is that two correlated features have the potential to share information that is useful. Imagine we are working with strongly correlated variables. Furthermore, we propose that predictor_1 is highly predictive in our model, since predictor_1 and predictor_2 are correlated, we can conclude that predictor_2 would also be highly predictive. The problem then arises when one of these predictors are used, the other predictor will no longer be predictor since they share their information. Another way to think about it, is that we could replace these two predictors with just one predictor with minimal loss of information. This is one of the reasons why we sometimes want to do dimension reduction, as seen in Chapter 68.\nWe will see how we can example the correlation structure to figure out which variables we can eliminate, this is covered in Chapter 83. This is a more specialized version of the methods we cover in Chapter 79 as we are looking at correlation to determine which variablees to remove rather then their relationship to the outcome.\nAnother method that works well in anything PCA related, these are covered in Chapter 70 and Chapter 71. The resulting data coming out of PCA will be uncorrelated."
  },
  {
    "objectID": "correlated-filter.html#pros-and-cons",
    "href": "correlated-filter.html#pros-and-cons",
    "title": "83  High Correlation Filter",
    "section": "83.1 Pros and Cons",
    "text": "83.1 Pros and Cons\n\n83.1.1 Pros\n\n\n83.1.2 Cons"
  },
  {
    "objectID": "correlated-filter.html#r-examples",
    "href": "correlated-filter.html#r-examples",
    "title": "83  High Correlation Filter",
    "section": "83.2 R Examples",
    "text": "83.2 R Examples"
  },
  {
    "objectID": "correlated-filter.html#python-examples",
    "href": "correlated-filter.html#python-examples",
    "title": "83  High Correlation Filter",
    "section": "83.3 Python Examples",
    "text": "83.3 Python Examples"
  },
  {
    "objectID": "outliers.html",
    "href": "outliers.html",
    "title": "84  Overview",
    "section": "",
    "text": "When we talk about outliers, we mean values that are different from the rest of the values. This is typically seen as extreme values. Let us talk about it with an example first. Below is the famous Ames housing data set. We have plotted the living area against the sale price.\n\n\n\n\n\nFigure 84.1: Two groups of points doesn’t appear close to the main group of points.\n\n\n\n\nTwo groups of observations appear to quite far away from the rest of the points. We are in luck as these points are discussed in the data directory. The relevant quote is shown below:\n\nThere are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don’t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately).\n\nThese types of points would typically be called outliers. They are different enough from the rest of the observations. “Different enough” is by itself really hard to define. And we wont try in this book. What we will show is some ways that people define it and let you decide what is best for your data with regards to treatment.\nIn this care, we had enough domain knowledge to be able to determine the reason for these points to be outliers, and how to deal with them. We won’t always be this lucky. These points were outliers in 2 ways. 3 of the points didn’t include the full price of the house, so it could be classified as a error. If we think of this data set as “houses with know full prices” then we could exclude them as not fitting that criteria. The other two houses are outliers in a purely numerical sense. They take values that are much more different than the rest of the observations.\nWhen you remove observations in a systematic way, regardless of if you think they are outliers or not. then you you are limiting the domain where you model is expected to work in. This may be fine or not, it will depend on your specific problem you working on. But be very careful not to remove actual observations from your data.\nWe have a handful of different ways to deal with outliers. the first choice is to not. Some model types doesn’t care about outliers that much. Anything that uses distances are very affected by outliers. Tree based models doesn’t. Some other preprocessing method such as Chapter 13 also isn’t affected by outliers.\nIf you are planning on handling outliers you want to start by identifying them. In Chapter 85 we look at how we can identity and remove outliers. In Section 4.4 we cover numerical transformations that lessen the effect that outliers has on the distribution. Instead of removing the specific observation that has outliers, or transform the whole variable, we can choose to only modify the value of the outliers. We look at method on how to do that in Chapter 86. In addition to all of these methods we can also add additional indicator variables to denote whether an observation is an outlier or not, this is covered in Chapter 87.\nLastly, it might be appropriate to treat the outliers as a separate data set, and fit a specific model to that part of the data."
  },
  {
    "objectID": "outliers-remove.html#pros-and-cons",
    "href": "outliers-remove.html#pros-and-cons",
    "title": "85  Removal",
    "section": "85.1 Pros and Cons",
    "text": "85.1 Pros and Cons\n\n85.1.1 Pros\n\n\n85.1.2 Cons"
  },
  {
    "objectID": "outliers-remove.html#r-examples",
    "href": "outliers-remove.html#r-examples",
    "title": "85  Removal",
    "section": "85.2 R Examples",
    "text": "85.2 R Examples"
  },
  {
    "objectID": "outliers-remove.html#python-examples",
    "href": "outliers-remove.html#python-examples",
    "title": "85  Removal",
    "section": "85.3 Python Examples",
    "text": "85.3 Python Examples"
  },
  {
    "objectID": "outliers-imputation.html#pros-and-cons",
    "href": "outliers-imputation.html#pros-and-cons",
    "title": "86  Imputation",
    "section": "86.1 Pros and Cons",
    "text": "86.1 Pros and Cons\n\n86.1.1 Pros\n\n\n86.1.2 Cons"
  },
  {
    "objectID": "outliers-imputation.html#r-examples",
    "href": "outliers-imputation.html#r-examples",
    "title": "86  Imputation",
    "section": "86.2 R Examples",
    "text": "86.2 R Examples"
  },
  {
    "objectID": "outliers-imputation.html#python-examples",
    "href": "outliers-imputation.html#python-examples",
    "title": "86  Imputation",
    "section": "86.3 Python Examples",
    "text": "86.3 Python Examples"
  },
  {
    "objectID": "outliers-indicate.html#pros-and-cons",
    "href": "outliers-indicate.html#pros-and-cons",
    "title": "87  Indicate",
    "section": "87.1 Pros and Cons",
    "text": "87.1 Pros and Cons\n\n87.1.1 Pros\n\n\n87.1.2 Cons"
  },
  {
    "objectID": "outliers-indicate.html#r-examples",
    "href": "outliers-indicate.html#r-examples",
    "title": "87  Indicate",
    "section": "87.2 R Examples",
    "text": "87.2 R Examples"
  },
  {
    "objectID": "outliers-indicate.html#python-examples",
    "href": "outliers-indicate.html#python-examples",
    "title": "87  Indicate",
    "section": "87.3 Python Examples",
    "text": "87.3 Python Examples"
  },
  {
    "objectID": "imbalenced.html",
    "href": "imbalenced.html",
    "title": "88  Overview",
    "section": "",
    "text": "Whether to keep this chapter in this book or not was considered, as the methods in this section are borderline feature engineering methods.\nThe potential problem with imbalanced data can most easily be seen in a classification setting. Propose we want to predict if an incoming email is spam or not. Furthermore, we assume that the spam-rate is low and around 1% of the incoming emails. If you are not careful, you can easily end up with a model that predicts “not spam” all the time, since it will be correct 100% of the times. This is a common scenario, and it happens all the time. One culprit could be that there isn’t enough information in the minority class to be able to distinguish it from the majority class. And that is okay, not all modeling problems are easy. But your model will do its best anyways.\nThere are a number of different ways to handle imbalanced data, we will list all the ways in broad strokes, and then cover the methods that we could count as feature engineering.\n\nusing the right performance metrics\nweights\nloss functions\nCalibrating the predictions\nSampling the data, can be done naively and with more advanced methods\n\nThe example above showcases why accuracy as a metric isn’t a good choice when the classes are not uniformly represented. So you can look at other metrics, such as precision, recall, ROC-AUC, or Brier score. You will need to know what metric works best for your project.\nAnother way we can handle this is by adding weights, either to the observations directly, or in the modeling framework as class weights. By giving your minority class high enough a weight forces your model to take them into consideration.\nRelated to the last point, some methods allow you the user to pass in custom objective functions, this can also be beneficial.\nEven if your model performs badly by default, your classification model might still have good separation, just not around the 50% cut-off point. Changing the threshold is another way you can overcome imbalanced data set.\nLastly, and the ways will be cover in this book is sampling of the data. There are a number of different methods that we will cover in this book. These methods cluster somehow, so for some groups we only explain the general idea.\nWe can split these methods into two groups, under-sampling methods and over-sampling methods. In the under-sampling methods, we are removing observations and in over-sampling we are adding observations. Adding observations are usually done by basing the new observations on the existing observations, exactly or by interpolation.\nOver-sampling methods we will cover are:\n\nUp-sampling in Chapter 89\nROSE in Chapter 90\nSMOTE in Chapter 91\nSMITE variants in Chapter 92\nBorderline Smote in Chapter 93\nAdaptive Synthetic Algorithm in Chapter 94\n\nUnder-sampling methods we will cover are:\n\nDown-sampling in Chapter 95\nNearMiss in Chapter 96\nTomek Links in Chapter 97\nCondensed Nearest Neighbor in Chapter 98\nEdited Nearest Neighbor in Chapter 99\nInstance Hardness Threshold in Chapter 100\nOne Sided Selection Chapter 101\n\nThere are also some methods that do these methods together. We won’t consider those methods by themselves and instead let you know that you can do over-sampling followed by under-sampling if you choose."
  },
  {
    "objectID": "imbalenced-upsample.html#pros-and-cons",
    "href": "imbalenced-upsample.html#pros-and-cons",
    "title": "89  Up-sampling",
    "section": "89.1 Pros and Cons",
    "text": "89.1 Pros and Cons\n\n89.1.1 Pros\n\n\n89.1.2 Cons"
  },
  {
    "objectID": "imbalenced-upsample.html#r-examples",
    "href": "imbalenced-upsample.html#r-examples",
    "title": "89  Up-sampling",
    "section": "89.2 R Examples",
    "text": "89.2 R Examples"
  },
  {
    "objectID": "imbalenced-upsample.html#python-examples",
    "href": "imbalenced-upsample.html#python-examples",
    "title": "89  Up-sampling",
    "section": "89.3 Python Examples",
    "text": "89.3 Python Examples"
  },
  {
    "objectID": "imbalenced-rose.html#pros-and-cons",
    "href": "imbalenced-rose.html#pros-and-cons",
    "title": "90  ROSE",
    "section": "90.1 Pros and Cons",
    "text": "90.1 Pros and Cons\n\n90.1.1 Pros\n\n\n90.1.2 Cons"
  },
  {
    "objectID": "imbalenced-rose.html#r-examples",
    "href": "imbalenced-rose.html#r-examples",
    "title": "90  ROSE",
    "section": "90.2 R Examples",
    "text": "90.2 R Examples"
  },
  {
    "objectID": "imbalenced-rose.html#python-examples",
    "href": "imbalenced-rose.html#python-examples",
    "title": "90  ROSE",
    "section": "90.3 Python Examples",
    "text": "90.3 Python Examples"
  },
  {
    "objectID": "imbalenced-smote.html#pros-and-cons",
    "href": "imbalenced-smote.html#pros-and-cons",
    "title": "91  SMOTE",
    "section": "91.1 Pros and Cons",
    "text": "91.1 Pros and Cons\n\n91.1.1 Pros\n\n\n91.1.2 Cons"
  },
  {
    "objectID": "imbalenced-smote.html#r-examples",
    "href": "imbalenced-smote.html#r-examples",
    "title": "91  SMOTE",
    "section": "91.2 R Examples",
    "text": "91.2 R Examples"
  },
  {
    "objectID": "imbalenced-smote.html#python-examples",
    "href": "imbalenced-smote.html#python-examples",
    "title": "91  SMOTE",
    "section": "91.3 Python Examples",
    "text": "91.3 Python Examples"
  },
  {
    "objectID": "imbalenced-smote-variants.html#pros-and-cons",
    "href": "imbalenced-smote-variants.html#pros-and-cons",
    "title": "92  SMOTE Variants",
    "section": "92.1 Pros and Cons",
    "text": "92.1 Pros and Cons\n\n92.1.1 Pros\n\n\n92.1.2 Cons"
  },
  {
    "objectID": "imbalenced-smote-variants.html#r-examples",
    "href": "imbalenced-smote-variants.html#r-examples",
    "title": "92  SMOTE Variants",
    "section": "92.2 R Examples",
    "text": "92.2 R Examples"
  },
  {
    "objectID": "imbalenced-smote-variants.html#python-examples",
    "href": "imbalenced-smote-variants.html#python-examples",
    "title": "92  SMOTE Variants",
    "section": "92.3 Python Examples",
    "text": "92.3 Python Examples"
  },
  {
    "objectID": "imbalenced-borderline-smote.html#pros-and-cons",
    "href": "imbalenced-borderline-smote.html#pros-and-cons",
    "title": "93  Borderline SMOTE",
    "section": "93.1 Pros and Cons",
    "text": "93.1 Pros and Cons\n\n93.1.1 Pros\n\n\n93.1.2 Cons"
  },
  {
    "objectID": "imbalenced-borderline-smote.html#r-examples",
    "href": "imbalenced-borderline-smote.html#r-examples",
    "title": "93  Borderline SMOTE",
    "section": "93.2 R Examples",
    "text": "93.2 R Examples"
  },
  {
    "objectID": "imbalenced-borderline-smote.html#python-examples",
    "href": "imbalenced-borderline-smote.html#python-examples",
    "title": "93  Borderline SMOTE",
    "section": "93.3 Python Examples",
    "text": "93.3 Python Examples"
  },
  {
    "objectID": "imbalenced-adasyn.html#pros-and-cons",
    "href": "imbalenced-adasyn.html#pros-and-cons",
    "title": "94  Adaptive Synthetic Algorithm",
    "section": "94.1 Pros and Cons",
    "text": "94.1 Pros and Cons\n\n94.1.1 Pros\n\n\n94.1.2 Cons"
  },
  {
    "objectID": "imbalenced-adasyn.html#r-examples",
    "href": "imbalenced-adasyn.html#r-examples",
    "title": "94  Adaptive Synthetic Algorithm",
    "section": "94.2 R Examples",
    "text": "94.2 R Examples"
  },
  {
    "objectID": "imbalenced-adasyn.html#python-examples",
    "href": "imbalenced-adasyn.html#python-examples",
    "title": "94  Adaptive Synthetic Algorithm",
    "section": "94.3 Python Examples",
    "text": "94.3 Python Examples"
  },
  {
    "objectID": "imbalenced-downsample.html#pros-and-cons",
    "href": "imbalenced-downsample.html#pros-and-cons",
    "title": "95  Down-Sampling",
    "section": "95.1 Pros and Cons",
    "text": "95.1 Pros and Cons\n\n95.1.1 Pros\n\n\n95.1.2 Cons"
  },
  {
    "objectID": "imbalenced-downsample.html#r-examples",
    "href": "imbalenced-downsample.html#r-examples",
    "title": "95  Down-Sampling",
    "section": "95.2 R Examples",
    "text": "95.2 R Examples"
  },
  {
    "objectID": "imbalenced-downsample.html#python-examples",
    "href": "imbalenced-downsample.html#python-examples",
    "title": "95  Down-Sampling",
    "section": "95.3 Python Examples",
    "text": "95.3 Python Examples"
  },
  {
    "objectID": "imbalenced-nearmiss.html#pros-and-cons",
    "href": "imbalenced-nearmiss.html#pros-and-cons",
    "title": "96  Near-Miss",
    "section": "96.1 Pros and Cons",
    "text": "96.1 Pros and Cons\n\n96.1.1 Pros\n\n\n96.1.2 Cons"
  },
  {
    "objectID": "imbalenced-nearmiss.html#r-examples",
    "href": "imbalenced-nearmiss.html#r-examples",
    "title": "96  Near-Miss",
    "section": "96.2 R Examples",
    "text": "96.2 R Examples"
  },
  {
    "objectID": "imbalenced-nearmiss.html#python-examples",
    "href": "imbalenced-nearmiss.html#python-examples",
    "title": "96  Near-Miss",
    "section": "96.3 Python Examples",
    "text": "96.3 Python Examples"
  },
  {
    "objectID": "imbalenced-tomek.html#pros-and-cons",
    "href": "imbalenced-tomek.html#pros-and-cons",
    "title": "97  Tomek Links",
    "section": "97.1 Pros and Cons",
    "text": "97.1 Pros and Cons\n\n97.1.1 Pros\n\n\n97.1.2 Cons"
  },
  {
    "objectID": "imbalenced-tomek.html#r-examples",
    "href": "imbalenced-tomek.html#r-examples",
    "title": "97  Tomek Links",
    "section": "97.2 R Examples",
    "text": "97.2 R Examples"
  },
  {
    "objectID": "imbalenced-tomek.html#python-examples",
    "href": "imbalenced-tomek.html#python-examples",
    "title": "97  Tomek Links",
    "section": "97.3 Python Examples",
    "text": "97.3 Python Examples"
  },
  {
    "objectID": "imbalenced-cnn.html#pros-and-cons",
    "href": "imbalenced-cnn.html#pros-and-cons",
    "title": "98  Condensed Nearest Neighbor",
    "section": "98.1 Pros and Cons",
    "text": "98.1 Pros and Cons\n\n98.1.1 Pros\n\n\n98.1.2 Cons"
  },
  {
    "objectID": "imbalenced-cnn.html#r-examples",
    "href": "imbalenced-cnn.html#r-examples",
    "title": "98  Condensed Nearest Neighbor",
    "section": "98.2 R Examples",
    "text": "98.2 R Examples"
  },
  {
    "objectID": "imbalenced-cnn.html#python-examples",
    "href": "imbalenced-cnn.html#python-examples",
    "title": "98  Condensed Nearest Neighbor",
    "section": "98.3 Python Examples",
    "text": "98.3 Python Examples"
  },
  {
    "objectID": "imbalenced-enn.html#pros-and-cons",
    "href": "imbalenced-enn.html#pros-and-cons",
    "title": "99  Edited Nearest Neighbor",
    "section": "99.1 Pros and Cons",
    "text": "99.1 Pros and Cons\n\n99.1.1 Pros\n\n\n99.1.2 Cons"
  },
  {
    "objectID": "imbalenced-enn.html#r-examples",
    "href": "imbalenced-enn.html#r-examples",
    "title": "99  Edited Nearest Neighbor",
    "section": "99.2 R Examples",
    "text": "99.2 R Examples"
  },
  {
    "objectID": "imbalenced-enn.html#python-examples",
    "href": "imbalenced-enn.html#python-examples",
    "title": "99  Edited Nearest Neighbor",
    "section": "99.3 Python Examples",
    "text": "99.3 Python Examples"
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#pros-and-cons",
    "href": "imbalenced-hardness-threshold.html#pros-and-cons",
    "title": "100  Instance Hardness Threshold",
    "section": "100.1 Pros and Cons",
    "text": "100.1 Pros and Cons\n\n100.1.1 Pros\n\n\n100.1.2 Cons"
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#r-examples",
    "href": "imbalenced-hardness-threshold.html#r-examples",
    "title": "100  Instance Hardness Threshold",
    "section": "100.2 R Examples",
    "text": "100.2 R Examples"
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#python-examples",
    "href": "imbalenced-hardness-threshold.html#python-examples",
    "title": "100  Instance Hardness Threshold",
    "section": "100.3 Python Examples",
    "text": "100.3 Python Examples"
  },
  {
    "objectID": "imbalenced-one-sided.html#pros-and-cons",
    "href": "imbalenced-one-sided.html#pros-and-cons",
    "title": "101  One Sided Selection",
    "section": "101.1 Pros and Cons",
    "text": "101.1 Pros and Cons\n\n101.1.1 Pros\n\n\n101.1.2 Cons"
  },
  {
    "objectID": "imbalenced-one-sided.html#r-examples",
    "href": "imbalenced-one-sided.html#r-examples",
    "title": "101  One Sided Selection",
    "section": "101.2 R Examples",
    "text": "101.2 R Examples"
  },
  {
    "objectID": "imbalenced-one-sided.html#python-examples",
    "href": "imbalenced-one-sided.html#python-examples",
    "title": "101  One Sided Selection",
    "section": "101.3 Python Examples",
    "text": "101.3 Python Examples"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "102  Overview",
    "section": "",
    "text": "Even though we try to cover as many methods as we can in this book. There will always be more methods. This can be types of data that doesn’t fit in any of the other sections in this book, or because they are too domain specific. Nevertheless, this chapter will cover a few methods and techniques we want to show, that doesn’t fit anywhere else. The purpose with these chapters is not solely to teach you about these methods directly, but to try to broaden where you try to find information\n\nWorking with ID variables in Chapter 103\nWorking with colors in Chapter 104\nWorking with zip codes in Chapter 105\nWorking with emails in Chapter 106"
  },
  {
    "objectID": "miscellaneous-id.html#pros-and-cons",
    "href": "miscellaneous-id.html#pros-and-cons",
    "title": "103  IDs",
    "section": "103.1 Pros and Cons",
    "text": "103.1 Pros and Cons\n\n103.1.1 Pros\n\n\n103.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-id.html#r-examples",
    "href": "miscellaneous-id.html#r-examples",
    "title": "103  IDs",
    "section": "103.2 R Examples",
    "text": "103.2 R Examples"
  },
  {
    "objectID": "miscellaneous-id.html#python-examples",
    "href": "miscellaneous-id.html#python-examples",
    "title": "103  IDs",
    "section": "103.3 Python Examples",
    "text": "103.3 Python Examples"
  },
  {
    "objectID": "miscellaneous-color.html#pros-and-cons",
    "href": "miscellaneous-color.html#pros-and-cons",
    "title": "104  Colors",
    "section": "104.1 Pros and Cons",
    "text": "104.1 Pros and Cons\n\n104.1.1 Pros\n\n\n104.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-color.html#r-examples",
    "href": "miscellaneous-color.html#r-examples",
    "title": "104  Colors",
    "section": "104.2 R Examples",
    "text": "104.2 R Examples"
  },
  {
    "objectID": "miscellaneous-color.html#python-examples",
    "href": "miscellaneous-color.html#python-examples",
    "title": "104  Colors",
    "section": "104.3 Python Examples",
    "text": "104.3 Python Examples"
  },
  {
    "objectID": "miscellaneous-zipcodes.html#pros-and-cons",
    "href": "miscellaneous-zipcodes.html#pros-and-cons",
    "title": "105  Zip Codes",
    "section": "105.1 Pros and Cons",
    "text": "105.1 Pros and Cons\n\n105.1.1 Pros\n\n\n105.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-zipcodes.html#r-examples",
    "href": "miscellaneous-zipcodes.html#r-examples",
    "title": "105  Zip Codes",
    "section": "105.2 R Examples",
    "text": "105.2 R Examples"
  },
  {
    "objectID": "miscellaneous-zipcodes.html#python-examples",
    "href": "miscellaneous-zipcodes.html#python-examples",
    "title": "105  Zip Codes",
    "section": "105.3 Python Examples",
    "text": "105.3 Python Examples"
  },
  {
    "objectID": "miscellaneous-email.html#pros-and-cons",
    "href": "miscellaneous-email.html#pros-and-cons",
    "title": "106  Emails",
    "section": "106.1 Pros and Cons",
    "text": "106.1 Pros and Cons\n\n106.1.1 Pros\n\n\n106.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-email.html#r-examples",
    "href": "miscellaneous-email.html#r-examples",
    "title": "106  Emails",
    "section": "106.2 R Examples",
    "text": "106.2 R Examples"
  },
  {
    "objectID": "miscellaneous-email.html#python-examples",
    "href": "miscellaneous-email.html#python-examples",
    "title": "106  Emails",
    "section": "106.3 Python Examples",
    "text": "106.3 Python Examples"
  },
  {
    "objectID": "spatial.html",
    "href": "spatial.html",
    "title": "107  Overview",
    "section": "",
    "text": "When we talk about spatial and geo-spatial feature engineering, we want to focus on transformation and enrichment of the data set, based on spatial information. Typically this will be longitude and latitude based, with the additional information be based on areas and regions such as cities or countries.\nWhat sets these methods apart from some of the other methods we see in this book, is that they almost always requires a reference data set to be able to perform the calculations. If you want to find the closest city to an observation, you need a data set of all the cities and their location. For all the methods in this section, the reader is expected to know how to gather this reference material for their problem.\nWe will split this data up into two types of methods, depending on your spatial information. point based methods and shape based methods.\nIn point based methods, you know the location of your observation, and you calculate where it is in relationship to something else. You could look at distances by finding the distance to a fixed point, or multiple points as covered in Chapter 108. You could find the nearest of something as covered in Chapter 109. These two methods are really different sides to the same coin. Another thing we could do is count the number of occurrences within a given distance or region. This is covered in Chapter 110. By knowing the location of something we are also able to query certain types of information such as “height from sea”, “rainfall in inches in 2000” and so on. We cover these types of methods in Chapter 111. We can also expand some of these concepts and look at spatial embeddings, we will be covered in Chapter 112.\nIn shape based methods, you don’t just have the positioning of your observation, but also its shape. This can be any shape; line, polygon, circle. The methods seen in point based methods can be applied to shape based methods, we just need to be a little more careful when performing the calculations. Since we are given the shape of our observation, there are characteristics we can extract from those that might be useful. We look at how we can incorporate that information in Chapter 113."
  },
  {
    "objectID": "spatial-distance.html#pros-and-cons",
    "href": "spatial-distance.html#pros-and-cons",
    "title": "108  Spatial Distance",
    "section": "108.1 Pros and Cons",
    "text": "108.1 Pros and Cons\n\n108.1.1 Pros\n\n\n108.1.2 Cons"
  },
  {
    "objectID": "spatial-distance.html#r-examples",
    "href": "spatial-distance.html#r-examples",
    "title": "108  Spatial Distance",
    "section": "108.2 R Examples",
    "text": "108.2 R Examples"
  },
  {
    "objectID": "spatial-distance.html#python-examples",
    "href": "spatial-distance.html#python-examples",
    "title": "108  Spatial Distance",
    "section": "108.3 Python Examples",
    "text": "108.3 Python Examples"
  },
  {
    "objectID": "spatial-nearest.html#pros-and-cons",
    "href": "spatial-nearest.html#pros-and-cons",
    "title": "109  Spatial Nearest",
    "section": "109.1 Pros and Cons",
    "text": "109.1 Pros and Cons\n\n109.1.1 Pros\n\n\n109.1.2 Cons"
  },
  {
    "objectID": "spatial-nearest.html#r-examples",
    "href": "spatial-nearest.html#r-examples",
    "title": "109  Spatial Nearest",
    "section": "109.2 R Examples",
    "text": "109.2 R Examples"
  },
  {
    "objectID": "spatial-nearest.html#python-examples",
    "href": "spatial-nearest.html#python-examples",
    "title": "109  Spatial Nearest",
    "section": "109.3 Python Examples",
    "text": "109.3 Python Examples"
  },
  {
    "objectID": "spatial-count.html#pros-and-cons",
    "href": "spatial-count.html#pros-and-cons",
    "title": "110  Spatial Count",
    "section": "110.1 Pros and Cons",
    "text": "110.1 Pros and Cons\n\n110.1.1 Pros\n\n\n110.1.2 Cons"
  },
  {
    "objectID": "spatial-count.html#r-examples",
    "href": "spatial-count.html#r-examples",
    "title": "110  Spatial Count",
    "section": "110.2 R Examples",
    "text": "110.2 R Examples"
  },
  {
    "objectID": "spatial-count.html#python-examples",
    "href": "spatial-count.html#python-examples",
    "title": "110  Spatial Count",
    "section": "110.3 Python Examples",
    "text": "110.3 Python Examples"
  },
  {
    "objectID": "spatial-query.html#pros-and-cons",
    "href": "spatial-query.html#pros-and-cons",
    "title": "111  Spatial Query",
    "section": "111.1 Pros and Cons",
    "text": "111.1 Pros and Cons\n\n111.1.1 Pros\n\n\n111.1.2 Cons"
  },
  {
    "objectID": "spatial-query.html#r-examples",
    "href": "spatial-query.html#r-examples",
    "title": "111  Spatial Query",
    "section": "111.2 R Examples",
    "text": "111.2 R Examples"
  },
  {
    "objectID": "spatial-query.html#python-examples",
    "href": "spatial-query.html#python-examples",
    "title": "111  Spatial Query",
    "section": "111.3 Python Examples",
    "text": "111.3 Python Examples"
  },
  {
    "objectID": "spatial-embedding.html#pros-and-cons",
    "href": "spatial-embedding.html#pros-and-cons",
    "title": "112  Spatial Embedding",
    "section": "112.1 Pros and Cons",
    "text": "112.1 Pros and Cons\n\n112.1.1 Pros\n\n\n112.1.2 Cons"
  },
  {
    "objectID": "spatial-embedding.html#r-examples",
    "href": "spatial-embedding.html#r-examples",
    "title": "112  Spatial Embedding",
    "section": "112.2 R Examples",
    "text": "112.2 R Examples"
  },
  {
    "objectID": "spatial-embedding.html#python-examples",
    "href": "spatial-embedding.html#python-examples",
    "title": "112  Spatial Embedding",
    "section": "112.3 Python Examples",
    "text": "112.3 Python Examples"
  },
  {
    "objectID": "spatial-characteristics.html#pros-and-cons",
    "href": "spatial-characteristics.html#pros-and-cons",
    "title": "113  Spatial Characteristics",
    "section": "113.1 Pros and Cons",
    "text": "113.1 Pros and Cons\n\n113.1.1 Pros\n\n\n113.1.2 Cons"
  },
  {
    "objectID": "spatial-characteristics.html#r-examples",
    "href": "spatial-characteristics.html#r-examples",
    "title": "113  Spatial Characteristics",
    "section": "113.2 R Examples",
    "text": "113.2 R Examples"
  },
  {
    "objectID": "spatial-characteristics.html#python-examples",
    "href": "spatial-characteristics.html#python-examples",
    "title": "113  Spatial Characteristics",
    "section": "113.3 Python Examples",
    "text": "113.3 Python Examples"
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "114  Overview",
    "section": "",
    "text": "A very common type of data is time-series data. This is data where the observations is taken across time with time stamps. This data will inherently be correlated with with itself as the same measurement for the same unit likely isn’t going to change too much. Time series data is typically modeling using different methods than for other predictive modeling for this reason.\nNevertheless there are still some methods that will provide useful for us. It is important to note that there is a time component to this data, which is what we are trying to predict. The observations can happen at regular intervals, once a day, or irregular intervals, each time the engine errors. Different types of data uses different types of models, and makes the feature engineering a little different depending on what you are trying to do.\nThis chapter assumes the user knows how to use time series data and the precautions that are need when working with data of this type.\nTODO: add link to appropriate resources\nOne area of work is modifying the sequence itself. Some of these transformations can be taken from Chapter 4. But there are some metrics that are specific to time series data. Likewise some metrics ar similar to what we do in Chapter 63 and Chapter 84, but special care needs to be taken with time series data.\n\nSmoothing in Chapter 115\nSliding window transformations in Chapter 116\nLog Interval Transformation in Chapter 117\nTime series specific handling of missing values in Chapter 118\nTime series specific handling of outliers in Chapter 119\n\nAnother area of methods is where we extract or modify the sequence or its data to create new variables, this is done with taking the time component into consideration. This will result in breaking about the time component in a decomposition kind of way, or by getting information out of the values themselves. One such example of the latter is found in the datetime section Chapter 40. Particularly like holiday extraction as seen in Chapter 41.\n\nDifferences in Chapter 120\nLagging features in Chapter 121\nrolling window features in Chapter 122\nexpanding window features in Chapter 123\nFourier decomposition in Chapter 124\nWavelet decomposition in Chapter 125"
  },
  {
    "objectID": "time-series-smooth.html#pros-and-cons",
    "href": "time-series-smooth.html#pros-and-cons",
    "title": "115  Smoothing",
    "section": "115.1 Pros and Cons",
    "text": "115.1 Pros and Cons\n\n115.1.1 Pros\n\n\n115.1.2 Cons"
  },
  {
    "objectID": "time-series-smooth.html#r-examples",
    "href": "time-series-smooth.html#r-examples",
    "title": "115  Smoothing",
    "section": "115.2 R Examples",
    "text": "115.2 R Examples"
  },
  {
    "objectID": "time-series-smooth.html#python-examples",
    "href": "time-series-smooth.html#python-examples",
    "title": "115  Smoothing",
    "section": "115.3 Python Examples",
    "text": "115.3 Python Examples"
  },
  {
    "objectID": "time-series-sliding.html#pros-and-cons",
    "href": "time-series-sliding.html#pros-and-cons",
    "title": "116  Sliding",
    "section": "116.1 Pros and Cons",
    "text": "116.1 Pros and Cons\n\n116.1.1 Pros\n\n\n116.1.2 Cons"
  },
  {
    "objectID": "time-series-sliding.html#r-examples",
    "href": "time-series-sliding.html#r-examples",
    "title": "116  Sliding",
    "section": "116.2 R Examples",
    "text": "116.2 R Examples"
  },
  {
    "objectID": "time-series-sliding.html#python-examples",
    "href": "time-series-sliding.html#python-examples",
    "title": "116  Sliding",
    "section": "116.3 Python Examples",
    "text": "116.3 Python Examples"
  },
  {
    "objectID": "time-series-log-interval.html#pros-and-cons",
    "href": "time-series-log-interval.html#pros-and-cons",
    "title": "117  Log Interval",
    "section": "117.1 Pros and Cons",
    "text": "117.1 Pros and Cons\n\n117.1.1 Pros\n\n\n117.1.2 Cons"
  },
  {
    "objectID": "time-series-log-interval.html#r-examples",
    "href": "time-series-log-interval.html#r-examples",
    "title": "117  Log Interval",
    "section": "117.2 R Examples",
    "text": "117.2 R Examples"
  },
  {
    "objectID": "time-series-log-interval.html#python-examples",
    "href": "time-series-log-interval.html#python-examples",
    "title": "117  Log Interval",
    "section": "117.3 Python Examples",
    "text": "117.3 Python Examples"
  },
  {
    "objectID": "time-series-missing.html#pros-and-cons",
    "href": "time-series-missing.html#pros-and-cons",
    "title": "118  Time series Missing values",
    "section": "118.1 Pros and Cons",
    "text": "118.1 Pros and Cons\n\n118.1.1 Pros\n\n\n118.1.2 Cons"
  },
  {
    "objectID": "time-series-missing.html#r-examples",
    "href": "time-series-missing.html#r-examples",
    "title": "118  Time series Missing values",
    "section": "118.2 R Examples",
    "text": "118.2 R Examples"
  },
  {
    "objectID": "time-series-missing.html#python-examples",
    "href": "time-series-missing.html#python-examples",
    "title": "118  Time series Missing values",
    "section": "118.3 Python Examples",
    "text": "118.3 Python Examples"
  },
  {
    "objectID": "time-series-outliers.html#pros-and-cons",
    "href": "time-series-outliers.html#pros-and-cons",
    "title": "119  Time Series outliers",
    "section": "119.1 Pros and Cons",
    "text": "119.1 Pros and Cons\n\n119.1.1 Pros\n\n\n119.1.2 Cons"
  },
  {
    "objectID": "time-series-outliers.html#r-examples",
    "href": "time-series-outliers.html#r-examples",
    "title": "119  Time Series outliers",
    "section": "119.2 R Examples",
    "text": "119.2 R Examples"
  },
  {
    "objectID": "time-series-outliers.html#python-examples",
    "href": "time-series-outliers.html#python-examples",
    "title": "119  Time Series outliers",
    "section": "119.3 Python Examples",
    "text": "119.3 Python Examples"
  },
  {
    "objectID": "time-series-diff.html#pros-and-cons",
    "href": "time-series-diff.html#pros-and-cons",
    "title": "120  Differences",
    "section": "120.1 Pros and Cons",
    "text": "120.1 Pros and Cons\n\n120.1.1 Pros\n\n\n120.1.2 Cons"
  },
  {
    "objectID": "time-series-diff.html#r-examples",
    "href": "time-series-diff.html#r-examples",
    "title": "120  Differences",
    "section": "120.2 R Examples",
    "text": "120.2 R Examples"
  },
  {
    "objectID": "time-series-diff.html#python-examples",
    "href": "time-series-diff.html#python-examples",
    "title": "120  Differences",
    "section": "120.3 Python Examples",
    "text": "120.3 Python Examples"
  },
  {
    "objectID": "time-series-lag.html#pros-and-cons",
    "href": "time-series-lag.html#pros-and-cons",
    "title": "121  Lagging features",
    "section": "121.1 Pros and Cons",
    "text": "121.1 Pros and Cons\n\n121.1.1 Pros\n\n\n121.1.2 Cons"
  },
  {
    "objectID": "time-series-lag.html#r-examples",
    "href": "time-series-lag.html#r-examples",
    "title": "121  Lagging features",
    "section": "121.2 R Examples",
    "text": "121.2 R Examples"
  },
  {
    "objectID": "time-series-lag.html#python-examples",
    "href": "time-series-lag.html#python-examples",
    "title": "121  Lagging features",
    "section": "121.3 Python Examples",
    "text": "121.3 Python Examples"
  },
  {
    "objectID": "time-series-rolling-window.html#pros-and-cons",
    "href": "time-series-rolling-window.html#pros-and-cons",
    "title": "122  Rolling Window",
    "section": "122.1 Pros and Cons",
    "text": "122.1 Pros and Cons\n\n122.1.1 Pros\n\n\n122.1.2 Cons"
  },
  {
    "objectID": "time-series-rolling-window.html#r-examples",
    "href": "time-series-rolling-window.html#r-examples",
    "title": "122  Rolling Window",
    "section": "122.2 R Examples",
    "text": "122.2 R Examples"
  },
  {
    "objectID": "time-series-rolling-window.html#python-examples",
    "href": "time-series-rolling-window.html#python-examples",
    "title": "122  Rolling Window",
    "section": "122.3 Python Examples",
    "text": "122.3 Python Examples"
  },
  {
    "objectID": "time-series-expanding-window.html#pros-and-cons",
    "href": "time-series-expanding-window.html#pros-and-cons",
    "title": "123  Expanding Window",
    "section": "123.1 Pros and Cons",
    "text": "123.1 Pros and Cons\n\n123.1.1 Pros\n\n\n123.1.2 Cons"
  },
  {
    "objectID": "time-series-expanding-window.html#r-examples",
    "href": "time-series-expanding-window.html#r-examples",
    "title": "123  Expanding Window",
    "section": "123.2 R Examples",
    "text": "123.2 R Examples"
  },
  {
    "objectID": "time-series-expanding-window.html#python-examples",
    "href": "time-series-expanding-window.html#python-examples",
    "title": "123  Expanding Window",
    "section": "123.3 Python Examples",
    "text": "123.3 Python Examples"
  },
  {
    "objectID": "time-series-fourier.html#pros-and-cons",
    "href": "time-series-fourier.html#pros-and-cons",
    "title": "124  Fourier Features",
    "section": "124.1 Pros and Cons",
    "text": "124.1 Pros and Cons\n\n124.1.1 Pros\n\n\n124.1.2 Cons"
  },
  {
    "objectID": "time-series-fourier.html#r-examples",
    "href": "time-series-fourier.html#r-examples",
    "title": "124  Fourier Features",
    "section": "124.2 R Examples",
    "text": "124.2 R Examples"
  },
  {
    "objectID": "time-series-fourier.html#python-examples",
    "href": "time-series-fourier.html#python-examples",
    "title": "124  Fourier Features",
    "section": "124.3 Python Examples",
    "text": "124.3 Python Examples"
  },
  {
    "objectID": "time-series-wavelet.html#pros-and-cons",
    "href": "time-series-wavelet.html#pros-and-cons",
    "title": "125  Wavelet",
    "section": "125.1 Pros and Cons",
    "text": "125.1 Pros and Cons\n\n125.1.1 Pros\n\n\n125.1.2 Cons"
  },
  {
    "objectID": "time-series-wavelet.html#r-examples",
    "href": "time-series-wavelet.html#r-examples",
    "title": "125  Wavelet",
    "section": "125.2 R Examples",
    "text": "125.2 R Examples"
  },
  {
    "objectID": "time-series-wavelet.html#python-examples",
    "href": "time-series-wavelet.html#python-examples",
    "title": "125  Wavelet",
    "section": "125.3 Python Examples",
    "text": "125.3 Python Examples"
  },
  {
    "objectID": "image.html#feature-extraction",
    "href": "image.html#feature-extraction",
    "title": "126  Overview",
    "section": "126.1 Feature Extraction",
    "text": "126.1 Feature Extraction\nIn the extraction setting, we take the images and try to extract smaller, hopefully smaller vectors of information. These could be simple statistics or larger and more complicated methods. One does not need to do this right away, and sometimes it is beneficent to apply some of the image modifications methods below before doing the extraction.\n\nEdge detection and corner detection in Chapter 127\ntexture analysis in Chapter 128"
  },
  {
    "objectID": "image.html#image-modification",
    "href": "image.html#image-modification",
    "title": "126  Overview",
    "section": "126.2 Image Modification",
    "text": "126.2 Image Modification\nSometimes the images you get in will not be in the best shape for your task at hand. This could be for various reasons. Applying color changes of different kinds can help highlight the important parts of the image, such that later preprocessing steps or models have a easier time picking up on it. Likewise you might need to scale the data to help the model and well as reduce noise. Lastly you will properly need to resize your images as many deep learning image modes works on fixed input sizes.\n\nGrayscale conversion in Chapter 129\ncolor modifications in Chapter 130\nnoise reduction in Chapter 131\nValue normalization in Chapter 132\nresizing in Chapter 133"
  },
  {
    "objectID": "image.html#augmentation",
    "href": "image.html#augmentation",
    "title": "126  Overview",
    "section": "126.3 Augmentation",
    "text": "126.3 Augmentation\nA common trick in when working with image data is to do augmentation. What we mean with that, is that we do different kinds of transformations to generate new images that contain the same information but in different ways. It creates a larger data set. With with the hopes of increasing the performance and generalization. Being able to detect cat pictures regardless if they are center in the image or not.\n\nChanging brightness in Chapter 134\nShifting, Flipping, Rotation in Chapter 135\nCropping, Scaling in Chapter 136"
  },
  {
    "objectID": "image.html#embeddings",
    "href": "image.html#embeddings",
    "title": "126  Overview",
    "section": "126.4 Embeddings",
    "text": "126.4 Embeddings\nWe can also take advantage of transfer learning. People have fit image deep learning models on many images before us. And some of these trained models can be reused for us. We will look at that in Chapter 137."
  },
  {
    "objectID": "image-edge-corner.html#pros-and-cons",
    "href": "image-edge-corner.html#pros-and-cons",
    "title": "127  Edge and corner detection",
    "section": "127.1 Pros and Cons",
    "text": "127.1 Pros and Cons\n\n127.1.1 Pros\n\n\n127.1.2 Cons"
  },
  {
    "objectID": "image-edge-corner.html#r-examples",
    "href": "image-edge-corner.html#r-examples",
    "title": "127  Edge and corner detection",
    "section": "127.2 R Examples",
    "text": "127.2 R Examples"
  },
  {
    "objectID": "image-edge-corner.html#python-examples",
    "href": "image-edge-corner.html#python-examples",
    "title": "127  Edge and corner detection",
    "section": "127.3 Python Examples",
    "text": "127.3 Python Examples"
  },
  {
    "objectID": "image-texture.html#pros-and-cons",
    "href": "image-texture.html#pros-and-cons",
    "title": "128  Texture Analysis",
    "section": "128.1 Pros and Cons",
    "text": "128.1 Pros and Cons\n\n128.1.1 Pros\n\n\n128.1.2 Cons"
  },
  {
    "objectID": "image-texture.html#r-examples",
    "href": "image-texture.html#r-examples",
    "title": "128  Texture Analysis",
    "section": "128.2 R Examples",
    "text": "128.2 R Examples"
  },
  {
    "objectID": "image-texture.html#python-examples",
    "href": "image-texture.html#python-examples",
    "title": "128  Texture Analysis",
    "section": "128.3 Python Examples",
    "text": "128.3 Python Examples"
  },
  {
    "objectID": "image-grayscale.html#pros-and-cons",
    "href": "image-grayscale.html#pros-and-cons",
    "title": "129  Greyscale conversion",
    "section": "129.1 Pros and Cons",
    "text": "129.1 Pros and Cons\n\n129.1.1 Pros\n\n\n129.1.2 Cons"
  },
  {
    "objectID": "image-grayscale.html#r-examples",
    "href": "image-grayscale.html#r-examples",
    "title": "129  Greyscale conversion",
    "section": "129.2 R Examples",
    "text": "129.2 R Examples"
  },
  {
    "objectID": "image-grayscale.html#python-examples",
    "href": "image-grayscale.html#python-examples",
    "title": "129  Greyscale conversion",
    "section": "129.3 Python Examples",
    "text": "129.3 Python Examples"
  },
  {
    "objectID": "image-colors.html#pros-and-cons",
    "href": "image-colors.html#pros-and-cons",
    "title": "130  Color Modifications",
    "section": "130.1 Pros and Cons",
    "text": "130.1 Pros and Cons\n\n130.1.1 Pros\n\n\n130.1.2 Cons"
  },
  {
    "objectID": "image-colors.html#r-examples",
    "href": "image-colors.html#r-examples",
    "title": "130  Color Modifications",
    "section": "130.2 R Examples",
    "text": "130.2 R Examples"
  },
  {
    "objectID": "image-colors.html#python-examples",
    "href": "image-colors.html#python-examples",
    "title": "130  Color Modifications",
    "section": "130.3 Python Examples",
    "text": "130.3 Python Examples"
  },
  {
    "objectID": "image-noise.html#pros-and-cons",
    "href": "image-noise.html#pros-and-cons",
    "title": "131  Noise Reduction",
    "section": "131.1 Pros and Cons",
    "text": "131.1 Pros and Cons\n\n131.1.1 Pros\n\n\n131.1.2 Cons"
  },
  {
    "objectID": "image-noise.html#r-examples",
    "href": "image-noise.html#r-examples",
    "title": "131  Noise Reduction",
    "section": "131.2 R Examples",
    "text": "131.2 R Examples"
  },
  {
    "objectID": "image-noise.html#python-examples",
    "href": "image-noise.html#python-examples",
    "title": "131  Noise Reduction",
    "section": "131.3 Python Examples",
    "text": "131.3 Python Examples"
  },
  {
    "objectID": "image-normalization.html#pros-and-cons",
    "href": "image-normalization.html#pros-and-cons",
    "title": "132  Value Normalization",
    "section": "132.1 Pros and Cons",
    "text": "132.1 Pros and Cons\n\n132.1.1 Pros\n\n\n132.1.2 Cons"
  },
  {
    "objectID": "image-normalization.html#r-examples",
    "href": "image-normalization.html#r-examples",
    "title": "132  Value Normalization",
    "section": "132.2 R Examples",
    "text": "132.2 R Examples"
  },
  {
    "objectID": "image-normalization.html#python-examples",
    "href": "image-normalization.html#python-examples",
    "title": "132  Value Normalization",
    "section": "132.3 Python Examples",
    "text": "132.3 Python Examples"
  },
  {
    "objectID": "image-resize.html#pros-and-cons",
    "href": "image-resize.html#pros-and-cons",
    "title": "133  Resizing",
    "section": "133.1 Pros and Cons",
    "text": "133.1 Pros and Cons\n\n133.1.1 Pros\n\n\n133.1.2 Cons"
  },
  {
    "objectID": "image-resize.html#r-examples",
    "href": "image-resize.html#r-examples",
    "title": "133  Resizing",
    "section": "133.2 R Examples",
    "text": "133.2 R Examples"
  },
  {
    "objectID": "image-resize.html#python-examples",
    "href": "image-resize.html#python-examples",
    "title": "133  Resizing",
    "section": "133.3 Python Examples",
    "text": "133.3 Python Examples"
  },
  {
    "objectID": "image-brightness.html#pros-and-cons",
    "href": "image-brightness.html#pros-and-cons",
    "title": "134  Changing Brightness",
    "section": "134.1 Pros and Cons",
    "text": "134.1 Pros and Cons\n\n134.1.1 Pros\n\n\n134.1.2 Cons"
  },
  {
    "objectID": "image-brightness.html#r-examples",
    "href": "image-brightness.html#r-examples",
    "title": "134  Changing Brightness",
    "section": "134.2 R Examples",
    "text": "134.2 R Examples"
  },
  {
    "objectID": "image-brightness.html#python-examples",
    "href": "image-brightness.html#python-examples",
    "title": "134  Changing Brightness",
    "section": "134.3 Python Examples",
    "text": "134.3 Python Examples"
  },
  {
    "objectID": "image-shift-flip-rotate.html#pros-and-cons",
    "href": "image-shift-flip-rotate.html#pros-and-cons",
    "title": "135  Shifting, Flipping, and Rotation",
    "section": "135.1 Pros and Cons",
    "text": "135.1 Pros and Cons\n\n135.1.1 Pros\n\n\n135.1.2 Cons"
  },
  {
    "objectID": "image-shift-flip-rotate.html#r-examples",
    "href": "image-shift-flip-rotate.html#r-examples",
    "title": "135  Shifting, Flipping, and Rotation",
    "section": "135.2 R Examples",
    "text": "135.2 R Examples"
  },
  {
    "objectID": "image-shift-flip-rotate.html#python-examples",
    "href": "image-shift-flip-rotate.html#python-examples",
    "title": "135  Shifting, Flipping, and Rotation",
    "section": "135.3 Python Examples",
    "text": "135.3 Python Examples"
  },
  {
    "objectID": "image-crop-scale.html#pros-and-cons",
    "href": "image-crop-scale.html#pros-and-cons",
    "title": "136  Cropping and Scaling",
    "section": "136.1 Pros and Cons",
    "text": "136.1 Pros and Cons\n\n136.1.1 Pros\n\n\n136.1.2 Cons"
  },
  {
    "objectID": "image-crop-scale.html#r-examples",
    "href": "image-crop-scale.html#r-examples",
    "title": "136  Cropping and Scaling",
    "section": "136.2 R Examples",
    "text": "136.2 R Examples"
  },
  {
    "objectID": "image-crop-scale.html#python-examples",
    "href": "image-crop-scale.html#python-examples",
    "title": "136  Cropping and Scaling",
    "section": "136.3 Python Examples",
    "text": "136.3 Python Examples"
  },
  {
    "objectID": "image-embeddings.html#pros-and-cons",
    "href": "image-embeddings.html#pros-and-cons",
    "title": "137  Image embeddings",
    "section": "137.1 Pros and Cons",
    "text": "137.1 Pros and Cons\n\n137.1.1 Pros\n\n\n137.1.2 Cons"
  },
  {
    "objectID": "image-embeddings.html#r-examples",
    "href": "image-embeddings.html#r-examples",
    "title": "137  Image embeddings",
    "section": "137.2 R Examples",
    "text": "137.2 R Examples"
  },
  {
    "objectID": "image-embeddings.html#python-examples",
    "href": "image-embeddings.html#python-examples",
    "title": "137  Image embeddings",
    "section": "137.3 Python Examples",
    "text": "137.3 Python Examples"
  },
  {
    "objectID": "relational.html",
    "href": "relational.html",
    "title": "138  Overview",
    "section": "",
    "text": "So far in this book we have almost exclusively talked about tabular data. with the exception of image data in Chapter 126, and to some degree time series data as seen in Chapter 114. The latter doesn’t quite count as is it typically stored in a tabular way. This section will talk about the scenario where you are working with more than 1 table. A situation that is quite common.\nWhen you have data, one way to store is in a data base. You have a number of smaller tables with information. Once you want to do some kind of modeling, you go to your data base and query out the data so you get it in a tabular format that our modeling tools will accept.\nTODO: add some kind of diagram here\nIt is at this stage we can use some feature engineering tricks. Propose that we are looking at daily sales targets for a number of different store. There will be tables of the individual store performances, who works there, the items the carry and so on. As well as their past behavior. Using the knowledge of these cross tables can be very valuable. We will look at two ways to handle this. Manually in Chapter 139 and automatically in Chapter 140."
  },
  {
    "objectID": "relational-manual.html#pros-and-cons",
    "href": "relational-manual.html#pros-and-cons",
    "title": "139  Manual",
    "section": "139.1 Pros and Cons",
    "text": "139.1 Pros and Cons\n\n139.1.1 Pros\n\n\n139.1.2 Cons"
  },
  {
    "objectID": "relational-manual.html#r-examples",
    "href": "relational-manual.html#r-examples",
    "title": "139  Manual",
    "section": "139.2 R Examples",
    "text": "139.2 R Examples"
  },
  {
    "objectID": "relational-manual.html#python-examples",
    "href": "relational-manual.html#python-examples",
    "title": "139  Manual",
    "section": "139.3 Python Examples",
    "text": "139.3 Python Examples"
  },
  {
    "objectID": "relational-auto.html#pros-and-cons",
    "href": "relational-auto.html#pros-and-cons",
    "title": "140  Automatic",
    "section": "140.1 Pros and Cons",
    "text": "140.1 Pros and Cons\n\n140.1.1 Pros\n\n\n140.1.2 Cons"
  },
  {
    "objectID": "relational-auto.html#r-examples",
    "href": "relational-auto.html#r-examples",
    "title": "140  Automatic",
    "section": "140.2 R Examples",
    "text": "140.2 R Examples"
  },
  {
    "objectID": "relational-auto.html#python-examples",
    "href": "relational-auto.html#python-examples",
    "title": "140  Automatic",
    "section": "140.3 Python Examples",
    "text": "140.3 Python Examples"
  },
  {
    "objectID": "video.html",
    "href": "video.html",
    "title": "141  Overview",
    "section": "",
    "text": "This chapter is still tentative, if you have any methods you need think would fit in this chapter, please post an issue in the Github repository.\nWIP"
  },
  {
    "objectID": "video-tmp.html#pros-and-cons",
    "href": "video-tmp.html#pros-and-cons",
    "title": "142  Temporary",
    "section": "142.1 Pros and Cons",
    "text": "142.1 Pros and Cons\n\n142.1.1 Pros\n\n\n142.1.2 Cons"
  },
  {
    "objectID": "video-tmp.html#r-examples",
    "href": "video-tmp.html#r-examples",
    "title": "142  Temporary",
    "section": "142.2 R Examples",
    "text": "142.2 R Examples"
  },
  {
    "objectID": "video-tmp.html#python-examples",
    "href": "video-tmp.html#python-examples",
    "title": "142  Temporary",
    "section": "142.3 Python Examples",
    "text": "142.3 Python Examples"
  },
  {
    "objectID": "sound.html",
    "href": "sound.html",
    "title": "143  Overview",
    "section": "",
    "text": "This chapter is still tentative, if you have any methods you need think would fit in this chapter, please post an issue in the Github repository.\nWIP"
  },
  {
    "objectID": "sound-tmp.html#pros-and-cons",
    "href": "sound-tmp.html#pros-and-cons",
    "title": "144  Temporary",
    "section": "144.1 Pros and Cons",
    "text": "144.1 Pros and Cons\n\n144.1.1 Pros\n\n\n144.1.2 Cons"
  },
  {
    "objectID": "sound-tmp.html#r-examples",
    "href": "sound-tmp.html#r-examples",
    "title": "144  Temporary",
    "section": "144.2 R Examples",
    "text": "144.2 R Examples"
  },
  {
    "objectID": "sound-tmp.html#python-examples",
    "href": "sound-tmp.html#python-examples",
    "title": "144  Temporary",
    "section": "144.3 Python Examples",
    "text": "144.3 Python Examples"
  },
  {
    "objectID": "order.html",
    "href": "order.html",
    "title": "145  Order of transformations",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "sparse.html",
    "href": "sparse.html",
    "title": "146  What should you do if you have sparse data?",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "147  How Different Models Deal With Input",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "148  Summary",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Micci-Barreca, Daniele. 2001. “A Preprocessing Scheme for\nHigh-Cardinality Categorical Attributes in Classification and Prediction\nProblems.” SIGKDD Explor. Newsl. 3 (1): 27–32. https://doi.org/10.1145/507533.507538."
  }
]
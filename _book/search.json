[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feature Engineering A-Z",
    "section": "",
    "text": "Preface\nWelcome to “Feature Engineering A-Z”! This book is written to be used as a reference guide to nearly all feature engineering methods you will encounter. This is reflected in the chapter structure. Any question a practitioner is having should be answered by looking at the index and finding the right chapter.\nEach section tries to be as comprehensive as possible with the number of different methods and solutions that are presented. A section on dimensionality reduction should list all the practical methods that could be used, as well as a comparison between the methods to help the reader decide what would be most appropriate. This does not mean that all methods are recommended to use. A number of these methods have little and narrow use cases. Methods that are deemed too domain-specific have been excluded from this book.\nEach chapter will cover a specific method or small group of methods. This will include motivations and explanations for the method. Whenever possible each method will be accompanied by mathematical formulas and visualizations to illustrate the mechanics of the method. A small pros and cons list is provided for each method. Lastly, each section will include code snippets showcasing how to implement the methods. This is done in R and Python, using tidymodels and scikit-learn respectively. This book is a methods book first, and a coding book second.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface-1",
    "href": "index.html#preface-1",
    "title": "Feature Engineering A-Z",
    "section": "Preface",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#what-does-this-book-not-cover",
    "href": "index.html#what-does-this-book-not-cover",
    "title": "Feature Engineering A-Z",
    "section": "What does this book not cover?",
    "text": "What does this book not cover?\nTo keep the scope of this book as focused as possible, the following topics will not be covered in this book:\n\nwhole process modeling\ncase studies\ndeployment details\ndomain-specific methods\n\nFor whole process modeling see instead “Hands-On Machine Learning with Scikit-learn, Keras & Tensorflow” (2017), “Tidy modeling with R” (2022), “Approaching (almost) any machine learning problem” (2020) and “Applied Predictive Modeling” (2013) are all great resources. For feature engineering books that tell more of a story by going through case studies, I recommended: “Python Feature Engineering Cookbook” (2020), “Feature Engineering Bookcamp” (2022) And “Feature Engineering and Selection” (2019). I have found that books on deployment domain-specific methods are highly related to the field and stack that you are using and am not able to give broad advice.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#who-is-this-book-for",
    "href": "index.html#who-is-this-book-for",
    "title": "Feature Engineering A-Z",
    "section": "Who is this book for?",
    "text": "Who is this book for?\nThis book is designed to be used by people involved in the modeling of data. These can include but are not limited to data scientists, students, professors, data analysts and machine learning engineers. The reference style nature of the book makes it useful for beginners and seasoned professionals. A background in the basics of modeling, statistics and machine learning would be helpful. Feature engineering as a practice is tightly connected to the rest of the machine learning pipeline so knowledge of the other components is key.\nMany educational resources skip over the finer details of feature engineering methods, which is where this book tries to fill the gap.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Feature Engineering A-Z",
    "section": "License",
    "text": "License\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#rendering-details",
    "href": "index.html#rendering-details",
    "title": "Feature Engineering A-Z",
    "section": "Rendering details",
    "text": "Rendering details\nThis book is rendered using quarto (1.6.42), R (4.4.2) and Python (3.12.8). The website source is hosted on Github.\nThe following R packages are used to render the book, with tidymodels, recipes, embed, themis, textrecipes and timetk being the main packages.\n\n\n\ncorrr (0.4.4)\ndapr (0.0.4)\nembed (1.1.4)\n\n\nextrasteps (0.0.0.9000)\nggforce (0.4.2)\nggraph (2.2.1)\n\n\njaneaustenr (1.0.0)\njsonlite (2.0.0)\nkeras (2.15.0)\n\n\nlme4 (1.1-35.5)\nMatrix (1.7-3)\npatchwork (1.2.0)\n\n\nprismatic (1.1.2)\nreadr (2.1.5)\nremotes (2.5.0)\n\n\nreshape (0.8.9)\nreticulate (1.42.0)\nrmarkdown (2.27)\n\n\nsplines2 (0.5.3)\nstopwords (2.3)\ntext2vec (0.6.4)\n\n\ntextfeatures (0.3.3)\ntextrecipes (1.0.6)\ntfse (0.5.0)\n\n\ntidymodels (1.2.0)\ntidyverse (2.0.0)\n\n\n\n\nThe following Python libraries are used to render the book, with scikit-learn and feature-engine being the main ones.\n\n\n\n\n\n\n\n\nabsl-py (2.2.2)\nappnope (0.1.4)\nasttokens (2.4.1)\n\n\nastunparse (1.6.3)\ncategory-encoders (2.6.3)\ncertifi (2025.1.31)\n\n\ncffi (1.16.0)\ncharset-normalizer (3.4.1)\ncolorama (0.4.6)\n\n\ncomm (0.2.1)\ndebugpy (1.8.1)\ndecorator (5.1.1)\n\n\nexecuting (2.0.1)\nfeature-engine (1.6.2)\nfeazdata (0.0.3)\n\n\nflatbuffers (25.2.10)\ngast (0.6.0)\ngoogle-pasta (0.2.0)\n\n\ngrpcio (1.71.0)\nh5py (3.13.0)\nidna (3.10)\n\n\nipykernel (6.29.2)\nipython (8.21.0)\njedi (0.19.1)\n\n\njoblib (1.3.2)\njupyter-client (8.6.0)\njupyter-core (5.7.1)\n\n\nkeras (3.9.2)\nlibclang (18.1.1)\nmarkdown (3.7)\n\n\nmarkdown-it-py (3.0.0)\nmarkupsafe (3.0.2)\nmatplotlib-inline (0.1.6)\n\n\nmdurl (0.1.2)\nml-dtypes (0.5.1)\nnamex (0.0.8)\n\n\nnest-asyncio (1.6.0)\nnumpy (1.26.4)\nnumpy (2.1.3)\n\n\nopt-einsum (3.4.0)\noptree (0.14.1)\npackaging (23.2)\n\n\npandas (2.2.0)\npandas (2.2.3)\nparso (0.8.3)\n\n\npatsy (0.5.6)\npexpect (4.9.0)\nplatformdirs (4.2.0)\n\n\nprompt-toolkit (3.0.43)\nprotobuf (5.29.4)\npsutil (5.9.8)\n\n\nptyprocess (0.7.0)\npure-eval (0.2.2)\npycparser (2.21)\n\n\npygments (2.17.2)\npython-dateutil (2.8.2)\npytz (2024.1)\n\n\npywin32 (306)\npyyaml (6.0.1)\npyzmq (25.1.2)\n\n\nrequests (2.32.3)\nrich (14.0.0)\nscikit-learn (1.4.0)\n\n\nscipy (1.12.0)\nscipy (1.15.2)\nsetuptools (75.7.0)\n\n\nsix (1.16.0)\nstack-data (0.6.3)\nstatsmodels (0.14.1)\n\n\nstatsmodels (0.14.4)\ntensorboard (2.19.0)\ntensorboard-data-server (0.7.2)\n\n\ntensorflow (2.19.0)\ntermcolor (3.0.1)\nthreadpoolctl (3.3.0)\n\n\ntornado (6.4)\ntraitlets (5.14.1)\ntyping-extensions (4.13.1)\n\n\ntzdata (2024.1)\nurllib3 (2.3.0)\nwcwidth (0.2.13)\n\n\nwerkzeug (3.1.3)\nwheel (0.45.1)\nwrapt (1.17.2)",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#can-i-contribute",
    "href": "index.html#can-i-contribute",
    "title": "Feature Engineering A-Z",
    "section": "Can I contribute?",
    "text": "Can I contribute?\nPlease feel free to improve the quality of this content by submitting pull requests. A merged PR will make you appear in the contributor list. It will, however, be considered a donation of your work to this project. You are still bound by the conditions of the license, meaning that you are not considered an author, copyright holder, or owner of the content once it has been merged in.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Feature Engineering A-Z",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI’m so thankful for the contributions, help, and perspectives of people who have supported us in this project. There are several I would like to thank in particular.\nI would like to thank my Posit colleagues on the tidymodels team (Hannah Frick, Max Kuhn, and Simon Couch) as well as the rest of our coworkers on the Posit open-source team. I also thank Javier Orraca-Deatcu, Matt Dancho and Mike Mahoney for looking over some of the chapters before the first release.\nThis book was written in the open, and multiple people contributed via pull requests or issues. Special thanks goes to the four people who contributed via GitHub pull requests (in alphabetical order by username): Javier Orraca-Deatcu (@JavOrraca), Sol Feuerwerker (@sfeuerwerker), Murad Khalil (@MuradKhalil), Roberto Rosati (@bioruffo).\n\n\n\n\nGalli, S. 2020. Python Feature Engineering Cookbook: Over 70 Recipes for Creating, Engineering, and Transforming Features to Build Machine Learning Models. Packt Publishing. https://books.google.com/books?id=2c_LDwAAQBAJ.\n\n\nGéron, Aurélien. 2017. Hands-on Machine Learning with Scikit-Learn and TensorFlow : Concepts, Tools, and Techniques to Build Intelligent Systems. Sebastopol, CA: O’Reilly Media.\n\n\nKuhn, M., and K. Johnson. 2013. Applied Predictive Modeling. SpringerLink : Bücher. Springer New York. https://books.google.com/books?id=xYRDAAAAQBAJ.\n\n\n———. 2019. Feature Engineering and Selection: A Practical Approach for Predictive Models. Chapman & Hall/CRC Data Science Series. CRC Press. https://books.google.com/books?id=q5alDwAAQBAJ.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly Media. https://books.google.com/books?id=98J6EAAAQBAJ.\n\n\nOzdemir, S. 2022. Feature Engineering Bookcamp. Manning. https://books.google.com/books?id=3n6HEAAAQBAJ.\n\n\nThakur, A. 2020. Approaching (Almost) Any Machine Learning Problem. Amazon Digital Services LLC - Kdp. https://books.google.com/books?id=ZbgAEAAAQBAJ.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nIt is commonly said that feature engineering, much like machine learning, is an art rather than a science. The purpose of this book is to reinforce this perspective through the exploration of feature engineering tools and techniques. This foundational understanding will make encountering future methods less intimidating.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#how-to-deal-with",
    "href": "introduction.html#how-to-deal-with",
    "title": "Introduction",
    "section": "How to Deal With …",
    "text": "How to Deal With …\nThis book is structured according to the types of data and problems you will encounter. Each section specifies a type of data or problem, and each chapter details a method or group of methods that can be useful in dealing with that type. So for example the Numeric section contains methods that deal with numeric variables such as Logarithms and and Max Abs Scaling, and the Categorical section contains methods that deal with categorical variables such as Dummy Encoding and Hashing Encoding. There should be sections and chapters for most methods you will find in practice that aren’t too domain-specific.\nIt is because of this structure that this book is most suited as reference material, each time you encounter some data you are unsure how to deal with, you find the corresponding section and study the methods listed to see which would be best for your use case. This isn’t to say that you can’t read this book from end to end. The sections have been ordered roughly such that earlier chapters are broadly useful and later chapters touch on less used data types and problems.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#sec-modeling",
    "href": "introduction.html#sec-modeling",
    "title": "Introduction",
    "section": "Where does feature engineering fit into the modeling workflow?",
    "text": "Where does feature engineering fit into the modeling workflow?\nWhen we talk about the modeling workflow, it starts at the data source and ends with a fitted model. The fitted model in this instance should be created such that it can be used for the downstream task, be it inference or prediction.\n\n\n\n\n\n\nInference and Prediction\n\n\n\nIn some data science organizations, the term “inference” is frequently used interchangeably with “prediction,” denoting the process of generating prediction outputs from a trained model. However, in this book, we will use “inference” specifically to refer to statistical inference, which involves drawing conclusions about populations or scientific truths from data analysis.\n\n\nWe want to make sure that the feature engineering methods we are applying are done correctly to avoid problems with the modeling. Things we especially want to avoid are data leakage, overfitting, and high computational cost.\n\n\n\n\n\n\nTODO\n\n\n\nAdd diagram of modeling workflow from data source to model\n\n\nWhen applying feature engineering methods, we need to think about trained and untrained methods. Trained methods will perform a calculation doing the training of the method, and then using the extracted values to perform the transformation again. We see this in the Normalization chapter, where we explore centering and scaling. To implement centering, we adjust each variable by subtracting its mean value, computed using the training dataset. Since this value needs to be calculated, it becomes a trained method. Examples of untrained methods are logarithmic transformation and datetime value extraction. These methods are static in nature, meaning their execution can be applied at the observation-level without parameter-level inferences.\nIn practice, this means that untrained methods can be applied before the data-splitting procedure, as it would give the same results regardless of when it was done. Trained methods have to be performed after the data-splitting to ensure you don’t have data leakage. The wrinkle to this is that untrained methods applied to variables that have already been transformed by a trained method will have to also be done after the data-splitting.\n\n\n\n\n\n\nTODO\n\n\n\nadd a diagram for untrained/trained rule\n\n\nSome untrained methods have a high computational cost, such as BERT. If you are unsure about when to apply a feature engineering method, a general rule of thumb that errs on the side of caution is to apply the method after the data-splitting procedure.\nIn the examples of this book, we will show how to perform methods and techniques using {recipes} on the R side, as they can be used together with the rest of tidymodels to make sure the calculations are done correctly. On the Python side, we show the methods by using transformers, that should then be used inside a sklearn.pipeline.Pipeline() to make sure the calculations are done correctly.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#why-do-we-use-thresholds",
    "href": "introduction.html#why-do-we-use-thresholds",
    "title": "Introduction",
    "section": "Why do we use thresholds?",
    "text": "Why do we use thresholds?\nOftentimes, when we use a method that selects something with a quantity, we end up doing it with a threshold instead of counting directly. The answer to this is purely practical, as it leaves less ambiguity. When selecting these features to keep in a feature selection routine as seen in the Too Many variables section is a good example. It is easier to write the code that selects every feature that has more than X amount of variability. On the other hand, if we said “Give me the 25 most useful features”, we might have 4 variables tied for 25th place. Now we have another problem. Does it keep all of them in, leaving 28 variables? If we do that, we violate our request of 25 variables. What if we select the first? Then we arbitrarily give a bias towards variables early in the data set. What if we randomly select among the ties? Then we introduce randomness into the method.\nIt is for the above reasons that many methods in feature engineering and machine learning use thresholds instead of precise numbers.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#sec-terminology",
    "href": "introduction.html#sec-terminology",
    "title": "Introduction",
    "section": "Terminology",
    "text": "Terminology\nBelow are some terminology clarifications since the term usage in this book may differ from other books. When a method is known by multiple names, the additional name(s) will be listed at the beginning of each chapter. The index will likewise point you to the right chapter regardless of which name you use.\n\nEDA\nOtherwise known as exploratory data analysis is the part of the modeling process where you look at the data very carefully. This will be done in part using descriptive statistics and visualization. This should be done after splitting the data, and only on the training data set. A project may be scrapped at this stage due to the limited usefulness of the data. Spending a lot of time in EDA is almost always fruitful as you gain insight into how to use and transform the data best with feature engineering methods.\n\n\nObservations\nThis book will mostly be working with rectangular data. In this context, each observation is defined as a row, with the columns holding the specific characteristics for each observation.\nThe observational unit can change depending on the data. Consider the following examples consisting of restaurants:\n\nIf we were looking at a data set of restaurant health code inspections, you are likely to see the data with one row per inspection\nIf your data set represented general business information about each restaurant, each observation may represent one unique restaurant\nIf you were a restaurant owner planning future schedules, you could think of each day/week as an observation\n\nReading this book will not tell you how to think about your data; You alone possess the subject matter expertise specific to your data set and problem statement. However, once your data is in the right format and order, we can expose you to possible feature engineering methods.\n\n\nLearned\nSome methods require information to be transformed that we are not able to supply beforehand. In the case of centering of numeric variables described in the normalization chapter, you need to know the mean value of the training data set to apply this transformation. This means is the sufficient information needed to perform the calculations and is the reason why the method is a learned method.\nIn contrast, taking the square root of a variable as described in the Square root chapter isn’t a learned method as there isn’t any sufficient information needed. The method can be applied immediately.\n\n\nSupervised / Unsupervised\nSome methods use the outcome to guide the calculations. If the outcome is used, the method is said to be supervised. Most methods are unsupervised.\n\n\nLevels\nVariables that contain non-numeric information are typically called qualitative or categorical variables. These can be things such as eye color, street names, names, grades, car models and subscription types. Where there is a finite known set of values a categorical variable can take, we call these values the levels of that variable. So the levels of the variables containing weekdays are “Monday”, “Tuesday”, “Wednesday”, “Thursday”, “Friday”, “Saturday”, and “Sunday”. But the names of our subscribers don’t have levels as we don’t know all of them.\nWe will sometimes bend this definition, as it is sometimes useful to pretend that a variable has a finite known set of values, even if it doesn’t.\n\n\nLinear models\nWe talk about linear models as models that are specified as a linear combination of features. These models tend to be simple, and fast to use, but having the limitation of “linear combination of features” means that they struggle when non-linear effects exist in the data set.\n\n\nEmbedding\nThe word embedding is frequently utilized in machine learning and artificial intelligence documentation, however, we will use it to refer to the numeric transformation of data point. We see this often in text embeddings, where a free-from-text field is turned into a fixed-length numerical vector.\nSomething being an embedding doesn’t mean that it is useful. However, with care and signal, useful representations of the data can be created. The reason why we have embeddings in the first place is that most machine learning models require numerical features for the models to work.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "numeric.html",
    "href": "numeric.html",
    "title": "1  Numeric Overview",
    "section": "",
    "text": "1.1 Numeric Overview\nData can come in all shapes and sizes, but the most common one is the numeric variable. These are values such as age, height, deficit, price, and quantity and we call these quantitative variables. They are plentiful in most data sets and immediately usable in all statistical and machine learning models. That being said, this does not imply that certain transformations wouldn’t improve model performance.\nThe following chapters will focus on different methods that follow 1-to-1 or 1-to-more transformations. These methods will mostly be applied one at a time to each variable. Methods that take many variables and return fewer variables such as dimensionality reduction methods will be covered in the Too Many variables section.\nWhen working with a single variable at a time, there are several problems we can encounter. Understanding the problem and how each method addresses it is key to getting the most out of numeric variables.\nThe four main types of problems we deal with when working with individual numeric variables are:",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "numeric.html#distributional-problems",
    "href": "numeric.html#distributional-problems",
    "title": "1  Numeric Overview",
    "section": "1.2 Distributional problems",
    "text": "1.2 Distributional problems\nTODO: add more explanation to this section\n\nlogarithm\nsqrt\nBoxCox\nYeo-Johnson\nPercentile",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "numeric.html#sec-numeric-scaling-issues",
    "href": "numeric.html#sec-numeric-scaling-issues",
    "title": "1  Numeric Overview",
    "section": "1.3 Scaling issues",
    "text": "1.3 Scaling issues\nThe topic of feature scaling is important and used widely in all of machine learning. This chapter will go over what feature scaling is and why we want to use it. The following chapters will each go over a different method of feature scaling.\n\n\n\n\n\n\nNote\n\n\n\nThere is some disagreement about the naming of these topics. These types of methods are called feature scaling and scaling in different fields. This book will call this general class of methods feature scaling and will make notes for each specific method and what other names they go by.\n\n\nIn this book, we will define feature scaling as an operation that modifies variables using multiplication and addition. While broadly defined, the methods typically reduce to the following form:\n\\[\nX_{scaled} = \\dfrac{X - a}{b}\n\\tag{1.1}\\]\nThe main difference between the methods is how \\(a\\) and \\(b\\) are calculated. These are learned transformation methods. We use the training data to derive the right values of \\(a\\) and \\(b\\), and then these values are used to perform the transformations when applied to new data. The different methods might differ on what property is desired for the transformed variables, same range or same spread, but they never change the distribution itself. The power transformations we saw in the Box-Cox and Yeo-Johnson chapters, distort the transformations, where these feature scalings essentially perform a “zooming” effect.\n\n\n\nTable 1.1: All feature scaling methods\n\n\n\n\n\n\n\n\n\nMethod\nDefinition\n\n\n\n\nCentering\n\\(X_{scaled} = X - \\text{mean}(X)\\)\n\n\nScaling\n\\(X_{scaled} = \\dfrac{X}{\\text{sd}(X)}\\)\n\n\nMax-Abs\n\\(X_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\\)\n\n\nNormalization\n\\(X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\)\n\n\nMin-Max\n\\(X_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\)\n\n\nRobust\n\\(X_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)}\\)\n\n\n\n\n\n\nWe see here that all the methods in Table 1.1 follow Equation 1.1. Sometimes \\(a\\) and \\(b\\) take a value of 0, which is perfectly fine. Centering and scaling when used together is equal to normalization. They are kept separate in the table since they are sometimes used independently. Centering, scaling, and normalization will all be discussed in the Normalization chapter.\nThere are two main reasons why we want to perform feature scaling. Firstly, many different types of models take the magnitude of the variables into account when fitting the models, so having variables on different scales can be disadvantageous because some variables have high priorities. In turn, we get that the other variables have low priority. Models that work using Euclidean distances like KNN models are affected by this change. Regularized models such as lasso and ridge regression also need to be scaled since the regularization depends on the magnitude of the estimates. Secondly, some algorithms converge much faster when all the variables are on the same scale. These types of models produce the same fit, just at a slower pace than if you don’t scale the variables. Any algorithms using Gradient Descent fit into this category.\n\n\n\n\n\n\nTODO\n\n\n\nHave a KNN diagram show why this is important List which types of models need feature scaling. Should be a 2 column list. Left=name, right=comment %in% c(no effect, different fit, slow down)",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "numeric.html#non-linear-effect",
    "href": "numeric.html#non-linear-effect",
    "title": "1  Numeric Overview",
    "section": "1.4 Non-linear effect",
    "text": "1.4 Non-linear effect\n\nbinning\nsplines\npolynomial\n\n\n\n\n\n\n\nTODO\n\n\n\nShow different distributions, and how well the different methods do at dealing with them",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "numeric.html#sec-numeric-outliers-issues",
    "href": "numeric.html#sec-numeric-outliers-issues",
    "title": "1  Numeric Overview",
    "section": "1.5 Outliers",
    "text": "1.5 Outliers\n\n\n\n\n\n\nTODO\n\n\n\nFill in some text here, and list issues\nadd chapters that can handle outliers",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "numeric.html#other",
    "href": "numeric.html#other",
    "title": "1  Numeric Overview",
    "section": "1.6 Other",
    "text": "1.6 Other\nThere are any number of transformations we can apply to numeric data, other functions include:\n\nhyperbolic\nRelu\ninverse\ninverse logit\nlogit",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Numeric Overview</span>"
    ]
  },
  {
    "objectID": "numeric-logarithms.html",
    "href": "numeric-logarithms.html",
    "title": "2  Logarithms",
    "section": "",
    "text": "2.1 Logarithms\nLogarithms come as a tool to deal with highly skewed data. Consider the histogram below, it depicts the lot area for all the houses in the ames data set. It is highly skewed with the majority of areas being less than 10,000 with a handful greater than 50,000 with one over 200,000.\nThis data could cause some problems if we tried to use this variable in its unfinished state. We need to think about how numeric variables are being used in these models. Take a linear regression as an example. It fits under the assumption that there is a linear relationship between the response and the predictors. In other words, the model assumes that a 1000 increase in lot area is going to increase the sale price of the house by the same amount if it occurred to a 5000 lot area house or a 50000 lot area house. This might be a valid scenario for this housing market. Another possibility is that we are seeing diminishing returns, and each increase in lot area is going to cause a smaller increase in the predicted sale price.\nThe main identity that is interesting when using logarithms for preprocessing is the multiplicative identity\n\\[\n\\log_{base}(xy) = \\log_{base}(x) + \\log_{base}(y)\n\\]\nThis identity allows us to consider the distances in a specific non-linear way. On the linear scale, we have that the distance between 1 and 2 is the same as 20 to 21 and 10,000 to 10,001. Such a relation is not always what we observe. Consider the relationship between excitement (response) and the number of audience members (predictor) at a concert. Adding a singular audience member is not going to have the same effect for all audience sizes. If we consider \\(y = 1.5\\) then we can look at what happens each time the audience size is increased by 50%. For an initial audience size of 100, the following happens\n\\[\n\\log_{10}(150) = \\log_{10}(100 \\cdot 1.5) = \\log_{10}(100) + \\log_{10}(1.5) \\approx 2 + 0.176\n\\]\nand for an initial audience size of 10,000, we get\n\\[\n\\log_{10}(15,000) = \\log_{10}(10,000 \\cdot 1.5) = \\log_{10}(10,000) + \\log_{10}(1.5) \\approx 4 + 0.176\n\\]\nAnd we see that by using a logarithmic transformation we can translate a multiplicative increase into an additive increase.\nThe logarithmic transformation is perfect for such a scenario. If we take a log2() transformation on our data, then we get a new interpretation of our model. Now each doubling (because we used log2) of the lot area is going to result in the same predicted increase in sale price. We have essentially turned out predictors to work on doublings rather than additions. Below is the same chart as before, but on a logarithmic scale using base 2.\nit is important to note that we are not trying to make the variable normally distributed. What we are trying to accomplish is to remove the skewed nature of the variable. Likewise, this method should not be used as a variance reduction tool as that task is handled by doing normalization which we start exploring more in Scaling issues section.\nThe full equation we use is\n\\[\ny = log_{base}(x + offset)\n\\]\nWhere \\(x\\) is the input, \\(y\\) is the result. The base in and of itself makes a difference, as the choice of base only matters in a multiplicative way. Common choices for bases are \\(e\\), 2, and 10. I find that using a base of 2 yields nice interpretive properties as an increase of 1 in \\(x\\) results in a doubling of \\(y\\).\nOne thing we haven’t touched on yet is that logarithms are only defined for positive input. The offset is introduced to deal with this problem. The offset will default to 0 in most software you will encounter. If you know that your variables contain values from -100 to 10000, you can set offset = 101 to make sure we never take the logarithm of a negative number. A common scenario is non-negative, which contains 0, setting offset = 0.5 helps in that case. The choice of offset you choose does matter a little bit. The offset should generally be smaller than the smallest non-negative value you have, otherwise, you will influence the proportions too much. Below we use an offset of 0.1, and the relationship we get out of this is that distance from 0 to 1, is roughly the same as multiplying by 10.\nOn the other hand, if we set the offset to 0.001, we get that the difference from 0 to 1 is the same as the distance from 1 to 1000, or the same as multiplying by 1000.\nThe use of logarithms will have different effects on different types of models. Linear models will see a change in transformed variables since we can better model relationships between the response and dependent variable if the transformation leads to a more linear relationship. Tree-based models should in theory not be affected by the use of logarithms, however, some implementations use binning to reduce the number of splitting points for consideration. You will see a change if these bins are based on fixed-width intervals. This is expected to have minimal effect on the final model.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logarithms</span>"
    ]
  },
  {
    "objectID": "numeric-logarithms.html#pros-and-cons",
    "href": "numeric-logarithms.html#pros-and-cons",
    "title": "2  Logarithms",
    "section": "2.2 Pros and Cons",
    "text": "2.2 Pros and Cons\n\n2.2.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data sets alike\n\n\n\n2.2.2 Cons\n\nNeeds offset to deal with negative data\nIs not a universal fix. While it can improve right-skewed distributions (where most of the data points are on lower values), it has the opposite effect on a distribution that isn’t skewed or that is left-skewed. See the effect below on 10,000 uniformly distributed values:",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logarithms</span>"
    ]
  },
  {
    "objectID": "numeric-logarithms.html#r-examples",
    "href": "numeric-logarithms.html#r-examples",
    "title": "2  Logarithms",
    "section": "2.3 R Examples",
    "text": "2.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nlog_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_log(Lot_Area)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    10.4      215000\n 2     9.36     105000\n 3     9.57     172000\n 4     9.32     244000\n 5     9.53     189900\n 6     9.21     195500\n 7     8.50     213500\n 8     8.52     191500\n 9     8.59     236500\n10     8.92     189000\n# ℹ 2,920 more rows\n\n\nThe base can be changed by setting the base argument.\n\nlog_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_log(Lot_Area, base = 2)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     15.0     215000\n 2     13.5     105000\n 3     13.8     172000\n 4     13.4     244000\n 5     13.8     189900\n 6     13.3     195500\n 7     12.3     213500\n 8     12.3     191500\n 9     12.4     236500\n10     12.9     189000\n# ℹ 2,920 more rows\n\n\nIf we have non-positive values, which we do in the Wood_Deck_SF variable because it has quite a lot of zeroes, we get -Inf which isn’t going to work.\n\nlog_rec &lt;- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |&gt;\n  step_log(Wood_Deck_SF)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Wood_Deck_SF Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1         5.35     215000\n 2         4.94     105000\n 3         5.97     172000\n 4      -Inf        244000\n 5         5.36     189900\n 6         5.89     195500\n 7      -Inf        213500\n 8      -Inf        191500\n 9         5.47     236500\n10         4.94     189000\n# ℹ 2,920 more rows\n\n\nSetting the offset argument helps us to deal with that problem.\n\nlog_rec &lt;- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |&gt;\n  step_log(Wood_Deck_SF, offset = 0.5)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Wood_Deck_SF Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1        5.35      215000\n 2        4.95      105000\n 3        5.98      172000\n 4       -0.693     244000\n 5        5.36      189900\n 6        5.89      195500\n 7       -0.693     213500\n 8       -0.693     191500\n 9        5.47      236500\n10        4.95      189000\n# ℹ 2,920 more rows",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logarithms</span>"
    ]
  },
  {
    "objectID": "numeric-logarithms.html#python-examples",
    "href": "numeric-logarithms.html#python-examples",
    "title": "2  Logarithms",
    "section": "2.4 Python Examples",
    "text": "2.4 Python Examples\nWe are using the ames data set for examples. {feature_engine} provided the LogCpTransformer() which is just what we need in this instance. This transformer lets us set an offset C that is calculated before taking the logarithm to avoid taking the log of a non-positive number. If we knew that there weren’t any non-positive numbers then we could use the LogTransformer instead.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom feature_engine.transformation import LogCpTransformer\n\nct = ColumnTransformer(\n    [('log', LogCpTransformer(C=1), ['Wood_Deck_SF'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('log', LogCpTransformer(C=1),\n                                 ['Wood_Deck_SF'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('log', LogCpTransformer(C=1),\n                                 ['Wood_Deck_SF'])]) log['Wood_Deck_SF'] LogCpTransformerLogCpTransformer(C=1) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n/Users/emilhvitfeldt/Library/Caches/pypoetry/virtualenvs/feature-engineering-az-g81eMay_-py3.12/lib/python3.12/site-packages/feature_engine/transformation/log.py:388: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[5.35185813 4.94875989 5.97635091 ... 4.39444915 5.48479693 5.25227343]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n  X.loc[:, self.variables_] = np.log(X.loc[:, self.variables_] + self.C_)\n      log__Wood_Deck_SF  ... remainder__Latitude\n0                 5.352  ...              42.054\n1                 4.949  ...              42.053\n2                 5.976  ...              42.053\n3                 0.000  ...              42.051\n4                 5.361  ...              42.061\n...                 ...  ...                 ...\n2925              4.796  ...              41.989\n2926              5.106  ...              41.988\n2927              4.394  ...              41.987\n2928              5.485  ...              41.991\n2929              5.252  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logarithms</span>"
    ]
  },
  {
    "objectID": "numeric-sqrt.html",
    "href": "numeric-sqrt.html",
    "title": "3  Square Root",
    "section": "",
    "text": "3.1 Square Root\nAs we saw in the Logarithms, we sometimes have to deal with highly skewed data. Square roots are another way to deal with this issue, with some different pros and cons that make it better to use us in some situations. We will spend our time in this section to talk about what those are.\nBelow is a histogram of the average daily rate of the number of hotel stays. It is clear to see that this is another case where the data is highly skewed, with many values close to zero, but a few in the thousands.\nThis variable contains some negative values with the smallest being -6.38. We wouldn’t want to throw out the negative values. And we could think of many situations where both negative and positive values are part of a skewed distribution, especially financial. Bank account balances, delivery times, etc etc.\nWe need a method that transforms the scale to un-skew and also works with negative data. The square root could be what we are looking for. By itself, it takes as its input a positive number and returns the number that when multiplied by itself equals the input. This has the desired shrinking effect, where larger values are shrunk more than smaller values. Additionally, since its domain is the positive numbers (0 is a special case since it maps to itself) we can mirror it to work on negative numbers in the same way it worked on positive numbers. This gives us the signed square root\n\\[\ny = \\text{sign}(x)\\sqrt{\\left| x \\right|}\n\\]\nBelow we see the results of applying the signed square root.\nit is important to note that we are not trying to make the variable normally distributed. What we are trying to accomplish is to remove the skewed nature of the variable. Likewise, this method should not be used as a variance reduction tool as that task is handled by doing normalization which we start exploring more in the scaling issues section.\nIt doesn’t have the same power to shrink large values as logarithms do, but it will seamlessly work with negative values and it would allow you to pick up on quadratic effects that you wouldn’t otherwise be able to pick up if you hadn’t applied the transformation. It also doesn’t have good inferential properties. It preserves the order of the numeric values, but it doesn’t give us a good way to interpret changes.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Square Root</span>"
    ]
  },
  {
    "objectID": "numeric-sqrt.html#pros-and-cons",
    "href": "numeric-sqrt.html#pros-and-cons",
    "title": "3  Square Root",
    "section": "3.2 Pros and Cons",
    "text": "3.2 Pros and Cons\n\n3.2.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data sets alike\nCan be applied to all numbers, not just non-negative values\n\n\n\n3.2.2 Cons\n\nIt will leave regression coefficients virtually uninterpretable\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn’t skewed",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Square Root</span>"
    ]
  },
  {
    "objectID": "numeric-sqrt.html#r-examples",
    "href": "numeric-sqrt.html#r-examples",
    "title": "3  Square Root",
    "section": "3.3 R Examples",
    "text": "3.3 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\nlibrary(recipes)\n\nhotel_bookings |&gt;\n  select(lead_time, adr)\n\n# A tibble: 119,390 × 2\n   lead_time   adr\n       &lt;dbl&gt; &lt;dbl&gt;\n 1       342    0 \n 2       737    0 \n 3         7   75 \n 4        13   75 \n 5        14   98 \n 6        14   98 \n 7         0  107 \n 8         9  103 \n 9        85   82 \n10        75  106.\n# ℹ 119,380 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nsqrt_rec &lt;- recipe(lead_time ~ adr, data = hotel_bookings) |&gt;\n  step_sqrt(adr)\n\nsqrt_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nWarning in sqrt(new_data[[col_name]]): NaNs produced\n\n\n# A tibble: 119,390 × 2\n     adr lead_time\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  0          342\n 2  0          737\n 3  8.66         7\n 4  8.66        13\n 5  9.90        14\n 6  9.90        14\n 7 10.3          0\n 8 10.1          9\n 9  9.06        85\n10 10.3         75\n# ℹ 119,380 more rows\n\n\nif you want to do a signed square root instead, you can use step_mutate() which allows you to do any kind of transformations\n\nsigned_sqrt_rec &lt;- recipe(lead_time ~ adr, data = hotel_bookings) |&gt;\n  step_mutate(adr = sqrt(abs(adr)) * sign(adr))\n\nsigned_sqrt_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 119,390 × 2\n     adr lead_time\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  0          342\n 2  0          737\n 3  8.66         7\n 4  8.66        13\n 5  9.90        14\n 6  9.90        14\n 7 10.3          0\n 8 10.1          9\n 9  9.06        85\n10 10.3         75\n# ℹ 119,380 more rows",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Square Root</span>"
    ]
  },
  {
    "objectID": "numeric-sqrt.html#python-examples",
    "href": "numeric-sqrt.html#python-examples",
    "title": "3  Square Root",
    "section": "3.4 Python Examples",
    "text": "3.4 Python Examples\nWe are using the ames data set for examples. Since there isn’t a built-in transformer for square root, we can create our own using FunctionTransformer() and numpy.sqrt().\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nimport numpy as np\n\nsqrt_transformer = FunctionTransformer(np.sqrt)\n\nct = ColumnTransformer(\n    [('sqrt', sqrt_transformer, ['Wood_Deck_SF'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('sqrt',\n                                 FunctionTransformer(func=&lt;ufunc 'sqrt'&gt;),\n                                 ['Wood_Deck_SF'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('sqrt',\n                                 FunctionTransformer(func=&lt;ufunc 'sqrt'&gt;),\n                                 ['Wood_Deck_SF'])]) sqrt['Wood_Deck_SF']  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(func=&lt;ufunc 'sqrt'&gt;) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      sqrt__Wood_Deck_SF  ... remainder__Latitude\n0                 14.491  ...              42.054\n1                 11.832  ...              42.053\n2                 19.824  ...              42.053\n3                  0.000  ...              42.051\n4                 14.560  ...              42.061\n...                  ...  ...                 ...\n2925              10.954  ...              41.989\n2926              12.806  ...              41.988\n2927               8.944  ...              41.987\n2928              15.492  ...              41.991\n2929              13.784  ...              41.989\n\n[2930 rows x 74 columns]\n\n\nWe can also create and perform a signed square root transformation, by creating a function for signed_sqrt() and then using it in FunctionTransformer() as before\n\ndef signed_sqrt(x):\n  return np.sqrt(np.abs(x)) * np.sign(x)\n\nsigned_sqrt_transformer = FunctionTransformer(signed_sqrt)\n\nct = ColumnTransformer(\n    [('signed_sqrt', signed_sqrt_transformer, ['Wood_Deck_SF'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('signed_sqrt',\n                                 FunctionTransformer(func=&lt;function signed_sqrt at 0x35f85bc40&gt;),\n                                 ['Wood_Deck_SF'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('signed_sqrt',\n                                 FunctionTransformer(func=&lt;function signed_sqrt at 0x35f85bc40&gt;),\n                                 ['Wood_Deck_SF'])]) signed_sqrt['Wood_Deck_SF']  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(func=&lt;function signed_sqrt at 0x35f85bc40&gt;) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      signed_sqrt__Wood_Deck_SF  ... remainder__Latitude\n0                        14.491  ...              42.054\n1                        11.832  ...              42.053\n2                        19.824  ...              42.053\n3                         0.000  ...              42.051\n4                        14.560  ...              42.061\n...                         ...  ...                 ...\n2925                     10.954  ...              41.989\n2926                     12.806  ...              41.988\n2927                      8.944  ...              41.987\n2928                     15.492  ...              41.991\n2929                     13.784  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Square Root</span>"
    ]
  },
  {
    "objectID": "numeric-boxcox.html",
    "href": "numeric-boxcox.html",
    "title": "4  Box-Cox",
    "section": "",
    "text": "4.1 Box-Cox\nYou have likely heard a lot of talk about having normally distributed predictors. This isn’t that common of an assumption, and having a fairly non-skewed symmetric predictor is often enough. Linear Discriminant Analysis assumes Gaussian data, and that is about it (TODO add a reference here). Still, it is worthwhile to have more symmetric predictors, and this is where the Box-Cox transformation comes into play.\nIn Logarithms we saw how they could be used to change the distribution of a variable. One of the downsides is that if we want to get closer to normality, it doesn’t do well unless applied to a log-normally distributed variable. The Box-Cox transformation tries to find an optimal power transformation. This method was originally intended to be used on the outcome of a model.\nIt works by using maximum likelihood estimation to estimate a transformation parameter \\(\\lambda\\) in the following equation that would optimize the normality of \\(x^*\\)\n\\[\nx^* = \\left\\{\n    \\begin{array}{ll}\n      \\dfrac{x^\\lambda - 1}{\\lambda \\tilde{x}^{\\lambda - 1}}, & \\lambda \\neq 0 \\\\\n      \\tilde{x} \\log x & \\lambda = 0\n    \\end{array}\n  \\right.\n\\]\nwhere \\(\\tilde{x}\\) is the geometric mean of \\(x\\). It is worth noting again, that what we are optimizing over is the value of \\(\\lambda\\). This is also a case of a trained preprocessing method when used on the predictors. We need to estimate the parameter \\(\\lambda\\) on the training data set, then use the estimated value to apply the transformation to the training and test data set to avoid data leakage. Lastly, Box-Cox only works with positive numbers. Take a look at the Yeo-Johnson method that tries to accomplish the same thing, and it works on positive as well as negative numbers.\nLet us see some examples of Box-Cox at work. Below is three different simulated distribution, before and after they have been transformed by Box-Cox.\nThe original distributions have some left or right skewness. And the transformed columns look better, in the sense that they are less skewed and they are fairly symmetric around the center. Are they perfectly normal? No! but these transformations might be beneficial. The next set of distributions wasn’t so lucky.\nThe Box-Cox method isn’t magic and will only give you something more normally distributed if the distribution can be made more normally distributed by applying a power transformation.\nThe first distribution here is uniformly random. The resulting transformation ends up more skewed, even if only a little bit, than the original distribution because this method is not intended for this type of data. We are seeing similar results with the bi-modal distributions.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Box-Cox</span>"
    ]
  },
  {
    "objectID": "numeric-boxcox.html#pros-and-cons",
    "href": "numeric-boxcox.html#pros-and-cons",
    "title": "4  Box-Cox",
    "section": "4.2 Pros and Cons",
    "text": "4.2 Pros and Cons\n\n4.2.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\n\n\n\n4.2.2 Cons\n\nDoesn’t work with negative values\nIsn’t a universal fix",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Box-Cox</span>"
    ]
  },
  {
    "objectID": "numeric-boxcox.html#r-examples",
    "href": "numeric-boxcox.html#r-examples",
    "title": "4  Box-Cox",
    "section": "4.3 R Examples",
    "text": "4.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform Box-Cox transformations.\n\nboxcox_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_BoxCox(Lot_Area) |&gt;\n  prep()\n\nboxcox_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# ℹ 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nboxcox_rec |&gt;\n  tidy(1)\n\n# A tibble: 1 × 3\n  terms    value id          \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 Lot_Area 0.129 BoxCox_3gJXR",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Box-Cox</span>"
    ]
  },
  {
    "objectID": "numeric-boxcox.html#python-examples",
    "href": "numeric-boxcox.html#python-examples",
    "title": "4  Box-Cox",
    "section": "4.4 Python Examples",
    "text": "4.4 Python Examples\nWe are using the ames data set for examples. {feature_engine} provided the BoxCoxTransformer() which is just what we need in this instance. We note that this transformer does not work on non-positive data.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom feature_engine.transformation import BoxCoxTransformer\n\nct = ColumnTransformer(\n    [('boxcox', BoxCoxTransformer(), ['Lot_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('boxcox', BoxCoxTransformer(), ['Lot_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('boxcox', BoxCoxTransformer(), ['Lot_Area'])]) boxcox['Lot_Area'] BoxCoxTransformerBoxCoxTransformer() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      boxcox__Lot_Area  ... remainder__Latitude\n0               21.839  ...              42.054\n1               18.229  ...              42.053\n2               18.928  ...              42.053\n3               18.093  ...              42.051\n4               18.821  ...              42.061\n...                ...  ...                 ...\n2925            16.979  ...              41.989\n2926            17.343  ...              41.988\n2927            17.872  ...              41.987\n2928            17.733  ...              41.991\n2929            17.604  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Box-Cox</span>"
    ]
  },
  {
    "objectID": "numeric-yeojohnson.html",
    "href": "numeric-yeojohnson.html",
    "title": "5  Yeo-Johnson",
    "section": "",
    "text": "5.1 Yeo-Johnson\nYou have likely heard a lot of talk about having normally distributed predictors. This isn’t that common of an assumption, and having a fairly non-skewed symmetric predictor is often enough. Linear Discriminant Analysis assumes Gaussian data, and that is about it (TODO add a reference here). Still, it is worthwhile to have more symmetric predictors, and this is where the Yeo-Johnson transformation comes into play.\nThis method is very similar to the Box-Cox method, except it doesn’t have the restriction that the variable \\(x\\) needs to be positive.\nIt works by using maximum likelihood estimation to estimate a transformation parameter \\(\\lambda\\) in the following equation that would optimize the normality of \\(x^*\\)\n\\[\nx^* = \\left\\{\n    \\begin{array}{ll}\n      \\dfrac{(x + 1) ^ \\lambda - 1}{\\lambda}              & \\lambda \\neq 0, x \\geq 0 \\\\\n      \\log(x + 1)                                         & \\lambda =    0, x \\geq 0 \\\\\n      - \\dfrac{(-x + 1) ^ {2 - \\lambda} - 1}{2 - \\lambda} & \\lambda \\neq 2, x &lt;    0 \\\\\n      - \\log(-x + 1)                                      & \\lambda =    2, x &lt;    0\n    \\end{array}\n  \\right.\n\\]\nIt is worth noting again, that what we are optimizing over is the value of \\(\\lambda\\). This is also a case of a trained preprocessing method when used on the predictors. We need to estimate the parameter \\(\\lambda\\) on the training data set, then use the estimated value to apply the transformation to the training and test data set to avoid data leakage.\nIf the values of \\(x\\) are strictly positive, then the Yeo-Johnson transformation is the same as the Box-Cox transformation of \\(x + 1\\), if the values of \\(x\\) are strictly negative then the transformation is the Box-Cox transformation of \\(-x + 1\\) with the power \\(2 - \\lambda\\). The interpretation of \\(\\lambda\\) isn’t as easy as for the Box-Cox method.\nLet us see some examples of Yeo-Johnson at work. Below is three different simulated distribution, before and after they have been transformed by Yeo-Johnson.\nWe have the original distributions that have some left or right skewness. And the transformed columns look better, in the sense that they are less skewed and they are fairly symmetric around the center. Are they perfectly normal? No! but these transformations might be beneficial. We also notice how these methods work, even when there are negative values.\nThe Yeo-Johnson method isn’t magic and will only give you something more normally distributed if the distribution can be made more normally distributed by applying the above formula would give you some more normally distributed values.\nThe first distribution here is uniformly random. The resulting transformation ends up more skewed, even if only a little bit, than the original distribution because this method is not intended for this type of data. We are seeing similar results with the bi-modal distributions.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Yeo-Johnson</span>"
    ]
  },
  {
    "objectID": "numeric-yeojohnson.html#pros-and-cons",
    "href": "numeric-yeojohnson.html#pros-and-cons",
    "title": "5  Yeo-Johnson",
    "section": "5.2 Pros and Cons",
    "text": "5.2 Pros and Cons\n\n5.2.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\nCan handle negative values\n\n\n\n5.2.2 Cons\n\nIsn’t a universal fix",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Yeo-Johnson</span>"
    ]
  },
  {
    "objectID": "numeric-yeojohnson.html#r-examples",
    "href": "numeric-yeojohnson.html#r-examples",
    "title": "5  Yeo-Johnson",
    "section": "5.3 R Examples",
    "text": "5.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform Yeo-Johnson transformations, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nyeojohnson_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_YeoJohnson(Lot_Area) |&gt;\n  prep()\n\nyeojohnson_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# ℹ 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nyeojohnson_rec |&gt;\n  tidy(1)\n\n# A tibble: 1 × 3\n  terms    value id              \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1 Lot_Area 0.129 YeoJohnson_3gJXR",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Yeo-Johnson</span>"
    ]
  },
  {
    "objectID": "numeric-yeojohnson.html#python-examples",
    "href": "numeric-yeojohnson.html#python-examples",
    "title": "5  Yeo-Johnson",
    "section": "5.4 Python Examples",
    "text": "5.4 Python Examples\nWe are using the ames data set for examples. {feature_engine} provided the YeoJohnsonTransformer() that we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom feature_engine.transformation import YeoJohnsonTransformer\n\nct = ColumnTransformer(\n    [('yeojohnson', YeoJohnsonTransformer(), ['Lot_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('yeojohnson', YeoJohnsonTransformer(),\n                                 ['Lot_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('yeojohnson', YeoJohnsonTransformer(),\n                                 ['Lot_Area'])]) yeojohnson['Lot_Area'] YeoJohnsonTransformerYeoJohnsonTransformer() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      yeojohnson__Lot_Area  ... remainder__Latitude\n0                   21.823  ...              42.054\n1                   18.218  ...              42.053\n2                   18.915  ...              42.053\n3                   18.082  ...              42.051\n4                   18.808  ...              42.061\n...                    ...  ...                 ...\n2925                16.969  ...              41.989\n2926                17.332  ...              41.988\n2927                17.861  ...              41.987\n2928                17.721  ...              41.991\n2929                17.593  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Yeo-Johnson</span>"
    ]
  },
  {
    "objectID": "numeric-percentile.html",
    "href": "numeric-percentile.html",
    "title": "6  Percentile Scaling",
    "section": "",
    "text": "6.1 Percentile Scaling\nPercentile scaling (also sometimes called Rank Scaling or quantile scaling) is a method where we apply a non-linear transformation to our data where each value is the percentile of the training data.\nThis does a couple of things for us. It naturally constrains the transformed data into the range \\([0, 1]\\), and it deals with outlier values nicely in the sense that they don’t change the transformation that much. Moreover, if the testing distribution is close to the training distribution then the transformed distribution would be approximately uniformly distributed between 0 and 1.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Percentile Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-percentile.html#pros-and-cons",
    "href": "numeric-percentile.html#pros-and-cons",
    "title": "6  Percentile Scaling",
    "section": "6.2 Pros and Cons",
    "text": "6.2 Pros and Cons\n\n6.2.1 Pros\n\nTransformation isn’t affected much by outliers\n\n\n\n6.2.2 Cons\n\nDoesn’t allow to exact reverse transformation\nIsn’t ideal if training data doesn’t have that many unique values",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Percentile Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-percentile.html#r-examples",
    "href": "numeric-percentile.html#r-examples",
    "title": "6  Percentile Scaling",
    "section": "6.3 R Examples",
    "text": "6.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ℹ 2,920 more rows\n\n\nThe {recipes} step to do this transformation is step_percentile(). It defaults to the calculation of 100 percentiles and uses those to transform the data\n\npercentile_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_percentile(Lot_Area) |&gt;\n  prep()\n\npercentile_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    0.989     215000\n 2    0.756     105000\n 3    0.898     172000\n 4    0.717     244000\n 5    0.883     189900\n 6    0.580     195500\n 7    0.104     213500\n 8    0.106     191500\n 9    0.120     236500\n10    0.259     189000\n# ℹ 2,920 more rows\n\n\nWe can use the tidy() method to pull out what the specific values are for each percentile\n\npercentile_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms    value percentile id              \n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n 1 Lot_Area 1300           0 percentile_Bp5vK\n 2 Lot_Area 1680           1 percentile_Bp5vK\n 3 Lot_Area 2040.          2 percentile_Bp5vK\n 4 Lot_Area 2362.          3 percentile_Bp5vK\n 5 Lot_Area 2779.          4 percentile_Bp5vK\n 6 Lot_Area 3188.          5 percentile_Bp5vK\n 7 Lot_Area 3674.          6 percentile_Bp5vK\n 8 Lot_Area 3901.          7 percentile_Bp5vK\n 9 Lot_Area 4122.          8 percentile_Bp5vK\n10 Lot_Area 4435           9 percentile_Bp5vK\n# ℹ 89 more rows\n\n\nYou can change the granularity by using the options argument. In this example, we are calculating 500 points evenly spaced between 0 and 1, both inclusive.\n\npercentile500_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_percentile(Lot_Area, options = list(probs = (0:500)/500)) |&gt;\n  prep()\n\npercentile500_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    0.989     215000\n 2    0.755     105000\n 3    0.899     172000\n 4    0.717     244000\n 5    0.884     189900\n 6    0.580     195500\n 7    0.103     213500\n 8    0.106     191500\n 9    0.118     236500\n10    0.254     189000\n# ℹ 2,920 more rows\n\n\nAnd we can see the more precise numbers.\n\npercentile500_rec |&gt;\n  tidy(1)\n\n# A tibble: 457 × 4\n   terms    value percentile id              \n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n 1 Lot_Area 1300         0   percentile_RUieL\n 2 Lot_Area 1487.        0.2 percentile_RUieL\n 3 Lot_Area 1531.        0.4 percentile_RUieL\n 4 Lot_Area 1605.        0.6 percentile_RUieL\n 5 Lot_Area 1680         0.8 percentile_RUieL\n 6 Lot_Area 1879.        1.4 percentile_RUieL\n 7 Lot_Area 1890         1.6 percentile_RUieL\n 8 Lot_Area 1946.        1.8 percentile_RUieL\n 9 Lot_Area 2040.        2   percentile_RUieL\n10 Lot_Area 2136.        2.2 percentile_RUieL\n# ℹ 447 more rows\n\n\nNotice how there are only 457 values in this output. This is happening because some percentile has been collapsed to save space since if the value for the 10.4 and 10.6 percentile is the same, we just store the 10.6 value.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Percentile Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-percentile.html#python-examples",
    "href": "numeric-percentile.html#python-examples",
    "title": "6  Percentile Scaling",
    "section": "6.4 Python Examples",
    "text": "6.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the QuantileTransformer() method we can use. We can use the n_quantiles argument to change the number of quantiles to use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\nct = ColumnTransformer(\n    [('Quantile', QuantileTransformer(n_quantiles = 500), ['Lot_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('Quantile',\n                                 QuantileTransformer(n_quantiles=500),\n                                 ['Lot_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('Quantile',\n                                 QuantileTransformer(n_quantiles=500),\n                                 ['Lot_Area'])]) Quantile['Lot_Area']  QuantileTransformer?Documentation for QuantileTransformerQuantileTransformer(n_quantiles=500) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      Quantile__Lot_Area  ... remainder__Latitude\n0                  0.989  ...              42.054\n1                  0.755  ...              42.053\n2                  0.899  ...              42.053\n3                  0.717  ...              42.051\n4                  0.884  ...              42.061\n...                  ...  ...                 ...\n2925               0.300  ...              41.989\n2926               0.427  ...              41.988\n2927               0.639  ...              41.987\n2928               0.586  ...              41.991\n2929               0.536  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Percentile Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-normalization.html",
    "href": "numeric-normalization.html",
    "title": "7  Normalization",
    "section": "",
    "text": "7.1 Normalization\nNormalization is a method where we modify a variable by subtracting the mean and dividing by the standard deviation\n\\[X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\]\nPerforming this transformation means that the resulting variable will have a mean of 0 and a standard deviation and variance of 1. This method is a learned transformation. So we use the training data to derive the right values of \\(\\text{sd}(X)\\) and \\(\\text{mean}(X)\\) and then these values are used to perform the transformations when applied to new data. It is a common misconception that this transformation is done to make the data normally distributed. This transformation doesn’t change the distribution, it scales the values. Below is a figure Figure 7.1 that illustrates that point\nIn Figure 7.1 we see some distribution, before and after applying normalization to it. Both the original and transformed distribution are bimodal, and the transformed distribution is no more normal than the original. That is fine because the transformation did its job by moving the values close to 0 and a specific spread, which in this case is a variance of 1.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Normalization</span>"
    ]
  },
  {
    "objectID": "numeric-normalization.html#pros-and-cons",
    "href": "numeric-normalization.html#pros-and-cons",
    "title": "7  Normalization",
    "section": "7.2 Pros and Cons",
    "text": "7.2 Pros and Cons\n\n7.2.1 Pros\n\nIf you don’t have any severe outliers then you will rarely see any downsides to applying normalization\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n7.2.2 Cons\n\nNot all software solutions will be helpful when applying this transformation to a constant variable. You will often get a “division by 0” error\nCannot be used with sparse data as it isn’t preserved because of the centering that is happening. If you only scale the data you don’t have a problem\nThis transformation is highly affected by outliers, as they affect the mean and standard deviation quite a lot\n\nBelow is the figure Figure 7.2 is an illustration of the effect of having a single high value. In this case, a single observation with the value 10000 moved the transformed distribution much tighter around zero. And all but removed the variance of the non-outliers.\n\n\n\n\n\n\n\n\nFigure 7.2: Outliers can have a big effect on the resulting distribution when applying normalization.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Normalization</span>"
    ]
  },
  {
    "objectID": "numeric-normalization.html#r-examples",
    "href": "numeric-normalization.html#r-examples",
    "title": "7  Normalization",
    "section": "7.3 R Examples",
    "text": "7.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\n{recipes} provides a step to perform scaling, centering, and normalization. They are called step_scale(), step_center() and step_normalize() respectively.\nBelow is an example using step_scale()\n\nscale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  prep()\n\nscale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    4.03          1.66        0.627\n 2     105000    1.47          1.11        0    \n 3     172000    1.81          3.11        0.605\n 4     244000    1.42          0           0    \n 5     189900    1.76          1.68        0    \n 6     195500    1.27          2.85        0.112\n 7     213500    0.624         0           0    \n 8     191500    0.635         0           0    \n 9     236500    0.684         1.88        0    \n10     189000    0.952         1.11        0    \n# ℹ 2,920 more rows\n\n\nWe can also pull out the value of the standard deviation for each variable that was affected using tidy()\n\nscale_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 3\n   terms            value id         \n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage     33.5  scale_FGmgk\n 2 Lot_Area       7880.   scale_FGmgk\n 3 Year_Built       30.2  scale_FGmgk\n 4 Year_Remod_Add   20.9  scale_FGmgk\n 5 Mas_Vnr_Area    179.   scale_FGmgk\n 6 BsmtFin_SF_1      2.23 scale_FGmgk\n 7 BsmtFin_SF_2    169.   scale_FGmgk\n 8 Bsmt_Unf_SF     440.   scale_FGmgk\n 9 Total_Bsmt_SF   441.   scale_FGmgk\n10 First_Flr_SF    392.   scale_FGmgk\n# ℹ 23 more rows\n\n\nWe could also have used step_center() and step_scale() together in one recipe\n\ncenter_scale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  prep()\n\ncenter_scale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# ℹ 2,920 more rows\n\n\nUsing tidy() we can see information about each step\n\ncenter_scale_rec |&gt;\n  tidy()\n\n# A tibble: 2 × 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_tSRk5\n2      2 step      scale  TRUE    FALSE scale_kjP2v \n\n\nAnd we can pull out the means using tidy(1)\n\ncenter_scale_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 3\n   terms             value id          \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage      57.6  center_tSRk5\n 2 Lot_Area       10148.   center_tSRk5\n 3 Year_Built      1971.   center_tSRk5\n 4 Year_Remod_Add  1984.   center_tSRk5\n 5 Mas_Vnr_Area     101.   center_tSRk5\n 6 BsmtFin_SF_1       4.18 center_tSRk5\n 7 BsmtFin_SF_2      49.7  center_tSRk5\n 8 Bsmt_Unf_SF      559.   center_tSRk5\n 9 Total_Bsmt_SF   1051.   center_tSRk5\n10 First_Flr_SF    1160.   center_tSRk5\n# ℹ 23 more rows\n\n\nand the standard deviation using tidy(2)\n\ncenter_scale_rec |&gt;\n  tidy(2)\n\n# A tibble: 33 × 3\n   terms            value id         \n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage     33.5  scale_kjP2v\n 2 Lot_Area       7880.   scale_kjP2v\n 3 Year_Built       30.2  scale_kjP2v\n 4 Year_Remod_Add   20.9  scale_kjP2v\n 5 Mas_Vnr_Area    179.   scale_kjP2v\n 6 BsmtFin_SF_1      2.23 scale_kjP2v\n 7 BsmtFin_SF_2    169.   scale_kjP2v\n 8 Bsmt_Unf_SF     440.   scale_kjP2v\n 9 Total_Bsmt_SF   441.   scale_kjP2v\n10 First_Flr_SF    392.   scale_kjP2v\n# ℹ 23 more rows\n\n\nSince these steps often follow each other, we often use the step_normalize() as a shortcut to do both operations in one step\n\nscale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  prep()\n\nscale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# ℹ 2,920 more rows\n\n\nAnd we can still pull out the means and standard deviations using tidy()\n\nscale_rec |&gt;\n  tidy(1) |&gt;\n  filter(terms %in% c(\"Lot_Area\", \"Wood_Deck_SF\", \"Mas_Vnr_Area\"))\n\n# A tibble: 6 × 4\n  terms        statistic   value id             \n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          \n1 Lot_Area     mean      10148.  normalize_ucdPw\n2 Mas_Vnr_Area mean        101.  normalize_ucdPw\n3 Wood_Deck_SF mean         93.8 normalize_ucdPw\n4 Lot_Area     sd         7880.  normalize_ucdPw\n5 Mas_Vnr_Area sd          179.  normalize_ucdPw\n6 Wood_Deck_SF sd          126.  normalize_ucdPw",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Normalization</span>"
    ]
  },
  {
    "objectID": "numeric-normalization.html#python-examples",
    "href": "numeric-normalization.html#python-examples",
    "title": "7  Normalization",
    "section": "7.4 Python Examples",
    "text": "7.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the StandardScaler() method we can use. By default we can use this method to perform both the centering and scaling.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\nct = ColumnTransformer(\n    [('normalize', StandardScaler(), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('normalize', StandardScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('normalize', StandardScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) normalize['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  StandardScaler?Documentation for StandardScalerStandardScaler() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      normalize__Sale_Price  ...  remainder__Latitude\n0                     0.428  ...               42.054\n1                    -0.949  ...               42.053\n2                    -0.110  ...               42.053\n3                     0.791  ...               42.051\n4                     0.114  ...               42.061\n...                     ...  ...                  ...\n2925                 -0.479  ...               41.989\n2926                 -0.623  ...               41.988\n2927                 -0.611  ...               41.987\n2928                 -0.135  ...               41.991\n2929                  0.090  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nTo only perform scaling, you set with_mean=False.\n\nct = ColumnTransformer(\n    [('scaling', StandardScaler(with_mean=False, with_std=True), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('scaling', StandardScaler(with_mean=False),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('scaling', StandardScaler(with_mean=False),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) scaling['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  StandardScaler?Documentation for StandardScalerStandardScaler(with_mean=False) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      scaling__Sale_Price  ...  remainder__Latitude\n0                   2.692  ...               42.054\n1                   1.315  ...               42.053\n2                   2.153  ...               42.053\n3                   3.055  ...               42.051\n4                   2.378  ...               42.061\n...                   ...  ...                  ...\n2925                1.784  ...               41.989\n2926                1.640  ...               41.988\n2927                1.653  ...               41.987\n2928                2.128  ...               41.991\n2929                2.354  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nAnd to only perform centering you set with_mean=True.\n\nct = ColumnTransformer(\n    [('centering', StandardScaler(with_mean=True, with_std=False), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('centering', StandardScaler(with_std=False),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('centering', StandardScaler(with_std=False),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) centering['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  StandardScaler?Documentation for StandardScalerStandardScaler(with_std=False) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      centering__Sale_Price  ...  remainder__Latitude\n0                  34203.94  ...               42.054\n1                 -75796.06  ...               42.053\n2                  -8796.06  ...               42.053\n3                  63203.94  ...               42.051\n4                   9103.94  ...               42.061\n...                     ...  ...                  ...\n2925              -38296.06  ...               41.989\n2926              -49796.06  ...               41.988\n2927              -48796.06  ...               41.987\n2928              -10796.06  ...               41.991\n2929                7203.94  ...               41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Normalization</span>"
    ]
  },
  {
    "objectID": "numeric-range.html",
    "href": "numeric-range.html",
    "title": "8  Range Scaling",
    "section": "",
    "text": "8.1 Range Scaling\nRange scaling, also known as Min-Max scaling, is a method to make the data fit into a pre-defined interval. Typically the equation used to illustrate this method is shown so\n\\[\nX_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n\\tag{8.1}\\]\nThis equation only shows what happens if you use the software with defaults. This will scale your variable to be in the range \\([0, 1]\\). It is important to treat this as a learned method, and we will thus calculate \\(\\text{min}(X)\\) and \\(\\text{max}(X)\\) on the training data set, and use those to apply the transformation on new data. One of the benefits of this method is that it transforms any variable into a predefined range of 0 and 1. Unless of cause, a new data point is outside the range. Suppose that the original data was in the range \\(\\text{min}(X) = 5\\) and \\(\\text{max}(X) = 100\\), and we encounter a new data point with the value 200. Then according to Equation 8.2, we would get a transformed value of (200-5)/(100-5) = 2.0526 which is very much not between 0 and 1. We thus have a violation of our original goal. One way to get around this is to use clipping. Clipping is a technique when using this method where any values less than \\(\\text{min}(X)\\) will be noted as 0 and any values larger than \\(\\text{max}(X)\\) will be noted as 1. This little tick ensures that any new data points stay within the range. Expanding our equation to the following\n\\[\nX_{scaled}=    \n\\begin{cases}        \n0, & X_{new} &lt; \\text{min}(X_{train})\\\\        \n1 & X_{new} &gt; \\text{max}(X_{train})\\\\\n\\dfrac{X_{new} - \\text{min}(X_{train})}{\\text{max}(X_{train}) - \\text{min}(X_{train})}, & \\text{otherwise}\n\\end{cases}\n\\tag{8.2}\\]\nThe equation is a lot bigger, but what is different is that we specify that the transformation should be done using training data, and by adding a couple of branches to reflect the clipping. In practice, it will look something like Figure 8.1 where the original data had a range of \\([10, 32]\\).\nSadly we are not done quite yet. One last thing that Equation 8.2 doesn’t take into account is that sometimes people don’t want to transform to be into any range, not just \\([0, 1]\\). This gives us the final equation\n\\[\nX_{scaled}=    \n\\begin{cases}        \nR_{lower}, & X_{new} &lt; \\text{min}(X_{train})\\\\        \nR_{upper} & X_{new} &gt; \\text{max}(X_{train})\\\\\n\\dfrac{X_{new} - \\text{min}(X_{train})}{\\text{max}(X_{train}) - \\text{min}(X_{train})} \\cdot (R_{upper}-R_{lower}) + R_{lower}, & \\text{otherwise}\n\\end{cases}\n\\tag{8.3}\\]\nWhere Equation 8.3 now has \\(R_{lower}\\) to represent a user-defined lower bound, and \\(R_{upper}\\) representing the corresponding user-defined upper bound. I would recommend that you keep Equation 8.1 in mind when thinking about this method but also include a little footnote that doesn’t include all the little options.\nOne thing you should know is how this transformation is affected by outliers. Clipping essentially ignores the magnitude of how much an outlier is. There is no difference between a new value of 100 and 100000 to a variable that had a range of \\([0, 90]\\) on the training data set. This might be a problem, and it is up to you as the practitioner to decide. One option would be to turn off clipping, but it would violate the assumption that all future transformed observations will be within a specific range.\nBelow is the figure Figure 8.2 is an illustration of the effect of having a single high value. In this case, a single observation with the value `10000` moved the transformed distribution much tighter around zero. And all but removed the variance of the non-outliers.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Range Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-range.html#pros-and-cons",
    "href": "numeric-range.html#pros-and-cons",
    "title": "8  Range Scaling",
    "section": "8.2 Pros and Cons",
    "text": "8.2 Pros and Cons\n\n8.2.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale, provided that clipping wasn’t turned on\n\n\n\n8.2.2 Cons\n\nTurning on clipping diminishes the effect of outliers by rounding them up/down\nDoesn’t work with zero variance data as max(x) - min(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn’t preserved",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Range Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-range.html#r-examples",
    "href": "numeric-range.html#r-examples",
    "title": "8  Range Scaling",
    "section": "8.3 R Examples",
    "text": "8.3 R Examples\nstep_range() clips. Does allow the user to specify range step_minmax() doesn’t clip. Doesn’t allow users to specify a range. A PR is planned to allow users to turn off clipping in step_range()\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)#| \nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\nWe will be using the step_range() step for this\n\nrange_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_range(all_numeric_predictors()) |&gt;\n  prep()\n\nrange_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   0.142        0.147        0.07  \n 2     105000   0.0482       0.0983       0     \n 3     172000   0.0606       0.276        0.0675\n 4     244000   0.0461       0            0     \n 5     189900   0.0586       0.149        0     \n 6     195500   0.0406       0.253        0.0125\n 7     213500   0.0169       0            0     \n 8     191500   0.0173       0            0     \n 9     236500   0.0191       0.166        0     \n10     189000   0.0290       0.0983       0     \n# ℹ 2,920 more rows\n\n\nWe can also pull out what the min and max values were for each variable using tidy()\n\nrange_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 4\n   terms            min    max id         \n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage       0    313 range_FGmgk\n 2 Lot_Area        1300 215245 range_FGmgk\n 3 Year_Built      1872   2010 range_FGmgk\n 4 Year_Remod_Add  1950   2010 range_FGmgk\n 5 Mas_Vnr_Area       0   1600 range_FGmgk\n 6 BsmtFin_SF_1       0      7 range_FGmgk\n 7 BsmtFin_SF_2       0   1526 range_FGmgk\n 8 Bsmt_Unf_SF        0   2336 range_FGmgk\n 9 Total_Bsmt_SF      0   6110 range_FGmgk\n10 First_Flr_SF     334   5095 range_FGmgk\n# ℹ 23 more rows\n\n\nusing the min and max arguments we can set different ranges\n\nrange_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_range(all_numeric_predictors(), min = -2, max = 2) |&gt;\n  prep()\n\nrange_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    -1.43       -1.41         -1.72\n 2     105000    -1.81       -1.61         -2   \n 3     172000    -1.76       -0.896        -1.73\n 4     244000    -1.82       -2            -2   \n 5     189900    -1.77       -1.40         -2   \n 6     195500    -1.84       -0.989        -1.95\n 7     213500    -1.93       -2            -2   \n 8     191500    -1.93       -2            -2   \n 9     236500    -1.92       -1.33         -2   \n10     189000    -1.88       -1.61         -2   \n# ℹ 2,920 more rows",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Range Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-range.html#python-examples",
    "href": "numeric-range.html#python-examples",
    "title": "8  Range Scaling",
    "section": "8.4 Python Examples",
    "text": "8.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the MinMaxScaler() method we can use. By default we can use this method to perform both the centering and scaling.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\nct = ColumnTransformer(\n    [('minmax', MinMaxScaler(), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('minmax', MinMaxScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('minmax', MinMaxScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) minmax['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  MinMaxScaler?Documentation for MinMaxScalerMinMaxScaler() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      minmax__Sale_Price  ...  remainder__Latitude\n0                  0.272  ...               42.054\n1                  0.124  ...               42.053\n2                  0.215  ...               42.053\n3                  0.312  ...               42.051\n4                  0.239  ...               42.061\n...                  ...  ...                  ...\n2925               0.175  ...               41.989\n2926               0.159  ...               41.988\n2927               0.161  ...               41.987\n2928               0.212  ...               41.991\n2929               0.236  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nMinMaxScaler() doesn’t clip by default, we can turn this back on by setting clip=True.\n\nct = ColumnTransformer(\n    [('minmax', MinMaxScaler(clip=True), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('minmax', MinMaxScaler(clip=True),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('minmax', MinMaxScaler(clip=True),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) minmax['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  MinMaxScaler?Documentation for MinMaxScalerMinMaxScaler(clip=True) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      minmax__Sale_Price  ...  remainder__Latitude\n0                  0.272  ...               42.054\n1                  0.124  ...               42.053\n2                  0.215  ...               42.053\n3                  0.312  ...               42.051\n4                  0.239  ...               42.061\n...                  ...  ...                  ...\n2925               0.175  ...               41.989\n2926               0.159  ...               41.988\n2927               0.161  ...               41.987\n2928               0.212  ...               41.991\n2929               0.236  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nWe can also set a different range other than (0, 1) with the feature_range argument\n\nct = ColumnTransformer(\n    [('minmax', MinMaxScaler(feature_range=(-1,1)), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('minmax', MinMaxScaler(feature_range=(-1, 1)),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('minmax', MinMaxScaler(feature_range=(-1, 1)),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) minmax['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  MinMaxScaler?Documentation for MinMaxScalerMinMaxScaler(feature_range=(-1, 1)) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      minmax__Sale_Price  ...  remainder__Latitude\n0                 -0.455  ...               42.054\n1                 -0.752  ...               42.053\n2                 -0.571  ...               42.053\n3                 -0.377  ...               42.051\n4                 -0.523  ...               42.061\n...                  ...  ...                  ...\n2925              -0.650  ...               41.989\n2926              -0.681  ...               41.988\n2927              -0.679  ...               41.987\n2928              -0.576  ...               41.991\n2929              -0.528  ...               41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Range Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-maxabs.html",
    "href": "numeric-maxabs.html",
    "title": "9  Max Abs Scaling",
    "section": "",
    "text": "9.1 Max Abs Scaling\nThe Max-Abs scaling method works by making sure that the training data lies within the range \\([-1, 1]\\) by applying the following formula\n\\[\nX_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\n\\tag{9.1}\\]\nThis is similar to the scaling we saw in Chapter 7. And we see that the only difference is whether we are aiming for the statistical properly (standard deviation of 1) or a specific decision (dividing by the largest quantity seen). This method is a learned transformation. So we use the training data to derive the right value of \\(\\text{max}(\\text{abs}(X))\\) and then this value is used to perform the transformations when applied to new data. For this, there is no specific guidance as to which method you want to use and you need to look at your data and see what works best.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Max Abs Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-maxabs.html#pros-and-cons",
    "href": "numeric-maxabs.html#pros-and-cons",
    "title": "9  Max Abs Scaling",
    "section": "9.2 Pros and Cons",
    "text": "9.2 Pros and Cons\n\n9.2.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\nDoesn’t affect sparsity\nCan be used on a zero variance variable. Doesn’t matter much since you likely should get rid of it\n\n\n\n9.2.2 Cons\n\nIs highly affected by outliers",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Max Abs Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-maxabs.html#r-examples",
    "href": "numeric-maxabs.html#r-examples",
    "title": "9  Max Abs Scaling",
    "section": "9.3 R Examples",
    "text": "9.3 R Examples\nWe will be using the ames data set for these examples.\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\nWe will be using the step_maxabs() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_maxabs(all_numeric_predictors()) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   0.148        0.147        0.07  \n 2     105000   0.0540       0.0983       0     \n 3     172000   0.0663       0.276        0.0675\n 4     244000   0.0518       0            0     \n 5     189900   0.0643       0.149        0     \n 6     195500   0.0464       0.253        0.0125\n 7     213500   0.0229       0            0     \n 8     191500   0.0233       0            0     \n 9     236500   0.0250       0.166        0     \n10     189000   0.0348       0.0983       0     \n# ℹ 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 × 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   max          313 maxabs_Bp5vK\n 2 Lot_Area       max       215245 maxabs_Bp5vK\n 3 Year_Built     max         2010 maxabs_Bp5vK\n 4 Year_Remod_Add max         2010 maxabs_Bp5vK\n 5 Mas_Vnr_Area   max         1600 maxabs_Bp5vK\n 6 BsmtFin_SF_1   max            7 maxabs_Bp5vK\n 7 BsmtFin_SF_2   max         1526 maxabs_Bp5vK\n 8 Bsmt_Unf_SF    max         2336 maxabs_Bp5vK\n 9 Total_Bsmt_SF  max         6110 maxabs_Bp5vK\n10 First_Flr_SF   max         5095 maxabs_Bp5vK\n# ℹ 23 more rows",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Max Abs Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-maxabs.html#python-examples",
    "href": "numeric-maxabs.html#python-examples",
    "title": "9  Max Abs Scaling",
    "section": "9.4 Python Examples",
    "text": "9.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the MaxAbsScaler() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MaxAbsScaler\n\nct = ColumnTransformer(\n    [('maxabs', MaxAbsScaler(), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('maxabs', MaxAbsScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('maxabs', MaxAbsScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) maxabs['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  MaxAbsScaler?Documentation for MaxAbsScalerMaxAbsScaler() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      maxabs__Sale_Price  ...  remainder__Latitude\n0                  0.285  ...               42.054\n1                  0.139  ...               42.053\n2                  0.228  ...               42.053\n3                  0.323  ...               42.051\n4                  0.252  ...               42.061\n...                  ...  ...                  ...\n2925               0.189  ...               41.989\n2926               0.174  ...               41.988\n2927               0.175  ...               41.987\n2928               0.225  ...               41.991\n2929               0.249  ...               41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Max Abs Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-robust.html",
    "href": "numeric-robust.html",
    "title": "10  Robust Scaling",
    "section": "",
    "text": "10.1 Robust Scaling\nRobust scaling, is a scaling method that is typically done by removing the median and dividing by the interquartile range. As illustrated by Equation 10.1\n\\[\nX_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)}\n\\tag{10.1}\\]\nThis is the most common formulation of this method. This method is a learned transformation. So we use the training data to derive the right values of \\(\\text{Q3}(X)\\), \\(\\text{Q1}(X)\\), and \\(\\text{median}(X)\\) and then these values are used to perform the transformations when applied to new data. You are not bound to use Q1 and Q3, any quantiles can be used. Most software implementations allow you to modify the ranges. It is typically recommended that you pick a symmetric range like \\([0.1, 0.9]\\) or \\([0.3, 0.7]\\) unless you have a good reason why observations high or low should be excluded.\nThis method is normally showcased as a way to scale variables with outliers in them. That is true, so far that you don’t take the outer quantiles into consideration. The default range means that only 50% of the observations are used to calculate the scaling statistics. This is fine if you want to ignore the outliers, however, it is conventionally not a good idea to outright ignore outliers, so you might want to take a look at outlier issues before you throw away the information that is present in the outliers.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Robust Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-robust.html#pros-and-cons",
    "href": "numeric-robust.html#pros-and-cons",
    "title": "10  Robust Scaling",
    "section": "10.2 Pros and Cons",
    "text": "10.2 Pros and Cons\n\n10.2.1 Pros\n\nIsn’t affected by outliers\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n10.2.2 Cons\n\nCompletely ignores part of the data outside the quantile ranges\nDoesn’t work with near zero variance data as Q1(x) - Q3(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn’t preserved",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Robust Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-robust.html#r-examples",
    "href": "numeric-robust.html#r-examples",
    "title": "10  Robust Scaling",
    "section": "10.3 R Examples",
    "text": "10.3 R Examples\nWe will be using the ames data set for these examples.\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ℹ 2,920 more rows\n\n\nWe will be using the step_robust() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_robust(all_numeric_predictors()) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    5.43         1.25         0.688\n 2     105000    0.531        0.833        0    \n 3     172000    1.17         2.34         0.664\n 4     244000    0.419        0            0    \n 5     189900    1.07         1.26         0    \n 6     195500    0.132        2.14         0.123\n 7     213500   -1.10         0            0    \n 8     191500   -1.08         0            0    \n 9     236500   -0.984        1.41         0    \n10     189000   -0.471        0.833        0    \n# ℹ 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   lower        43  robust_Bp5vK\n 2 Lot_Frontage   median       63  robust_Bp5vK\n 3 Lot_Frontage   higher       78  robust_Bp5vK\n 4 Lot_Area       lower      7440. robust_Bp5vK\n 5 Lot_Area       median     9436. robust_Bp5vK\n 6 Lot_Area       higher    11555. robust_Bp5vK\n 7 Year_Built     lower      1954  robust_Bp5vK\n 8 Year_Built     median     1973  robust_Bp5vK\n 9 Year_Built     higher     2001  robust_Bp5vK\n10 Year_Remod_Add lower      1965  robust_Bp5vK\n# ℹ 89 more rows\n\n\nWe can also change the default range to allow more of the distribution to affect the calculations. This is done using the range argument.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_robust(all_numeric_predictors(), range = c(0.1, 0.9)) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.35          0.820       0.350 \n 2     105000   0.230         0.547       0     \n 3     172000   0.509         1.53        0.337 \n 4     244000   0.181         0           0     \n 5     189900   0.463         0.828       0     \n 6     195500   0.0570        1.41        0.0625\n 7     213500  -0.475         0           0     \n 8     191500  -0.467         0           0     \n 9     236500  -0.426         0.925       0     \n10     189000  -0.204         0.547       0     \n# ℹ 2,920 more rows\n\n\nwhen we pull out the ranges, we see that they are wider\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   lower         0  robust_RUieL\n 2 Lot_Frontage   median       63  robust_RUieL\n 3 Lot_Frontage   higher       91  robust_RUieL\n 4 Lot_Area       lower      4800  robust_RUieL\n 5 Lot_Area       median     9436. robust_RUieL\n 6 Lot_Area       higher    14299. robust_RUieL\n 7 Year_Built     lower      1925. robust_RUieL\n 8 Year_Built     median     1973  robust_RUieL\n 9 Year_Built     higher     2006  robust_RUieL\n10 Year_Remod_Add lower      1950  robust_RUieL\n# ℹ 89 more rows",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Robust Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-robust.html#python-examples",
    "href": "numeric-robust.html#python-examples",
    "title": "10  Robust Scaling",
    "section": "10.4 Python Examples",
    "text": "10.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the RobustScaler() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import RobustScaler\n\nct = ColumnTransformer(\n    [('robust', RobustScaler(), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('robust', RobustScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('robust', RobustScaler(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) robust['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  RobustScaler?Documentation for RobustScalerRobustScaler() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      robust__Sale_Price  ...  remainder__Latitude\n0                  0.655  ...               42.054\n1                 -0.655  ...               42.053\n2                  0.143  ...               42.053\n3                  1.000  ...               42.051\n4                  0.356  ...               42.061\n...                  ...  ...                  ...\n2925              -0.208  ...               41.989\n2926              -0.345  ...               41.988\n2927              -0.333  ...               41.987\n2928               0.119  ...               41.991\n2929               0.333  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nWe can also change the default range (0.25, 0.75) to allow more of the distribution to affect the calculations. This is done using the quantile_range argument.\n\nct = ColumnTransformer(\n    [('robust', RobustScaler(quantile_range=(10.0, 90.0)), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('robust',\n                                 RobustScaler(quantile_range=(10.0, 90.0)),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('robust',\n                                 RobustScaler(quantile_range=(10.0, 90.0)),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) robust['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  RobustScaler?Documentation for RobustScalerRobustScaler(quantile_range=(10.0, 90.0)) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      robust__Sale_Price  ...  remainder__Latitude\n0                  0.313  ...               42.054\n1                 -0.313  ...               42.053\n2                  0.068  ...               42.053\n3                  0.478  ...               42.051\n4                  0.170  ...               42.061\n...                  ...  ...                  ...\n2925              -0.100  ...               41.989\n2926              -0.165  ...               41.988\n2927              -0.159  ...               41.987\n2928               0.057  ...               41.991\n2929               0.159  ...               41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Robust Scaling</span>"
    ]
  },
  {
    "objectID": "numeric-binning.html",
    "href": "numeric-binning.html",
    "title": "11  Binning",
    "section": "",
    "text": "11.1 Binning\nBinning, is one way to represent a curve that makes it useful in modeling content since it allows us to model non-linear relationships between predictors and outcomes. This is a trained method.\nBeing able to transform a numeric variable that has a non-linear relationship with the outcome into one or more variables that do have linear relationships with the outcome is of great importance, as many models wouldn’t be able to work with these types of variables effectively themselves. Below is a toy example of one such variable\nHere we have a non-linear relationship. It is a fairly simple one, the outcome is high when the predictor takes values between 25 and 50, and outside the ranges, it takes over values. Given that this is a toy example, we do not have any expert knowledge regarding what we expect the relationship to be outside this range. The trend could go back up, it could go down or flatten out. We don’t know.\nBelow we see the way each of the new variables activates different areas of the domain.\nWhat we see in this example is that the regions appear to be evenly spaced over the observed values of the predictor. This is neither and good nor bad thing, but something we need to think about. Ideally, we want the different regions to represent something shared within them. If the cuts are happening at non-ideal locations we don’t get the full benefit. See below for such an example\nIdeally, we would want the region with the highest values of outcome to be in the same region. but It appears to be split between Bin 2 and Bin 3.\nBelow is a chart of how well using evenly split binning works when using it on our toy example. We notice that using binning doesn’t work that well for this type of data. And it makes sense, binning will naturally create a step function like we see below, and if your predictor doesn’t have that shape, you are unlikely to see as good performance as with Splines or Polynomial Expansion.\nImagine we have a different data set, where the relationship between predictor and outcomes isn’t as continuous, but instead has some breaks in it.\nThe evenly spaced cut points don’t give us good performance, since they didn’t happen to align with the breaks in the data. What we can do instead is to use a more supervised method of finding these splits. Such as using a CART model of xgboost model, to find the optimal breaks in the data. This would make the method supervised and outcome-dependent. Below is an example where we use a CARt model.\nWhile these results look promising, there are a couple of things that are worth remembering. Firstly, this is not a silver bullet, as we still need to tune the method, to find the optimal value for the model we are fitting. Secondly, this type of data is not seen that much in the wild, and if you are seeing them in your application, there is a good chance that you can manually encode this information without having to resort to these methods.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binning</span>"
    ]
  },
  {
    "objectID": "numeric-binning.html#pros-and-cons",
    "href": "numeric-binning.html#pros-and-cons",
    "title": "11  Binning",
    "section": "11.2 Pros and Cons",
    "text": "11.2 Pros and Cons\n\n11.2.1 Pros\n\nWorks fast computationally\nBehaves predictably outside the range of the predictors\nIf cuts are placed well, it can handle sudden changes in distributions\nInterpretable\ndoesn’t create correlated features\n\n\n\n11.2.2 Cons\n\nThe inherent rounding that happens, can lead to loss of performance and interpretations\narguably less interpretable than binning\ncan produce a lot of variables",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binning</span>"
    ]
  },
  {
    "objectID": "numeric-binning.html#r-examples",
    "href": "numeric-binning.html#r-examples",
    "title": "11  Binning",
    "section": "11.3 R Examples",
    "text": "11.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 × 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ℹ 2,920 more rows\n\n\n{recipes} has the function step_discretize() for just this occasion.\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 2\n$ Lot_Area   &lt;fct&gt; bin4, bin4, bin4, bin3, bin4, bin3, bin1, bin1, bin1, bin2,…\n$ Year_Built &lt;fct&gt; bin2, bin2, bin2, bin2, bin3, bin3, bin3, bin3, bin3, bin3,…\n\n\nIf you don’t like the default number of breaks created, you can use the num_breaks = 6 argument to change it.\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built, num_breaks = 6)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 2\n$ Lot_Area   &lt;fct&gt; bin6, bin5, bin6, bin5, bin6, bin4, bin1, bin1, bin1, bin2,…\n$ Year_Built &lt;fct&gt; bin2, bin3, bin2, bin3, bin5, bin5, bin5, bin4, bin4, bin5,…\n\n\nThis step technically creates a factor variable, but we can turn it into a series of indicator functions with step_dummy()\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built, num_breaks = 6) |&gt;\n  step_dummy(Lot_Area, Year_Built, one_hot = TRUE)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 12\n$ Lot_Area_bin1   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Lot_Area_bin2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, …\n$ Lot_Area_bin3   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, …\n$ Lot_Area_bin4   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, …\n$ Lot_Area_bin5   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, …\n$ Lot_Area_bin6   &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ Year_Built_bin1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year_Built_bin2 &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year_Built_bin3 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Year_Built_bin4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ Year_Built_bin5 &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, …\n$ Year_Built_bin6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binning</span>"
    ]
  },
  {
    "objectID": "numeric-binning.html#python-examples",
    "href": "numeric-binning.html#python-examples",
    "title": "11  Binning",
    "section": "11.4 Python Examples",
    "text": "11.4 Python Examples",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Binning</span>"
    ]
  },
  {
    "objectID": "numeric-splines.html",
    "href": "numeric-splines.html",
    "title": "12  Splines",
    "section": "",
    "text": "12.1 Splines\nSplines, is one way to represent a curve that makes it useful in modeling content, since it allows us to model non-linear relationships between predictors and outcomes. This is a trained method.\nBeing able to transform a numeric variable that has a non-linear relationship with the outcome into one or more variables that do have linear relationships with the outcome is of great importance, as many models wouldn’t be able to work with these types of variables effectively themselves. Below is a toy example of one such variable\nHere we have a non-linear relationship. It is a fairly simple one, the outcome is high when the predictor takes values between 25 and 50, and outside the ranges, it takes over values. Given that this is a toy example, we do not have any expert knowledge regarding what we expect the relationship to be outside this range. The trend could go back up, it could go down or flatten out. We don’t know.\nAs we saw in the Binning chapter, one way to deal with this non-linearity is to chop up the predictor and emit indicators for which region the values take. While this works, we are losing quite a lot of detail by the rounding that occurs. This is where splines come in. Imagine that instead of indicators for whether a value is within a range, we have a set of functions that gives each predictor value a set of values, related to its location in the distribution.\nAbove we see an example of a Basis function that creates 6 features. The curves represent the area where they are “activated”. So if the predictor has a value of 15 then the first basis function returns 0.40, the second basis function returns 0.20 and so one, with the last basis function returning 0.00 since it is all flat over there.\nThis is a trained method as the location and shape of these functions are determined by the distribution of the variables we are trying to apply the spline to.\nSo in this example, we are taking 1 numeric variable and turning it into 6 numeric variables.\nThis spline is set up in such a way, that each spline function signals if the values are close to a given region. This way we have a smooth transition throughout the distribution.\nIf we take a look at how this plays out when we bring back the outcome we can look at this visualization\nand we have that since the different spline features highlight different parts of the predictor, we have that at least some of them are useful when we look at the relationship between the predictor and the outcome.\nIt is important to point out that this transformation only uses the predictor variable to do its calculations. And the fact that it works in a modeling sense is that the outcome predictor relationship in this case, and many real-life cases, can helpfully be explained by “the predictor value has these values”.\nAs we see in the above visualization, some of these new predictors are not much better than the original. But a couple of them do appear to work pretty well, especially the 3rd one. Depending on which model we use, having these 6 variables is gonna give us higher performance than using the original variable alone.\nOne thing to note is that you will get back correlated features when using splines. Some values of the predictor will influence multiple of the spline features as the spline functions overlap. This is expected but is worth noting. If you are using a model type that doesn’t handle correlated features well, then you should take a look at the methods outlined in the Correlated section for ways to deal with correlated features.\nLastly, the above spline functions you saw were called B-splines, but they are not the only kind of splines you can use.\nAbove we see several different kinds of splines. As we can see they are all trying to do different things. You generally can’t go too wrong by picking any of them, but knowing the data can help guide which of them you should use. The M-splines intuitively can be seen as threshold features. The periodic example is also interesting. Many of the types of splines can be formulated to work periodically. This can be handy for data that has a naturally periodic nature to them.\nBelow is a chart of how well using splines works when using it on our toy example. Since the data isn’t that complicated, a small deg_free is sufficient to fit the data well.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Splines</span>"
    ]
  },
  {
    "objectID": "numeric-splines.html#pros-and-cons",
    "href": "numeric-splines.html#pros-and-cons",
    "title": "12  Splines",
    "section": "12.2 Pros and Cons",
    "text": "12.2 Pros and Cons\n\n12.2.1 Pros\n\nWorks fast computationally\nGood performance compared to binning\nis good at handling continuous changes in predictors\n\n\n\n12.2.2 Cons\n\narguably less interpretable than binning\ncreates correlated features\ncan produce a lot of variables\nhave a hard time modeling sudden changes in distributions",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Splines</span>"
    ]
  },
  {
    "objectID": "numeric-splines.html#r-examples",
    "href": "numeric-splines.html#r-examples",
    "title": "12  Splines",
    "section": "12.3 R Examples",
    "text": "12.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 × 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ℹ 2,920 more rows\n\n\n{recipes} provides a number of steps to perform spline operations, each of them starting with step_spline_. Let us use a B-spline and a M-spline as examples here:\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area) |&gt;\n  step_spline_monotone(Year_Built)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 20\n$ Lot_Area_01   &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.00…\n$ Lot_Area_02   &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, …\n$ Lot_Area_03   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0…\n$ Lot_Area_04   &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, …\n$ Lot_Area_05   &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0078526499, …\n$ Lot_Area_06   &lt;dbl&gt; 0.000000000, 0.283603274, 0.000000000, 0.507327055, 0.00…\n$ Lot_Area_07   &lt;dbl&gt; 0.73399934, 0.71382012, 0.96474057, 0.48414256, 0.971047…\n$ Lot_Area_08   &lt;dbl&gt; 2.408258e-01, 2.576602e-03, 3.503161e-02, 6.777374e-04, …\n$ Lot_Area_09   &lt;dbl&gt; 2.444735e-02, 3.441535e-09, 2.277849e-04, 0.000000e+00, …\n$ Lot_Area_10   &lt;dbl&gt; 7.274651e-04, 0.000000e+00, 3.035128e-08, 0.000000e+00, …\n$ Year_Built_01 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Year_Built_02 &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1…\n$ Year_Built_03 &lt;dbl&gt; 0.9991483, 0.9995281, 0.9977527, 1.0000000, 1.0000000, 1…\n$ Year_Built_04 &lt;dbl&gt; 0.9041892, 0.9210803, 0.8649607, 0.9884577, 1.0000000, 1…\n$ Year_Built_05 &lt;dbl&gt; 0.20672563, 0.24041792, 0.14882796, 0.54043388, 0.999999…\n$ Year_Built_06 &lt;dbl&gt; 4.792231e-04, 1.169978e-03, 2.995144e-05, 3.881537e-02, …\n$ Year_Built_07 &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 4.491099e-07, …\n$ Year_Built_08 &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.221125…\n$ Year_Built_09 &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, …\n$ Year_Built_10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n\n\nWe can set the deg_free argument to specify how many spline features we want for each of the splines.\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area, deg_free = 3) |&gt;\n  step_spline_monotone(Year_Built, deg_free = 4)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 7\n$ Lot_Area_1   &lt;dbl&gt; 0.31422525, 0.13110895, 0.16045431, 0.12580964, 0.1557218…\n$ Lot_Area_2   &lt;dbl&gt; 0.0521839123, 0.0066461383, 0.0103524317, 0.0060782666, 0…\n$ Lot_Area_3   &lt;dbl&gt; 2.888757e-03, 1.123014e-04, 2.226446e-04, 9.788684e-05, 2…\n$ Year_Built_1 &lt;dbl&gt; 0.9827669, 0.9841047, 0.9798397, 0.9914201, 0.9999212, 0.…\n$ Year_Built_2 &lt;dbl&gt; 0.8614458, 0.8686207, 0.8464715, 0.9129756, 0.9968924, 0.…\n$ Year_Built_3 &lt;dbl&gt; 0.5411581, 0.5539857, 0.5156159, 0.6440229, 0.9532064, 0.…\n$ Year_Built_4 &lt;dbl&gt; 0.1653539, 0.1729990, 0.1508264, 0.2341901, 0.6731684, 0.…\n\n\nThese steps have more arguments, so we can change other things. The B-splines created by step_spline_b() default to cubic splines, but we can change that by specifying which polynomial degree with want with the degree argument.\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area, deg_free = 3, degree = 1) |&gt;\n  step_spline_monotone(Year_Built, deg_free = 4)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 7\n$ Lot_Area_1   &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.0000000…\n$ Lot_Area_2   &lt;dbl&gt; 0.89690903, 0.99540159, 0.98247163, 0.99766006, 0.9846078…\n$ Lot_Area_3   &lt;dbl&gt; 0.103090969, 0.004598405, 0.017528365, 0.002339940, 0.015…\n$ Year_Built_1 &lt;dbl&gt; 0.9827669, 0.9841047, 0.9798397, 0.9914201, 0.9999212, 0.…\n$ Year_Built_2 &lt;dbl&gt; 0.8614458, 0.8686207, 0.8464715, 0.9129756, 0.9968924, 0.…\n$ Year_Built_3 &lt;dbl&gt; 0.5411581, 0.5539857, 0.5156159, 0.6440229, 0.9532064, 0.…\n$ Year_Built_4 &lt;dbl&gt; 0.1653539, 0.1729990, 0.1508264, 0.2341901, 0.6731684, 0.…",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Splines</span>"
    ]
  },
  {
    "objectID": "numeric-splines.html#python-examples",
    "href": "numeric-splines.html#python-examples",
    "title": "12  Splines",
    "section": "12.4 Python Examples",
    "text": "12.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the SplineTransformer() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import SplineTransformer\n\nct = ColumnTransformer(\n    [('spline', SplineTransformer(), ['Lot_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('spline', SplineTransformer(), ['Lot_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('spline', SplineTransformer(), ['Lot_Area'])]) spline['Lot_Area']  SplineTransformer?Documentation for SplineTransformerSplineTransformer() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      spline__Lot_Area_sp_0  ...  remainder__Latitude\n0                     0.013  ...               42.054\n1                     0.088  ...               42.053\n2                     0.072  ...               42.053\n3                     0.090  ...               42.051\n4                     0.075  ...               42.061\n...                     ...  ...                  ...\n2925                  0.112  ...               41.989\n2926                  0.105  ...               41.988\n2927                  0.095  ...               41.987\n2928                  0.098  ...               41.991\n2929                  0.100  ...               41.989\n\n[2930 rows x 80 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Splines</span>"
    ]
  },
  {
    "objectID": "numeric-polynomial.html",
    "href": "numeric-polynomial.html",
    "title": "13  Polynomial Expansion",
    "section": "",
    "text": "13.1 Polynomial Expansion\nPolynomial expansion, is one way to turn a numeric variable into something that can represent a non-linear relationship between two variables. This is useful in modeling content since it allows us to model non-linear relationships between predictors and outcomes. This is a trained method.\nBeing able to transform a numeric variable that has a non-linear relationship with the outcome into one or more variables that do have linear relationships with the outcome is of great importance, as many models wouldn’t be able to work with these types of variables effectively themselves. Below is a toy example of one such variable\nHere we have a non-linear relationship. It is a fairly simple one, the outcome is high when the predictor takes values between 25 and 50, and outside the ranges, it takes over values. Given that this is a toy example, we do not have any expert knowledge regarding what we expect the relationship to be outside this range. The trend could go back up, it could go down or flatten out. We don’t know.\nAs we saw in the Binning, one way to deal with this non-linearity is to chop up the predictor and emit indicators for which region the values take. While this works, we are losing quite a lot of detail by the rounding that occurs.\nWe know that we can fit a polynomial function to some data set. And it would take the following format\n\\[\n\\text{poly}(x,\\ \\text{degree} = n) = a_0 + a_1 x + a_2 x ^ 2 + \\cdots + a_n x ^ n\n\\]\nThis can then be used to generate features. Each feature is done by taking the value to a given degree and multiplying it according to the corresponding coefficient.\nIn the above table, we see a small example of how this could be done, using a 4th-degree polynomial with coefficients of 1. If we were to look at the individual functions over the domain of our data we see the following\nThis is to be expected, but we are noticing that these curves look quite similar, and the values these functions are taking very quickly escalate. And this is for predictor values between 0 and 100, higher values will get even higher, possibly causing overflow issues.\nWe also have an issue where the values appear quite correlated since the functions are all increasing. As we see below, the correlation between the variables is close to 1 for all pairs.\nThis is a problem that needs to be dealt with. The way we can deal with this is by calculating orthogonal polynomials instead. We have that any set polynomial function can be rewritten as a set of orthogonal polynomial functions.\nWith this, we deal with the two problems we had before. As seen in the figure below, the functions take smaller values within their ranges\nAnd since they are orthogonal by design, we won’t have to worry about correlated features.\nThe interpretation of these polynomial features is not as easy as with Ninning or Splines, but the calculations are quite fast and versatile.\nBelow is a chart of how well using polynomial expansion works when using it on our toy example. Since the data isn’t that complicated, having a degree larger than 1 will do the trick.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Polynomial Expansion</span>"
    ]
  },
  {
    "objectID": "numeric-polynomial.html#pros-and-cons",
    "href": "numeric-polynomial.html#pros-and-cons",
    "title": "13  Polynomial Expansion",
    "section": "13.2 Pros and Cons",
    "text": "13.2 Pros and Cons\n\n13.2.1 Pros\n\nWorks fast computationally\nGood performance compared to binning\nDoesn’t create correlated features\nis good at handling continuous changes in predictors\n\n\n\n13.2.2 Cons\n\narguably less interpretable than binning and splines\ncan produce a lot of variables\nhave a hard time modeling sudden changes in distributions",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Polynomial Expansion</span>"
    ]
  },
  {
    "objectID": "numeric-polynomial.html#r-examples",
    "href": "numeric-polynomial.html#r-examples",
    "title": "13  Polynomial Expansion",
    "section": "13.3 R Examples",
    "text": "13.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 × 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ℹ 2,920 more rows\n\n\n{recipes} has the function step_poly() for just this occasion.\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built)\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 4\n$ Lot_Area_poly_1   &lt;dbl&gt; 5.070030e-02, 3.456477e-03, 9.658577e-03, 2.373161e-…\n$ Lot_Area_poly_2   &lt;dbl&gt; -0.052288355, -0.006139895, -0.013560043, -0.0048015…\n$ Year_Built_poly_1 &lt;dbl&gt; -0.0069377547, -0.0063268386, -0.0081595868, -0.0020…\n$ Year_Built_poly_2 &lt;dbl&gt; -0.0188536923, -0.0189190631, -0.0186090288, -0.0183…\n\n\nIf you don’t like the default number of features created, you can use the degree argument to change it.\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built, degree = 5)\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 10\n$ Lot_Area_poly_1   &lt;dbl&gt; 5.070030e-02, 3.456477e-03, 9.658577e-03, 2.373161e-…\n$ Lot_Area_poly_2   &lt;dbl&gt; -0.052288355, -0.006139895, -0.013560043, -0.0048015…\n$ Lot_Area_poly_3   &lt;dbl&gt; 0.0024951091, 0.0067956902, 0.0110336270, 0.00588901…\n$ Lot_Area_poly_4   &lt;dbl&gt; 0.0390305341, -0.0078110499, -0.0092519823, -0.00723…\n$ Lot_Area_poly_5   &lt;dbl&gt; -0.0649379780, 0.0051370320, 0.0004088393, 0.0055404…\n$ Year_Built_poly_1 &lt;dbl&gt; -0.0069377547, -0.0063268386, -0.0081595868, -0.0020…\n$ Year_Built_poly_2 &lt;dbl&gt; -0.0188536923, -0.0189190631, -0.0186090288, -0.0183…\n$ Year_Built_poly_3 &lt;dbl&gt; -0.0031709327, -0.0044248985, -0.0006208212, -0.0124…\n$ Year_Built_poly_4 &lt;dbl&gt; 1.420211e-02, 1.311711e-02, 1.609112e-02, 3.358699e-…\n$ Year_Built_poly_5 &lt;dbl&gt; 0.009938840, 0.011096007, 0.007277173, 0.015173692, …\n\n\nwhile you properly shouldn’t, you can turn off the orthogonal polynomials by setting options = list(raw = TRUE).\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built, options = list(raw = TRUE))\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 4\n$ Lot_Area_poly_1   &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005,…\n$ Lot_Area_poly_2   &lt;dbl&gt; 1009332900, 135070884, 203547289, 124545600, 1912689…\n$ Year_Built_poly_1 &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995…\n$ Year_Built_poly_2 &lt;dbl&gt; 3841600, 3845521, 3833764, 3873024, 3988009, 3992004…",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Polynomial Expansion</span>"
    ]
  },
  {
    "objectID": "numeric-polynomial.html#python-examples",
    "href": "numeric-polynomial.html#python-examples",
    "title": "13  Polynomial Expansion",
    "section": "13.4 Python Examples",
    "text": "13.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the PolynomialFeatures(). We can use it out of the box.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nct = ColumnTransformer(\n    [('polynomial', PolynomialFeatures(), ['Lot_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('polynomial', PolynomialFeatures(),\n                                 ['Lot_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('polynomial', PolynomialFeatures(),\n                                 ['Lot_Area'])]) polynomial['Lot_Area']  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      polynomial__1  ...  remainder__Latitude\n0               1.0  ...               42.054\n1               1.0  ...               42.053\n2               1.0  ...               42.053\n3               1.0  ...               42.051\n4               1.0  ...               42.061\n...             ...  ...                  ...\n2925            1.0  ...               41.989\n2926            1.0  ...               41.988\n2927            1.0  ...               41.987\n2928            1.0  ...               41.991\n2929            1.0  ...               41.989\n\n[2930 rows x 76 columns]\n\n\nOr we can change the degree using the degree argument\n\nct = ColumnTransformer(\n    [('polynomial', PolynomialFeatures(degree = 4), ['Lot_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('polynomial', PolynomialFeatures(degree=4),\n                                 ['Lot_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('polynomial', PolynomialFeatures(degree=4),\n                                 ['Lot_Area'])]) polynomial['Lot_Area']  PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures(degree=4) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      polynomial__1  ...  remainder__Latitude\n0               1.0  ...               42.054\n1               1.0  ...               42.053\n2               1.0  ...               42.053\n3               1.0  ...               42.051\n4               1.0  ...               42.061\n...             ...  ...                  ...\n2925            1.0  ...               41.989\n2926            1.0  ...               41.988\n2927            1.0  ...               41.987\n2928            1.0  ...               41.991\n2929            1.0  ...               41.989\n\n[2930 rows x 78 columns]",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Polynomial Expansion</span>"
    ]
  },
  {
    "objectID": "numeric-arithmetic.html",
    "href": "numeric-arithmetic.html",
    "title": "14  Arithmetic",
    "section": "",
    "text": "14.1 Arithmetic\nWith domain knowledge, we can generate quite good features by combining multiple existing numeric variables. This chapter is less about methods, but rather exploring and reinforcing the idea that powerful transformations can happen when you apply simple arithmetic operators carefully and thoughtfully.\nAn important kind of transformation we can get with arithmetic operators is ratios. Simply dividing two columns by each other can give us better insights than what we could get without them. The use of ratios is already common practice in most industries. What we are trying to do is identify them and apply them if they are not already there. Ratios come in many fashions such as the debt ratio (total debt/total assets), bathrooms-to-bedrooms ratio (number of bathrooms/bedrooms), and orders-to-reviews ratio (orders/reviews).\nBelow we are seeing such a ratio in action. This fictitious data set contains the number of deposits and withdrawals from some customers. There is a clear effect between the predictors deposits and withdrawals and the outcome variable, but they contribute to the outcome together. By taking the ratio between these two variables we get a singular variable that has a linear relationship with the outcome.\nThe ordering of these ratios doesn’t matter to the performance but can be helpful later on when we try to interpret the model later on. Another thing we often do is to take the logarithm of the resulting ratio to give us some better intervals.\nWe can do some of the same tricks with multiplication as well. This is also known as feature crosses. Again we can have features that jointly contain information that relates to the outcome, in a way that combining them multiplicatively is better explained.\nRatios and crosses are in essence the same thing, with them being the same after the inversion of one of the variables. The same way division and multiplication are connected. Using these simple arithmetic operations we can combine 2 or more variables in ways that enhance our understanding of the data.\nOne could imagine that you could try all possible combinations for a set of variables and operations. That is likely to be too much as the number of created variables would increase exponentially, with each newly created variable being a candidate for future combinations. This is not to say that it is an impossible task but should be used as a last resort. Methods in Too Many section should be tried first.",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Arithmetic</span>"
    ]
  },
  {
    "objectID": "numeric-arithmetic.html#pros-and-cons",
    "href": "numeric-arithmetic.html#pros-and-cons",
    "title": "14  Arithmetic",
    "section": "14.2 Pros and Cons",
    "text": "14.2 Pros and Cons\n\n14.2.1 Pros\n\nCreate interpretable features\n\n\n\n14.2.2 Cons\n\nRequires manual effort to find and create",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Arithmetic</span>"
    ]
  },
  {
    "objectID": "numeric-arithmetic.html#r-examples",
    "href": "numeric-arithmetic.html#r-examples",
    "title": "14  Arithmetic",
    "section": "14.3 R Examples",
    "text": "14.3 R Examples",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Arithmetic</span>"
    ]
  },
  {
    "objectID": "numeric-arithmetic.html#python-examples",
    "href": "numeric-arithmetic.html#python-examples",
    "title": "14  Arithmetic",
    "section": "14.4 Python Examples",
    "text": "14.4 Python Examples",
    "crumbs": [
      "Numeric Features",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Arithmetic</span>"
    ]
  },
  {
    "objectID": "categorical.html",
    "href": "categorical.html",
    "title": "15  Categorical Overview",
    "section": "",
    "text": "15.1 Categorical Overview\nOne of the most common types of data you can encounter is categorical variables. These variables take non-numeric values and are things like; names, animal, housetype zipcode, mood, and so on. We call these qualitative variables and you will have to find a way to deal with them as they are plentiful in most data sets you will encounter. You will, however, unlike numerical variables, have to transform these variables into numeric variables as most models only work with numeric variables. The main exception to this is tree-based models such as decision trees, random forests, and boosted trees. This is of cause a theoretical fact, as some implementations of tree-based models don’t support categorical variables.\nThe above description is a small but useful simplification. A categorical variable can take a known or an unknown number of unique values. day_of_the_week and zipcode are examples of variables with a fixed known number of values. Even if our data only contains Sundays, Mondays, and Thursdays we know that there are 7 different possible options. On the other hand, there are plenty of categorical variables where the levels are realistically unknown, such as company_name, street_name, and food_item. This distinction matters as it can be used to inform the pre-processing that will be done.\nAnother nuance is some categorical variables have an inherent order to them. So the variables size with the values \"small\", \"medium\", and \"large\", clearly have an ordering to then \"small\" &lt; \"medium\" &lt; \"large\". This is unlike the theoretical car_color with the values “blue”,“red”, and“black”`, which doesn’t have a natural ordering. Depending on whether a variable has an ordering, we can use that information. This is something that doesn’t have to be dealt with, but the added information can be useful.\nLastly, we have the encoding these categorical variables have. The same variable household_pet can be encoded as [\"cat\", \"cat\", \"dog\", \"cat\", \"hamster\"] or as [1, 1, 2, 1, 3]. The latter (hopefully) is accompanied by a data dictionary saying [1 = \"cat\", 2 = \"dog\", 3 = \"hamster\"]. These variables contain the exact same information, but the encoding is vastly different, and if you are not careful to treat household_pet as a categorical variable the model believes that \"hamster\" - \"cat\" = \"dog\".\nThe chapters in this section can be put into 2 categories:",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical Overview</span>"
    ]
  },
  {
    "objectID": "categorical.html#categorical-to-categorical",
    "href": "categorical.html#categorical-to-categorical",
    "title": "15  Categorical Overview",
    "section": "15.2 Categorical to Categorical",
    "text": "15.2 Categorical to Categorical\nThese methods take a categorical variable and improve them. Whether it means cleaning levels, collapsing levels, or making sure it handles new levels correctly. These Tasks as not always needed depending on the method you are using but they are generally helpful to apply. One method that would have been located here if it wasn’t for the fact that it has a whole section by itself is dealing with missing values.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical Overview</span>"
    ]
  },
  {
    "objectID": "categorical.html#categorical-to-numerical",
    "href": "categorical.html#categorical-to-numerical",
    "title": "15  Categorical Overview",
    "section": "15.3 Categorical to Numerical",
    "text": "15.3 Categorical to Numerical\nThe vast majority of the chapters in these chapters concern methods that take a categorical variable and produce one or more numerical variables suitable for modeling. There are quite a lot of different methods, all have upsides and downsides and they will all be explored in the remaining chapters.\nThese methods can further be categorized. This categorization allows us to better compare and contrast the methods to see how they are similar and different. The vast majority of methods are applied one column at a time. The notable exception is the combination multi-dummy extraction method.\nThe one-column-at-a-time methods can be put into 3 groups; dummy-based, target-based, and other.\nThe methods that are variations of Dummy Encoding are:\n\nBinary Encoding\nHashing Encoding\nThermometer Encoding\nMulti-Dummy Encoding\n\nThe methods that are variations of Target Encoding are:\n\nLeave One Out Encoding\nLeaf Encoding\nGLMM Encoding\nCatBoost Encoding\nJames-Stein encoding\nM-Estimator Encoding\nQuantile Encoding\nSummary Encoding\n\nThe remaining methods don’t neatly slot into the above categories:\n\nLabel Encoding\nOrdinal Encoding\nFrequency Encoding\nWeight of Evidence Encoding\n\nWhat you might notice is that many of these methods boil down to a left join when applied to new data. The difference is how you calculate the table, which we could consider an embedding table.\nThis idea gives rise to another way to deal with categorical variables. This is done by manually creating enrichment tables and then using them with categorical variables. One could imagine having a city predictor. It could be effectively encoded by the methods seen in this section. But we might be able to add more signals by enriching with characteristics of the city, like population, n_hospitals, and country It can be seen as a simple version of relational methods as seen in Chapter 136.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Categorical Overview</span>"
    ]
  },
  {
    "objectID": "categorical-cleaning.html",
    "href": "categorical-cleaning.html",
    "title": "16  Cleaning",
    "section": "",
    "text": "16.1 Cleaning\nFor cleaning categorical data, there is a whole host of problems you can have in your data. We won’t be able to describe how to deal with every single type of problem. Instead, we will go over a class of common problems. In an ideal world, you wouldn’t have these problems, but that isn’t one we are living in right now. Mistakes are happening for a multitude of reasons and it is your job to deal with them.\nLook at the following vector of spellings of St Albans, the city in England.\nThey all refer to the same city name, they don’t just agree on the spelling. If we didn’t perform any cleaning, most of the following methods would assume that these spellings are completely different works and should be treated as such. Depending on the method this would vastly change the output and dilute any information you may have in your data.\nThis will of course depend on whether these spelling differences are of importance in your model. Only you, the practitioner, will know the answer to that question. This is where you need to be in touch with the people generating your data. In the above situation, one could imagine that the data comes from 2 sources; one using a drop-down menu to select the city, and one where the city is being typed in manually on a tablet. There will be big differences. In a case such as that, we would want all these spellings to be fixed up.\nDomain knowledge is important. In the case of spell-checking, we need to be careful not to over-correct the data by collapsing two different items together by accident.\nBack to the example at hand. The first thing we notice is that there is a difference in capitalization. In this case, since we presumably are working with a variable of city names, capitalization shouldn’t matter. A common trick is to either turn everything upper-case or lower-case. I prefer to lower-casing as I find the results easier to read.\nBy turning everything into lower-case, we were able to remove 2 of the errors. Next, we see that some of these spellings include periods. For this example, I’m going to decide that they are safe to remove as well.\nThis removes yet another of our problems. This next problem is horrible and is so easy to overlook. Notice how the second spelling has a double space between st and alban? At a glance that can be hard to see. Likewise, the last spelling has a trailing space. Let us fix this as well.\nNow we are left with two values, which we manually would have to deal with. But even if we had to manually write out the mapping, it is a lot easier to write it for 2 different spellings than 7.\nAnother problem that you may or may not run into, depends on the resilience of your modeling package. Some implementations are rather fragile when it comes to the column names of your data, Non-ASCII characters, punctuation and even spaces can cause errors. At this point in our journey, we are almost there, and we can replace the spaces with underscores to be left with st_albans.\nOne problem we didn’t see in the above example is what happens when you are working with accented characters. Consider the German word “schön.” The o with an umlaut (two dots over it) is a fairly simple character, but it can be represented in a couple of different ways. We can either use a single character \\U00f6 to represent the letter with an umlaut. Alternatively, we can use two characters, one for the o and one character to denote the presence of two dots over the previous character \\U0308. As you can imagine this can happen with many words if you have free-form data from a wide array of backgrounds. The method needed to fix this problem is called Unicode Normalization.\nThis chapter was not meant to scare you, what I hope you get away from this chapter is that it is very important that you look at your data carefully, especially categorical variables that can be varied in so many unexpected ways. Nevertheless, using the above-described techniques, combined with some subject matter expertise should get you quite a long way.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Cleaning</span>"
    ]
  },
  {
    "objectID": "categorical-cleaning.html#r-examples",
    "href": "categorical-cleaning.html#r-examples",
    "title": "16  Cleaning",
    "section": "16.2 R examples",
    "text": "16.2 R examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a good data set for these examples\n\n\nUse janitor\ntextrecipes::step_clean_levels()",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Cleaning</span>"
    ]
  },
  {
    "objectID": "categorical-cleaning.html#python-examples",
    "href": "categorical-cleaning.html#python-examples",
    "title": "16  Cleaning",
    "section": "16.3 Python Examples",
    "text": "16.3 Python Examples",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Cleaning</span>"
    ]
  },
  {
    "objectID": "categorical-unseen.html",
    "href": "categorical-unseen.html",
    "title": "17  Unseen Levels",
    "section": "",
    "text": "17.1 Unseen Levels\nWhen you are dealing with categorical variables, it is understood that they can take many values. And we have various methods about how to deal with these categorical values, regardless of what values they take. One problem that eventually will happen for you is that you try to apply a trained preprocessor on data that has levels in your categorical variable that you haven’t seen before. This can happen when you are fitting your model using resampled data when you are applying your model on the testing data set, or, if you are unlucky, at some future time in production.\nThe reason why you need to think about this problem is that some methods and/or models will complain and even error if you are providing unseen levels. Some implementations will allow you to deal with this at the method level. Other methods such as Hashing Encoding don’t care at all that you have unseen levels in your data.\nOne surefire way to deal with this issue is to add a step in your data preprocessing pipeline that will turn any unseen levels into \"unseen\". What this method does in practice, is that it looks at your categorical variables during training, taking note of all the levels it sees and saving them. Then any time the preprocessing is applied to new data it will look at the levels again, and if it sees a level it hasn’t seen, label it \"unseen\" (or any other meaningful label that doesn’t conflict with the data). This way, you have any future levels.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unseen Levels</span>"
    ]
  },
  {
    "objectID": "categorical-unseen.html#r-examples",
    "href": "categorical-unseen.html#r-examples",
    "title": "17  Unseen Levels",
    "section": "17.2 R Examples",
    "text": "17.2 R Examples\nWe will be using the nycflights13 data set. We are downsampling just a bit to only work on the first day and doing a test-train split.\n\nlibrary(recipes)\nlibrary(rsample)\nlibrary(nycflights13)\n\nflights &lt;- flights |&gt;\n  filter(year == 2013, month == 1, day == 1)\n\nset.seed(13630)\nflights_split &lt;- initial_split(flights)\nflights_train &lt;- training(flights_split)\nflights_test &lt;- testing(flights_split)\n\nNow we are doing the cardinal sin by looking at the testing data. But in this case, it is okay because we are doing it for educational purposes.\n\nflights_train |&gt; pull(carrier) |&gt; unique() |&gt; sort()\n\n [1] \"9E\" \"AA\" \"B6\" \"DL\" \"EV\" \"F9\" \"FL\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\nflights_test |&gt; pull(carrier) |&gt; unique() |&gt; sort()\n\n [1] \"9E\" \"AA\" \"AS\" \"B6\" \"DL\" \"EV\" \"FL\" \"HA\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\n\nNotice that the testing data includes the carrier \"AS\" and \"HA\" but the training data doesn’t know that. Let us see what would happen if we were to calculate dummy variables without doing any adjusting.\n\ndummy_spec &lt;- recipe(arr_delay ~ carrier, data = flights_train) |&gt;\n  step_dummy(carrier)\n\ndummy_spec_prepped &lt;- prep(dummy_spec)\n\nbake(dummy_spec_prepped, new_data = flights_test)\n\n# A tibble: 211 × 12\n   arr_delay carrier_AA carrier_B6 carrier_DL carrier_EV carrier_F9 carrier_FL\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        12          0          0          0          0          0          0\n 2         8          1          0          0          0          0          0\n 3       -14          0          0          0          0          0          0\n 4        -6          0          1          0          0          0          0\n 5        -3          1          0          0          0          0          0\n 6       -33          0          0          1          0          0          0\n 7        -7          1          0          0          0          0          0\n 8         5          0          1          0          0          0          0\n 9        31          1          0          0          0          0          0\n10       -10         NA         NA         NA         NA         NA         NA\n# ℹ 201 more rows\n# ℹ 5 more variables: carrier_MQ &lt;dbl&gt;, carrier_UA &lt;dbl&gt;, carrier_US &lt;dbl&gt;,\n#   carrier_VX &lt;dbl&gt;, carrier_WN &lt;dbl&gt;\n\n\nWe get a warning, and if you look at the rows that were affected we see that it produces NAs. Let us now use the function step_novel() that implements the above-described method.\n\nnovel_spec &lt;- recipe(arr_delay ~ carrier, data = flights_train) |&gt;\n  step_novel(carrier) |&gt;\n  step_dummy(carrier) \n\nnovel_spec_prepped &lt;- prep(novel_spec)\n\nbake(novel_spec_prepped, new_data = flights_test)\n\n# A tibble: 211 × 13\n   arr_delay carrier_AA carrier_B6 carrier_DL carrier_EV carrier_F9 carrier_FL\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        12          0          0          0          0          0          0\n 2         8          1          0          0          0          0          0\n 3       -14          0          0          0          0          0          0\n 4        -6          0          1          0          0          0          0\n 5        -3          1          0          0          0          0          0\n 6       -33          0          0          1          0          0          0\n 7        -7          1          0          0          0          0          0\n 8         5          0          1          0          0          0          0\n 9        31          1          0          0          0          0          0\n10       -10          0          0          0          0          0          0\n# ℹ 201 more rows\n# ℹ 6 more variables: carrier_MQ &lt;dbl&gt;, carrier_UA &lt;dbl&gt;, carrier_US &lt;dbl&gt;,\n#   carrier_VX &lt;dbl&gt;, carrier_WN &lt;dbl&gt;, carrier_new &lt;dbl&gt;\n\n\nAnd we see that we get no error or anything.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unseen Levels</span>"
    ]
  },
  {
    "objectID": "categorical-unseen.html#python-examples",
    "href": "categorical-unseen.html#python-examples",
    "title": "17  Unseen Levels",
    "section": "17.3 Python Examples",
    "text": "17.3 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Unseen Levels</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html",
    "href": "categorical-dummy.html",
    "title": "18  Dummy Encoding",
    "section": "",
    "text": "18.1 Dummy Encoding\nWe have some categorical variables and we want to turn them into numerical values, one of the most common ways of going about it is to create dummy variables. Dummy variables are variables that only take the values 0 and 1 to indicate the absence or presence of the levels in a categorical variable. This is nicely shown with an example.\nConsidering this short categorical variable of animals, we observe there are 3 unique values “cat”, “dog”, and “horse”.\nWith just this knowledge we can create the corresponding dummy variables. There should be 3 columns one for each of the levels\nFrom this, we have a couple of observations. Firstly the length of each of these variables is equal to the length of the original categorical variable. The number of columns corresponds to the number of levels. Lastly, the sum of all the values on each row equals 1 since all the rows contain one 1 and the remaining 0s. This means that for even a small number of levels, you get sparse data. Sparse data is data where there are a lot of zeroes, meaning that it would take less space to store where the non-zero values are instead of all the values. You can read more about how and when to care about sparse data in Chapter 144. What this means for dummy variable creation, is that depending on whether your software can handle sparse data, you might need to limit the number of levels in your categorical variables. One way to do this would be to collapse levels together, which you can read about in Chapter 35.\nDummy variable creation is a trained method. This means that during the training step, the levels of each categorical variable are saved, and then these and only these values are used for dummy variable creation. If we assumed that the above example data were used to train the preprocessor, and we passed in the values [\"dog\", \"cat\", \"cat\", \"dog\"] during future applications, we would expect the following dummy variables\nthe horse variable must be here too, even if it is empty as the subsequent preprocessing steps and model expect the horse variable to be present. Likewise, you can run into problems if the value \"duck\" was used as the preprocessor wouldn’t know what to do. These cases are talked about in Chapter 17 about unseen levels.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "href": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "title": "18  Dummy Encoding",
    "section": "18.2 Dummy or one-hot encoding",
    "text": "18.2 Dummy or one-hot encoding\n\n\n\n\n\n\nTODO\n\n\n\nadd diagram\n\n\nThe terms dummy encoding and one-hot encoding get thrown around interchangeably, but they do have different and distinct meanings. One-hot encoding is when you return k variables when you have k different levels. Like we have shown above\n\n\n\n\n\ncat\ndog\nhorse\n\n\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n0\n0\n1\n\n\n0\n1\n0\n\n\n1\n0\n0\n\n\n\n\n\nDummy encoding on the other hand returns k-1 variables, where the excluded one typically is the first one.\n\n\n\n\n\ndog\nhorse\n\n\n\n\n1\n0\n\n\n0\n0\n\n\n0\n1\n\n\n1\n0\n\n\n0\n0\n\n\n\n\n\nThese two encodings store the same information, even though the dummy encoding has 1 less column. Because we can deduce which observations are cat by finding the rows with all zeros. The main reason why one would use dummy variables is because of what some people call the dummy variable trap. When you use one-hot encoding, you are increasing the likelihood that you run into a collinearity problem. With the above example, if you included an intercept in your model you have that intercept = cat + dog + horse which gives perfect collinearity and would cause some models to error as they aren’t able to handle that.\n\n\n\n\n\n\nNote\n\n\n\nAn intercept is a variable that takes the value 1 for all entries.\n\n\nEven if you don’t include an intercept you could still run into collinearity. Imagine that in addition to the animal variable also creates a one-hot encoding of the home variable taking the two values \"house\" and \"apartment\", you would get the following indicator variables\n\n\n\n\n\ncat\ndog\nhorse\nhouse\napartment\n\n\n\n\n0\n1\n0\n0\n1\n\n\n1\n0\n0\n1\n0\n\n\n0\n0\n1\n0\n1\n\n\n0\n1\n0\n0\n1\n\n\n1\n0\n0\n1\n0\n\n\n\n\n\nAnd in this case, we have that house = cat + dog + horse - apartment which again is an example of perfect collinearity. Unless you have a reason to do otherwise I would suggest that you use dummy encoding in your models. Additionally, this leads to slightly smaller models as each categorical variable produces 1 less variable. It is worth noting that the choice between dummy encoding and one-hot encoding does matter for some models such as decision trees. Depending on what types of rules they can use. Being able to write animal == \"cat\" is easier then saying animal != \"dog\" & animal != \"horse\". This is unlikely to be an issue as many tree-based models can work with categorical variables directly without the need for encoding.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html#ordered-factors",
    "href": "categorical-dummy.html#ordered-factors",
    "title": "18  Dummy Encoding",
    "section": "18.3 Ordered factors",
    "text": "18.3 Ordered factors\n\n\n\n\n\n\nTODO\n\n\n\nfinish section",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html#contrasts",
    "href": "categorical-dummy.html#contrasts",
    "title": "18  Dummy Encoding",
    "section": "18.4 Contrasts",
    "text": "18.4 Contrasts\n\n\n\n\n\n\nTODO\n\n\n\nfinish section",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html#pros-and-cons",
    "href": "categorical-dummy.html#pros-and-cons",
    "title": "18  Dummy Encoding",
    "section": "18.5 Pros and Cons",
    "text": "18.5 Pros and Cons\n\n18.5.1 Pros\n\nVersatile and commonly used\nEasy interpretation\nWill rarely lead to a decrease in performance\n\n\n\n18.5.2 Cons\n\nDoes require fairly clean categorical levels\nCan be quite memory intensive if you have many levels in your categorical variables and you are unable to use sparse representation\nProvides a complete, but not necessarily compact set of variables",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html#r-examples",
    "href": "categorical-dummy.html#r-examples",
    "title": "18  Dummy Encoding",
    "section": "18.6 R Examples",
    "text": "18.6 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nAnd since MS_SubClass is a factor, we can verify that they match and that all the levels are observed\n\names |&gt; pull(MS_SubClass) |&gt; levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_dummy() step for this, which defaults to creating dummy variables\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 21\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1…\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; 0, 1, 0, 0, 0, 0…\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 1, 0, 1, 1, 1, 1…\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_C_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_I_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n\n\nWe can pull the factor levels for each variable by using tidy(). If a character vector was present in the data set, it would record the observed variables.\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 243 × 3\n   terms       columns                                id         \n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;      \n 1 MS_SubClass One_Story_1945_and_Older               dummy_Bp5vK\n 2 MS_SubClass One_Story_with_Finished_Attic_All_Ages dummy_Bp5vK\n 3 MS_SubClass One_and_Half_Story_Unfinished_All_Ages dummy_Bp5vK\n 4 MS_SubClass One_and_Half_Story_Finished_All_Ages   dummy_Bp5vK\n 5 MS_SubClass Two_Story_1946_and_Newer               dummy_Bp5vK\n 6 MS_SubClass Two_Story_1945_and_Older               dummy_Bp5vK\n 7 MS_SubClass Two_and_Half_Story_All_Ages            dummy_Bp5vK\n 8 MS_SubClass Split_or_Multilevel                    dummy_Bp5vK\n 9 MS_SubClass Split_Foyer                            dummy_Bp5vK\n10 MS_SubClass Duplex_All_Styles_and_Ages             dummy_Bp5vK\n# ℹ 233 more rows\n\n\nsetting one_hot = TRUE gives us the complete one-hot encoding results.\n\nonehot_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  prep()\n\nonehot_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 23\n$ MS_SubClass_One_Story_1946_and_Newer_All_Styles       &lt;dbl&gt; 1, 1, 1, 1, 0, 0…\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1…\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Floating_Village_Residential                &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; 0, 1, 0, 0, 0, 0…\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 1, 0, 1, 1, 1, 1…\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_C_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_I_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0…",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-dummy.html#python-examples",
    "href": "categorical-dummy.html#python-examples",
    "title": "18  Dummy Encoding",
    "section": "18.7 Python Examples",
    "text": "18.7 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the OneHotEncoder() method we can use. Below we see how it can be used with the MS_Zoning columns.\n\n\n\n\n\n\nNote\n\n\n\nWe are setting sparse_output=False in this example because we are having transform() return pandas data frames for better printing.\n\n\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer(\n    [('onehot', OneHotEncoder(sparse_output=False), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('onehot', OneHotEncoder(sparse_output=False),\n                                 ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('onehot', OneHotEncoder(sparse_output=False),\n                                 ['MS_Zoning'])]) onehot['MS_Zoning']  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(sparse_output=False) remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=(\"MS_Zoning.*\"))\n\n      onehot__MS_Zoning_A_agr  ...  onehot__MS_Zoning_Residential_Medium_Density\n0                         0.0  ...                                           0.0\n1                         0.0  ...                                           0.0\n2                         0.0  ...                                           0.0\n3                         0.0  ...                                           0.0\n4                         0.0  ...                                           0.0\n...                       ...  ...                                           ...\n2925                      0.0  ...                                           0.0\n2926                      0.0  ...                                           0.0\n2927                      0.0  ...                                           0.0\n2928                      0.0  ...                                           0.0\n2929                      0.0  ...                                           0.0\n\n[2930 rows x 7 columns]\n\n\nBy default OneHotEncoder() performs one-hot encoding, we can change this to dummy encoding by setting drop='first'.\n\nct = ColumnTransformer(\n    [('dummy', OneHotEncoder(sparse_output=False, drop='first'), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('dummy',\n                                 OneHotEncoder(drop='first',\n                                               sparse_output=False),\n                                 ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('dummy',\n                                 OneHotEncoder(drop='first',\n                                               sparse_output=False),\n                                 ['MS_Zoning'])]) dummy['MS_Zoning']  OneHotEncoder?Documentation for OneHotEncoderOneHotEncoder(drop='first', sparse_output=False) remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=(\"MS_Zoning.*\"))\n\n      dummy__MS_Zoning_C_all  ...  dummy__MS_Zoning_Residential_Medium_Density\n0                        0.0  ...                                          0.0\n1                        0.0  ...                                          0.0\n2                        0.0  ...                                          0.0\n3                        0.0  ...                                          0.0\n4                        0.0  ...                                          0.0\n...                      ...  ...                                          ...\n2925                     0.0  ...                                          0.0\n2926                     0.0  ...                                          0.0\n2927                     0.0  ...                                          0.0\n2928                     0.0  ...                                          0.0\n2929                     0.0  ...                                          0.0\n\n[2930 rows x 6 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-label.html",
    "href": "categorical-label.html",
    "title": "19  Label Encoding",
    "section": "",
    "text": "19.1 Label Encoding\nLabel encoding (also called integer encoding) is a method that maps the categorical levels into the integers 1 through n where n is the number of levels.\nThis method is a trained method since the preprocessor needs to keep a record of the possible values and their corresponding integer value. Unseen levels can be encoded outside the range to be either 0 or n + 1, allowing unseen levels to be handled with minimal extra work.\nThis method is often not ideal as the ordering of the levels will matter a lot for the performance of the model that needs to make sense of the generated numeric variables. For a variable with the levels “Studio”, “Apartment”, “Loft”, “Duplex”. This variable contains 4! = 4 * 3 * 2 * 1 = 24 different orderings. And since the number of permutations is calculated with factorials, this number goes up fast. With just 10 levels we are looking at 3,628,800 different orderings. Even if some orders are better than others, It would be a very slow task to iterate through to find which ones are good. If you have prior information about the levels, then you should use Ordinal Encoding.\nIf you are working on an implementation that works with factors, then they will be used. Otherwise, the ordering most likely will be alphabetical or in order of occurrence. You should check the documentation of your implementation to figure out which.\nThe performance of this method will depend a lot on the model. If you are working with a linear model, then you most likely are out of luck as it wouldn’t be able to use a variable where the values 2, 6, and 10 provide evidence one way, and the rest provide evidence the other way. Three-based models will be able to do better but would do even better if label encoding was applied to begin with.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Label Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-label.html#pros-and-cons",
    "href": "categorical-label.html#pros-and-cons",
    "title": "19  Label Encoding",
    "section": "19.2 Pros and Cons",
    "text": "19.2 Pros and Cons\n\n19.2.1 Pros\n\nOnly produces a single numeric variable for each categorical variable\nHas a way to handle unseen levels, although poorly\n\n\n\n19.2.2 Cons\n\nOrdering of the levels matters a lot!\nWill very often give inferior performance compared to other methods.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Label Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-label.html#r-examples",
    "href": "categorical-label.html#r-examples",
    "title": "19  Label Encoding",
    "section": "19.3 R Examples",
    "text": "19.3 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nLooking at the levels of MS_SubClass we see that levels are set in a specific way. It isn’t alphabetical, but there isn’t one clear order. No clarification of the ordering can be done in the data documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt.\n\names |&gt; pull(MS_SubClass) |&gt; levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_integer() step for this, which defaults to 1-based indexing\n\nlabel_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_integer(all_nominal_predictors()) |&gt;\n  prep()\n\nlabel_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\"))\n\n# A tibble: 2,930 × 2\n   MS_SubClass MS_Zoning\n         &lt;int&gt;     &lt;int&gt;\n 1           1         3\n 2           1         2\n 3           1         3\n 4           1         3\n 5           6         3\n 6           6         3\n 7          12         3\n 8          12         3\n 9          12         3\n10           6         3\n# ℹ 2,920 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Label Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-label.html#python-examples",
    "href": "categorical-label.html#python-examples",
    "title": "19  Label Encoding",
    "section": "19.4 Python Examples",
    "text": "19.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the OrdinalEncoder() method we can use. Below we see how it can be used with the MS_Zoning columns. We call this method label encoding only when categories='auto' as it automatically labels 0 to n_categories - 1.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\n\nct = ColumnTransformer(\n    [('label', OrdinalEncoder(categories='auto'), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('label', OrdinalEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('label', OrdinalEncoder(), ['MS_Zoning'])]) label['MS_Zoning']  OrdinalEncoder?Documentation for OrdinalEncoderOrdinalEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"label.*\")\n\n      label__MS_Zoning\n0                  5.0\n1                  4.0\n2                  5.0\n3                  5.0\n4                  5.0\n...                ...\n2925               5.0\n2926               5.0\n2927               5.0\n2928               5.0\n2929               5.0\n\n[2930 rows x 1 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Label Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-ordinal.html",
    "href": "categorical-ordinal.html",
    "title": "20  Ordinal Encoding",
    "section": "",
    "text": "20.1 Ordinal Encoding\nThis method is similar to Label Encoding, except that we manually specify the mapping. This method is generally used for ordinal variables as they are encoded with a natural ordering. Where this method shines compared to integer encoding is that we allow arbitrary values for encoding, thus we can have (cold = 1, warm = 5, hot = 20). But we might as well use (cold = -1, warm = 0, hot = 1) or (cold = 1.618, warm = 2.718, hot = 3.141). Although you would have a hard time justifying the latter. Nothing is stopping you from using this method with an unordered categorical variable, you just need to spend some time justifying your levels.\nThis method feels like it but isn’t a trained method. This is because we are providing the record of the possible values and their corresponding integer value. Unseen levels can be manually specified, but it isn’t entirely obvious what their value should be.\nManually setting values for your levels comes with some upsides and downsides. Assuming that you have the domain expertise to apply numeric values for the levels, removes a lot of the guesswork. This can be very effective if the numeric values selected for the levels have some intrinsic meaning. It can remove a lot of the guesswork and trial and error that we see in integer encoding. The downside is the other side of the coin. We need to have the domain expertise to be able to give the levels meaningful values, otherwise, we are doing no better than integer encoding.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinal Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-ordinal.html#pros-and-cons",
    "href": "categorical-ordinal.html#pros-and-cons",
    "title": "20  Ordinal Encoding",
    "section": "20.2 Pros and Cons",
    "text": "20.2 Pros and Cons\n\n20.2.1 Pros\n\nOnly produces a single numeric variable for each categorical variable\nPreserves the natural ordering of ordered values\n\n\n\n20.2.2 Cons\n\nWill very often give inferior performance compared to other methods\nUnseen levels need to be manually specified",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinal Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-ordinal.html#r-examples",
    "href": "categorical-ordinal.html#r-examples",
    "title": "20  Ordinal Encoding",
    "section": "20.3 R Examples",
    "text": "20.3 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Shape, Land_Slope)\n\n# A tibble: 2,930 × 2\n   Lot_Shape          Land_Slope\n   &lt;fct&gt;              &lt;fct&gt;     \n 1 Slightly_Irregular Gtl       \n 2 Regular            Gtl       \n 3 Slightly_Irregular Gtl       \n 4 Regular            Gtl       \n 5 Slightly_Irregular Gtl       \n 6 Slightly_Irregular Gtl       \n 7 Regular            Gtl       \n 8 Slightly_Irregular Gtl       \n 9 Slightly_Irregular Gtl       \n10 Regular            Gtl       \n# ℹ 2,920 more rows\n\n\nLooking at the levels of Lot_Shape and Land_Slope we see that they match the levels in the documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt. Furthermore, these variables are listed as ordinal, they just aren’t denoted like this in this data set.\n\names |&gt; pull(Lot_Shape) |&gt; levels()\n\n[1] \"Regular\"              \"Slightly_Irregular\"   \"Moderately_Irregular\"\n[4] \"Irregular\"           \n\names |&gt; pull(Land_Slope) |&gt; levels()\n\n[1] \"Gtl\" \"Mod\" \"Sev\"\n\n\nWe will fix that by turning them into ordered factors.\n\names &lt;- ames |&gt;\n  mutate(across(.cols = c(Lot_Shape, Land_Slope), .fns = as.ordered))\n\nto perform ordinal encoding we will use the step_ordinalscore() step. This defaults to giving each level values between 1 and n, much like step_integer().\n\nordinal_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_ordinalscore(Lot_Shape, Land_Slope) |&gt;\n  prep()\n\nordinal_rec |&gt;\n  bake(new_data = NULL, starts_with(\"Lot_Shape\"), starts_with(\"Land_Slope\"))\n\n# A tibble: 2,930 × 2\n   Lot_Shape Land_Slope\n       &lt;int&gt;      &lt;int&gt;\n 1         2          1\n 2         1          1\n 3         2          1\n 4         1          1\n 5         2          1\n 6         2          1\n 7         1          1\n 8         2          1\n 9         2          1\n10         1          1\n# ℹ 2,920 more rows\n\n\nWhat we can do is define a special transformation function for each of the steps. One way is to use the case_when() function\n\nLot_Shape_transformer &lt;- function(x) {\n  case_when(\n    x == \"Regular\" ~ 0, \n    x == \"Slightly_Irregular\" ~ -1,\n    x == \"Moderately_Irregular\" ~ -5,\n    x == \"Irregular\" ~ -10\n  )\n}\n\nIf you have the values for each of the levels as a vector or data, you can write the function to use that information as well.\n\nLand_Slope_values &lt;- c(Gtl = 0, Mod = 1, Sev = 5)\n\nLand_Slope_transformer &lt;- function(x) {\n  Land_Slope_values[x]\n}\n\nWith these functions, we can now apply them to the respective columns by using the convert argument.\n\nordinal_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_ordinalscore(Lot_Shape, convert = Lot_Shape_transformer) |&gt;\n  step_ordinalscore(Land_Slope, convert = Land_Slope_transformer) |&gt;\n  prep()\n\nordinal_rec |&gt;\n  bake(new_data = NULL, starts_with(\"Lot_Shape\"), starts_with(\"Land_Slope\")) |&gt;\n  distinct()\n\n# A tibble: 11 × 2\n   Lot_Shape Land_Slope\n       &lt;int&gt;      &lt;int&gt;\n 1        -1          0\n 2         0          0\n 3        -5          1\n 4        -1          1\n 5         0          1\n 6        -5          0\n 7         0          5\n 8        -1          5\n 9       -10          0\n10        -5          5\n11       -10          5",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinal Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-ordinal.html#python-examples",
    "href": "categorical-ordinal.html#python-examples",
    "title": "20  Ordinal Encoding",
    "section": "20.4 Python Examples",
    "text": "20.4 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Ordinal Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-binary.html",
    "href": "categorical-binary.html",
    "title": "21  Binary Encoding",
    "section": "",
    "text": "21.1 Binary Encoding\nBinary encoding encodes each category by encoding it as its binary representation. From the categorical variables, you assign an integer value to each level, in the same way as in Label Encoding. That value will then be converted to its binary representation, and that value will be returned.\nSuppose we have the following variable, and the values they take are (cat = 11, dog = 3, horse = 20). We are using a subset to gain a better understanding of what is happening.\nThe first thing we need to do is to calculate the binary representation of these numbers. And we should do it to 5 digits since it is the highest we need in this hypothetical example. 11 = 01011, 3 = 00011, 20 = 10100. We can then encode this in the following matrix\nEach we would be able to uniquely encode 2^5=32 different values with just 5 columns compared to the 32 it would take if you used dummy encoding from Chapter 18. In general, you will be able to encode n variables in ceiling(log2(n)) columns.\nThis method isn’t widely used. It does a good job of showing the midpoint between dummy encoding and label encoding in terms of how sparse we want to store our data. Its limitations come in terms of how interpretable the final model ends up being. Further, if you want to encode your data more compactly than dummy encoding, you will find better luck using some of the later described methods.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Binary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-binary.html#pros-and-cons",
    "href": "categorical-binary.html#pros-and-cons",
    "title": "21  Binary Encoding",
    "section": "21.2 Pros and Cons",
    "text": "21.2 Pros and Cons\n\n21.2.1 Pros\n\nuses fewer variables to store the same information as dummy encoding\n\n\n\n21.2.2 Cons\n\nLess interpretability compared to dummy variables",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Binary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-binary.html#r-examples",
    "href": "categorical-binary.html#r-examples",
    "title": "21  Binary Encoding",
    "section": "21.3 R Examples",
    "text": "21.3 R Examples\nWe will be using the ames data set for these examples. The step_encoding_binary() function from the extrasteps package allows us to perform binary encoding.\n\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nWe can then apply binary encoding using step_encoding_binary(). Notice how we only get 1 numeric variable for each categorical variable\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_encoding_binary(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 9\n$ MS_SubClass_1  &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1…\n$ MS_SubClass_2  &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0…\n$ MS_SubClass_4  &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0…\n$ MS_SubClass_8  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ MS_SubClass_16 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_1    &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ MS_Zoning_2    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ MS_Zoning_4    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_8    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nWe can pull the number of distinct levels of each variable by using tidy().\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 40 × 3\n   terms        value id                   \n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;                \n 1 MS_SubClass     16 encoding_binary_Bp5vK\n 2 MS_Zoning        7 encoding_binary_Bp5vK\n 3 Street           2 encoding_binary_Bp5vK\n 4 Alley            3 encoding_binary_Bp5vK\n 5 Lot_Shape        4 encoding_binary_Bp5vK\n 6 Land_Contour     4 encoding_binary_Bp5vK\n 7 Utilities        3 encoding_binary_Bp5vK\n 8 Lot_Config       5 encoding_binary_Bp5vK\n 9 Land_Slope       3 encoding_binary_Bp5vK\n10 Neighborhood    29 encoding_binary_Bp5vK\n# ℹ 30 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Binary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-binary.html#python-examples",
    "href": "categorical-binary.html#python-examples",
    "title": "21  Binary Encoding",
    "section": "21.4 Python Examples",
    "text": "21.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the BinaryEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.binary import BinaryEncoder\n\nct = ColumnTransformer(\n    [('binary', BinaryEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('binary', BinaryEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('binary', BinaryEncoder(), ['MS_Zoning'])]) binary['MS_Zoning'] BinaryEncoderBinaryEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"binary.*\")\n\n      binary__MS_Zoning_0  binary__MS_Zoning_1  binary__MS_Zoning_2\n0                       0                    0                    1\n1                       0                    1                    0\n2                       0                    0                    1\n3                       0                    0                    1\n4                       0                    0                    1\n...                   ...                  ...                  ...\n2925                    0                    0                    1\n2926                    0                    0                    1\n2927                    0                    0                    1\n2928                    0                    0                    1\n2929                    0                    0                    1\n\n[2930 rows x 3 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Binary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-frequency.html",
    "href": "categorical-frequency.html",
    "title": "22  Frequency Encoding",
    "section": "",
    "text": "22.1 Frequency Encoding\nFrequency encoding takes a categorical variable and replaces each level with its frequency in the training data set. This results in a single numeric variable, with values between 0 and 1. This is a trained method since we need to keep a record of the frequencies from the training data set.\nThis method isn’t a silver bullet, as it will only sometimes be useful. It is useful when the frequency/rarity of a category level is related to our outcome. Imagine we have data about wines and their producers, some big producers produce many wines, and small producers only produce a couple. This information could potentially be useful and would be easily captured in frequency encoding. This method is not able to distinguish between two levels that have the same frequency.\nUnseen levels can be automatically handled by giving them a value of 0 as they are unseen in the training data set. Thus no extra treatment is necessary. Sometimes taking the logarithm can be useful if you are having a big difference between the number of occurrences in your levels.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Frequency Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-frequency.html#pros-and-cons",
    "href": "categorical-frequency.html#pros-and-cons",
    "title": "22  Frequency Encoding",
    "section": "22.2 Pros and Cons",
    "text": "22.2 Pros and Cons\n\n22.2.1 Pros\n\nPowerful and simple when used correctly\nHigh interpretability\n\n\n\n22.2.2 Cons\n\nIs not able to distinguish between two levels that have the same frequency\nMay not add predictive power",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Frequency Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-frequency.html#r-examples",
    "href": "categorical-frequency.html#r-examples",
    "title": "22  Frequency Encoding",
    "section": "22.3 R Examples",
    "text": "22.3 R Examples\nWe will be using the ames data set for these examples. The step_encoding_frequency() function from the extrasteps package allows us to perform frequency encoding.\n\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ℹ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nWe can then apply frequency encoding using step_encoding_frequency(). Notice how we only get 1 numeric variable for each categorical variable\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_encoding_frequency(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\"))\n\n# A tibble: 2,930 × 2\n   MS_SubClass MS_Zoning\n         &lt;dbl&gt;     &lt;dbl&gt;\n 1      0.368    0.776  \n 2      0.368    0.00922\n 3      0.368    0.776  \n 4      0.368    0.776  \n 5      0.196    0.776  \n 6      0.196    0.776  \n 7      0.0655   0.776  \n 8      0.0655   0.776  \n 9      0.0655   0.776  \n10      0.196    0.776  \n# ℹ 2,920 more rows\n\n\nWe can pull the frequencies for each level of each variable by using tidy().\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 283 × 4\n   terms       level                                  frequency id              \n   &lt;chr&gt;       &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;           \n 1 MS_SubClass One_Story_1946_and_Newer_All_Styles      0.368   encoding_freque…\n 2 MS_SubClass One_Story_1945_and_Older                 0.0474  encoding_freque…\n 3 MS_SubClass One_Story_with_Finished_Attic_All_Ages   0.00205 encoding_freque…\n 4 MS_SubClass One_and_Half_Story_Unfinished_All_Ages   0.00614 encoding_freque…\n 5 MS_SubClass One_and_Half_Story_Finished_All_Ages     0.0980  encoding_freque…\n 6 MS_SubClass Two_Story_1946_and_Newer                 0.196   encoding_freque…\n 7 MS_SubClass Two_Story_1945_and_Older                 0.0437  encoding_freque…\n 8 MS_SubClass Two_and_Half_Story_All_Ages              0.00785 encoding_freque…\n 9 MS_SubClass Split_or_Multilevel                      0.0403  encoding_freque…\n10 MS_SubClass Split_Foyer                              0.0164  encoding_freque…\n# ℹ 273 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Frequency Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-frequency.html#python-examples",
    "href": "categorical-frequency.html#python-examples",
    "title": "22  Frequency Encoding",
    "section": "22.4 Python Examples",
    "text": "22.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the CountEncoder() method we can use. This performs count encoding, which we know is functionally equivalent to frequency encoding.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.count import CountEncoder\n\nct = ColumnTransformer(\n    [('count', CountEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('count',\n                                 CountEncoder(combine_min_nan_groups=True),\n                                 ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('count',\n                                 CountEncoder(combine_min_nan_groups=True),\n                                 ['MS_Zoning'])]) count['MS_Zoning'] CountEncoderCountEncoder(combine_min_nan_groups=True) remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"count.*\")\n\n      count__MS_Zoning\n0                 2273\n1                   27\n2                 2273\n3                 2273\n4                 2273\n...                ...\n2925              2273\n2926              2273\n2927              2273\n2928              2273\n2929              2273\n\n[2930 rows x 1 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Frequency Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-target.html",
    "href": "categorical-target.html",
    "title": "23  Target Encoding",
    "section": "",
    "text": "23.1 Target Encoding\nTarget encoding (also called mean encoding, likelihood encoding, or impact encoding) is a method that maps the categorical levels to probabilities of your target variable (Micci-Barreca 2001). This method is in some ways quite similar to frequency encoding. We are taking a single categorical variable, and turning it into a single numeric categorical variable.\nThis is a trained and supervised method since we are using the outcome of our modeling problem to guide the way this method is estimated. In the most simple formulation, target encoding is done by replacing each level of a categorical variable with the mean of the target variable within said level. The target variable will typically be the outcome, but that is not necessarily a requirement.\nConsider the following example data set\nIf we were to calculate target encoding on animal using cuteness as the target, we would first need to calculate the mean of cuteness within each\nTaking these means we can now use them as an encoding\nFrom the above example, we notice 3 things. Firstly, once the calculations have been done, applying the encoding to new data is a fairly easy procedure as it amounts to a left join.\nLastly, how will this method handle unseen levels?\nLet us think about the unseen levels first. If we have no information about a given class. This could happen in at least two different ways. Because the level is truly unseen because the company was just started and wasn’t known in the training data set. Or because the known level wasn’t observed, e.i. no Sundays in the training data set. Regardless of the reason, we will want to give these levels a baseline number. For this, we can use the mean value of the target, across all of the training data set. So for our toy example, we have a mean cuteness of 4, which we will assign to any new animal.\nThis value is by no means a good value, but it is an educated guess that can be calculated with ease. This also means that regardless of the distribution of the target, these values can be calculated.\nWe have so far talked about target encoding, from the perspective of a regression task. But target encoding is not limited to numeric outcomes, but can be used in classification settings as well. In the classification setting, where we have a categorical outcome, instead of calculating the mean of the target variable, we need to figure something else out. It could be calculating the probability of the first level, or we could try to go more linear by converting them to log odds. Now everything works as before.\nA type of data that is sometimes seen is hierarchical categorical variables. Typical examples are city-state and region-subregion. With hierarchical categorical variables you often end up with many levels, and target encoding can be used on such data, and can even use the hierarchical structure.\nThe encoding is calculated like normal, but the smoothing is done on a lower level than at the top. So instead of adjusting by the global mean, you smooth by the level below it.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Target Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-target.html#sec-target-low-count-groups",
    "href": "categorical-target.html#sec-target-low-count-groups",
    "title": "23  Target Encoding",
    "section": "23.2 Low count groups",
    "text": "23.2 Low count groups\nTarget encoding as described in this chapter, doesn’t apply smoothing to the statistics that are calculated. Depending on the data it won’t have too bad of a negative effect. This subsection goes over what happens if one or more of the groups have few counts. It will at the same time act as the main motivation, for why you shouldn’t be using the base form of target encoding as described in this chapter, and instead use one with smoothing as seen in Chapter 27.\nSuppose we have the following data.\n\n\n\n\n\ntarget\npredictor\n\n\n\n\nA\n0.4\n\n\nA\n0.6\n\n\nA\n0.5\n\n\nA\n0.2\n\n\nA\n0.5\n\n\nB\n0.2\n\n\nB\n0.3\n\n\nB\n0.2\n\n\nB\n0.4\n\n\nB\n0.6\n\n\nC\n0.1\n\n\n\n\n\nCalculating the means gives us the following encoding.\n\n\n\n\n\ntarget\npredictor\n\n\n\n\nA\n0.44\n\n\nB\n0.34\n\n\nC\n0.10\n\n\n\n\n\nHowever, how much can we trust this encoding? C only has a single value and A and C both have 5. Maybe 0.1 is a good estimate of the target means under C, but it could also be an outlier of the distribution itself. Taking a single value of A would give a value between 0.2 and 0.6.\nWe are overfitting to the training data set, by having a single observation for C alone determine the encoding. Smoothing would take the global mean (here 0.36) into account by having the mean encoding deviate from the global mean. Target levels with many counts would use a mean close to the class mean, and target levels with few counts would use a mean close to the global mean.\nWe see this in sports and online reviews. An athlete that scored 1 out of 1 goal isn’t a reliable statistic, as the athlete that scores 80 out of 100. Like-wise, 10 five-star reviews aren’t a good as 100 five-star reviews.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Target Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-target.html#pros-and-cons",
    "href": "categorical-target.html#pros-and-cons",
    "title": "23  Target Encoding",
    "section": "23.3 Pros and Cons",
    "text": "23.3 Pros and Cons\n\n23.3.1 Pros\n\nCan deal with categorical variables with many levels\nCan deal with unseen levels in a sensible way\n\n\n\n23.3.2 Cons\n\nCan be prone to overfitting",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Target Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-target.html#r-examples",
    "href": "categorical-target.html#r-examples",
    "title": "23  Target Encoding",
    "section": "23.4 R Examples",
    "text": "23.4 R Examples\nThe embed package comes with a couple of functions to do target encoding, we will look at step_lencode_glm() as it applied the method best described by this chapter. These functions are named such because they likelihood encode variables, and because the encodings can be calculated using no intercept models.\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\n\nlibrary(recipes)\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrec_target &lt;- recipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) |&gt;\n  prep()\n\nrec_target |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Neighborhood Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1      145097.     215000\n 2      145097.     105000\n 3      145097.     172000\n 4      145097.     244000\n 5      190647.     189900\n 6      190647.     195500\n 7      324229.     213500\n 8      324229.     191500\n 9      324229.     236500\n10      190647.     189000\n# ℹ 2,920 more rows\n\n\nAnd we see that it works as intended, we can pull out the exact levels using the tidy() method\n\nrec_target |&gt;\n  tidy(1)\n\n# A tibble: 29 × 4\n   level                value terms        id               \n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            \n 1 North_Ames         145097. Neighborhood lencode_glm_Bp5vK\n 2 College_Creek      201803. Neighborhood lencode_glm_Bp5vK\n 3 Old_Town           123992. Neighborhood lencode_glm_Bp5vK\n 4 Edwards            130843. Neighborhood lencode_glm_Bp5vK\n 5 Somerset           229707. Neighborhood lencode_glm_Bp5vK\n 6 Northridge_Heights 322018. Neighborhood lencode_glm_Bp5vK\n 7 Gilbert            190647. Neighborhood lencode_glm_Bp5vK\n 8 Sawyer             136751. Neighborhood lencode_glm_Bp5vK\n 9 Northwest_Ames     188407. Neighborhood lencode_glm_Bp5vK\n10 Sawyer_West        184070. Neighborhood lencode_glm_Bp5vK\n# ℹ 19 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Target Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-target.html#python-examples",
    "href": "categorical-target.html#python-examples",
    "title": "23  Target Encoding",
    "section": "23.5 Python Examples",
    "text": "23.5 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the TargetEncoder() method we can use. For this to work, we need to remember to specify an outcome when we fit().\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import TargetEncoder\n\nct = ColumnTransformer(\n    [('target', TargetEncoder(target_type=\"continuous\"), ['Neighborhood'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('target',\n                                 TargetEncoder(target_type='continuous'),\n                                 ['Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('target',\n                                 TargetEncoder(target_type='continuous'),\n                                 ['Neighborhood'])]) target['Neighborhood']  TargetEncoder?Documentation for TargetEncoderTargetEncoder(target_type='continuous') remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"target.*\")\n\n      target__Neighborhood\n0               145110.156\n1               145110.156\n2               145110.156\n3               145110.156\n4               190636.427\n...                    ...\n2925            162269.818\n2926            162269.818\n2927            162269.818\n2928            162269.818\n2929            162269.818\n\n[2930 rows x 1 columns]\n\n\n\n\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.” SIGKDD Explor. Newsl. 3 (1): 27–32. https://doi.org/10.1145/507533.507538.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Target Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-hashing.html",
    "href": "categorical-hashing.html",
    "title": "24  Hashing Encoding",
    "section": "",
    "text": "24.1 Hashing Encoding\nHashing encoding, also known as the hashing trick and feature hashing, is a method where we take in the categorical values, run those values through a fast hashing function and let the resulting value determine how the encoding is done.\nThere are a couple of things to unpack here. Hashing functions are one of them. But the first thing we will deal with is the motivation. Some categorical variables contain many unique levels, enough to make Dummy Encoding unfeasible. This happens for a couple of reasons. Firstly, we are creating many columns and thus high sparsity. Secondly, high cardinality often comes with many new unseen levels. Feature hashing can be used to deal with both.\nOne mental model is that we are constructing a dummy encoding, and then combining the columns in a deterministically random way. For this, we are using a hashing function. A hashing function is a function that takes an input, and returns a integer. The same value will always return the same output. It is computationally easy to perform this transformation, but it is hard to reverse it. This is not a downside as we don’t need to perform the reverse transformation. Furthermore a good hashing function outputs values evenly across their supported range, and similar values such as cat and cats will produce vastly different hashes, 1751422759 and 2517493566 respectively for the MurmurHash3 hashing function.\nThe MurmurHash3, which is commonly used for its speed, produces 32-bit hash values, which gives us integers between 1 and 2^32 = 4294967296. Producing 4294967296 columns would not help us, so what is typically done is to round these values down to a more manageable range. Specifically rounding by a power of 2 is common since that can be archived by bit manipulation. Suppose we round such that we only keep the 6 significant digits, then we are left with 2^6 = 64 values. And the hashes for cat is now 39 and cats is 62. They are still different, but now they take up a smaller space of possible values.\nOne thing that will happen when you use these hashing functions is that different levels hash to the same value. This is called a collision. And are technically a bad thing, as the model will be unable to distinguish between the influence of those two levels. However, it is not something to avoid at all costs. One of the main tenets of hashing encoding is that we are getting a trade-off between storage size and information. (TODO find better thing to say here).\nWith this in mind, the optional number of features produced by hashing encoding cannot be inferred directly, and we will need to try different values. Too few columns and we have too many collisions and the performance drops, too many columns and we run into a memory and speed issue.\nOne thing that is used to combat collisions is the use of a hashed sign function. Much like we are using a hashing function to generate integers, we will use a different hashing function to give us one of two values -1 and 1. This will determine the sign of each hashed word. This is done to lessen the negative effects of collisions as there is a 50% chance that a pair of strings that hash to the same values will have different signs and thus cancel each other out.\nThe main downside to this method is the lack of explainability, as the collisions make it so we can’t know for sure which level contributed to the effect of that variable. On the other hand, we get the added benefit of being able to handle unseen labels directly. These will not be of use directly, but they are handled in the sense that we don’t have to keep track of levels, as the hashing function just does its job.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hashing Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-hashing.html#pros-and-cons",
    "href": "categorical-hashing.html#pros-and-cons",
    "title": "24  Hashing Encoding",
    "section": "24.2 Pros and Cons",
    "text": "24.2 Pros and Cons\n\n24.2.1 Pros\n\nComputationally fast\nAllows for a fixed number of output columns\ngives less sparse output than dummy encoding\n\n\n\n24.2.2 Cons\n\nLoss of interpretability\nStill gives quite sparse output",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hashing Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-hashing.html#r-examples",
    "href": "categorical-hashing.html#r-examples",
    "title": "24  Hashing Encoding",
    "section": "24.3 R Examples",
    "text": "24.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a higher cardinality data set for this\n\n\nWe will be using the ames data set for these examples. The step_dummy_hash() function from the textrecipes package allows us to perform hashing encoding.\n\nlibrary(recipes)\nlibrary(textrecipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Exterior_1st)\n\n# A tibble: 2,930 × 2\n   Sale_Price Exterior_1st\n        &lt;int&gt; &lt;fct&gt;       \n 1     215000 BrkFace     \n 2     105000 VinylSd     \n 3     172000 Wd Sdng     \n 4     244000 BrkFace     \n 5     189900 VinylSd     \n 6     195500 VinylSd     \n 7     213500 CemntBd     \n 8     191500 HdBoard     \n 9     236500 CemntBd     \n10     189000 VinylSd     \n# ℹ 2,920 more rows\n\n\nWe will be using the step_dummy_hash() step for this. For illustrative purposes, we will be creating 8 columns, where in practice you would likely want this value higher.\n\ndummy_rec &lt;- recipe(Sale_Price ~ Exterior_1st, data = ames) |&gt;\n  step_dummy_hash(Exterior_1st, num_terms = 8) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 9\n$ Sale_Price               &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 19550…\n$ dummyhash_Exterior_1st_1 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1,…\n$ dummyhash_Exterior_1st_2 &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_3 &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_4 &lt;int&gt; -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ dummyhash_Exterior_1st_5 &lt;int&gt; 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0…\n$ dummyhash_Exterior_1st_6 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_7 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dummyhash_Exterior_1st_8 &lt;int&gt; 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0…",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hashing Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-hashing.html#python-examples",
    "href": "categorical-hashing.html#python-examples",
    "title": "24  Hashing Encoding",
    "section": "24.4 Python Examples",
    "text": "24.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the HashingEncoder() method we can use. For illustrative purposes, we will be creating 8 columns, where in practice you would likely want this value higher.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.hashing import HashingEncoder\n\nct = ColumnTransformer(\n    [('hasher', HashingEncoder(n_components=8), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('hasher', HashingEncoder(max_process=7),\n                                 ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('hasher', HashingEncoder(max_process=7),\n                                 ['MS_Zoning'])]) hasher['MS_Zoning'] HashingEncoderHashingEncoder(max_process=7) remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"hasher.*\")\n\n      hasher__col_0  hasher__col_1  ...  hasher__col_6  hasher__col_7\n0                 0              0  ...              0              0\n1                 0              1  ...              0              0\n2                 0              0  ...              0              0\n3                 0              0  ...              0              0\n4                 0              0  ...              0              0\n...             ...            ...  ...            ...            ...\n2925              0              0  ...              0              0\n2926              0              0  ...              0              0\n2927              0              0  ...              0              0\n2928              0              0  ...              0              0\n2929              0              0  ...              0              0\n\n[2930 rows x 8 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Hashing Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaveoneout.html",
    "href": "categorical-leaveoneout.html",
    "title": "25  Leave One Out Encoding",
    "section": "",
    "text": "25.1 Leave One Out Encoding\nLeave One Out Encoding, is a variation on target encoding. Where target encoding takes the mean of all rows within each target level, it instead excludes the value of the current row.\nOne of the main downsides to this approach is that since it needs the target which is most often the outcome and such not available for the test data set, it will thus not be able to do the row-wise adjustment and will behave exactly as the target encoding for the test data set.\nWhat this does in practice is that it shifts the influence of outliers within each level away from the whole group and onto the outlier itself. Consider a level that has the following target values 100, 10, 6, 5, 3, 8. The target encoded value would be 22 and the leave one out values would be different, but the most different one is the outlier at 100.\nThus we have that target encoding is influenced differently than leave one out encoding is. Which type of influence is better is up to you, the practitioner to determine based on your data and modeling problem.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Leave One Out Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaveoneout.html#pros-and-cons",
    "href": "categorical-leaveoneout.html#pros-and-cons",
    "title": "25  Leave One Out Encoding",
    "section": "25.2 Pros and Cons",
    "text": "25.2 Pros and Cons\n\n25.2.1 Pros\n\nDoesn’t hide the effort of outliers as compared to target encoding.\nCan deal with categorical variables with many levels\nCan deal with unseen levels in a sensible way\n\n\n\n25.2.2 Cons\n\nOnly have a meaningful difference compared to target encoding to training data set.\nCan be prone to overfitting",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Leave One Out Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaveoneout.html#r-examples",
    "href": "categorical-leaveoneout.html#r-examples",
    "title": "25  Leave One Out Encoding",
    "section": "25.3 R Examples",
    "text": "25.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Leave One Out Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaveoneout.html#python-examples",
    "href": "categorical-leaveoneout.html#python-examples",
    "title": "25  Leave One Out Encoding",
    "section": "25.4 Python Examples",
    "text": "25.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the LeaveOneOutEncoder() method we can use. For this to work, we need to remember to specify an outcome when we fit().\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\n\nfrom sklearn.preprocessing import TargetEncoder\n\nct = ColumnTransformer(\n    [('loo', LeaveOneOutEncoder(), ['Neighborhood'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('loo', LeaveOneOutEncoder(),\n                                 ['Neighborhood'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('loo', LeaveOneOutEncoder(),\n                                 ['Neighborhood'])]) loo['Neighborhood'] LeaveOneOutEncoderLeaveOneOutEncoder() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"loo.*\")\n\n      loo__Neighborhood\n0            145097.350\n1            145097.350\n2            145097.350\n3            145097.350\n4            190646.576\n...                 ...\n2925         162226.632\n2926         162226.632\n2927         162226.632\n2928         162226.632\n2929         162226.632\n\n[2930 rows x 1 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Leave One Out Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaf.html",
    "href": "categorical-leaf.html",
    "title": "26  Leaf Encoding",
    "section": "",
    "text": "26.1 Leaf Encoding\nLeaf encoding, also called decision tree encoding, is a method where a single decision tree fits using a target, typically the outcome, and a single categorical variable as the predictor. The encoding is then done by using the predictions of the tree to replace the categorical labels.\nThis should work in both classification and regression settings, but they serve different purposes. If used in a classification setting, we are replacing a categorial predictor with another categorical predictor with fewer levels. For regression settings, we have that the categorical predictor is replaced with a numeric variable. In some ways, this feels much like target encoding explored in Chapter 23.\nSuppose we use leaf encoding on the MS_SubClass predictor of the ames data set, using the numeric target Sale_Price. A possible fitted tree on that data would yield the following encoding table.\nThis table has 4 different values, meaning that the tree has 4 different leafs. Now prediction happens by using this lookup table.\nInstead, let’s see what happens if we choose a categorical target. Using the same MS_SubClass predictor, but instead using the categorical variable Lot_Shape as the target.\nAnd we now have a mapping that takes 16 levels and compresses them into n_distinct(res$leaf) levels. We note two insights for the categorical target case. Firstly, the number of unique levels can’t exceed the number of levels in the target. Because it is not possible to predict a level that doesn’t exist for the target. Secondly, you will produce the same or fewer levels in your leaf. We saw earlier that it is possible to produce fewer. To produce the same about of levels, we would need a target with the same or more levels than the predictor and have each predictor level map to a different target level.\nSince we are fitting a tree, it has the opportunity to be hyper-parameter-tuned, as the size and shape tree will affect the encoding. You will be fitting a different tree for each of the categorical variables you are encoding, and they likely won’t have the same optimal tree size. Here you have to make a choice. Either meticulously tune each tree in the broader scope of the model, or use decent defaults. The latter choice is likely the best one.\nLastly, this method doesn’t work with unseen levels as talked about in Chapter 17, as decision trees generally don’t have a way to handle unseen levels.\nhttps://feature-engine.trainindata.com/en/1.7.x/user_guide/encoding/index.html#decision-tree-encoding",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Leaf Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaf.html#pros-and-cons",
    "href": "categorical-leaf.html#pros-and-cons",
    "title": "26  Leaf Encoding",
    "section": "26.2 Pros and Cons",
    "text": "26.2 Pros and Cons\n\n26.2.1 Pros\n\nProduces a single column.\n\n\n\n26.2.2 Cons\n\nDoesn’t handle unseen levels.\nCan be unstable, due to using a decision tree.\nIt may be overly simplistic.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Leaf Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaf.html#r-examples",
    "href": "categorical-leaf.html#r-examples",
    "title": "26  Leaf Encoding",
    "section": "26.3 R Examples",
    "text": "26.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Leaf Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-leaf.html#python-examples",
    "href": "categorical-leaf.html#python-examples",
    "title": "26  Leaf Encoding",
    "section": "26.4 Python Examples",
    "text": "26.4 Python Examples\nWe are using the ames data set for examples. {feature_engine} provided the YeoJohnsonTransformer() that we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom feature_engine.encoding import DecisionTreeEncoder\n\nct = ColumnTransformer(\n    [('treeEncoding', DecisionTreeEncoder(), ['MS_SubClass'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('treeEncoding', DecisionTreeEncoder(),\n                                 ['MS_SubClass'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('treeEncoding', DecisionTreeEncoder(),\n                                 ['MS_SubClass'])]) treeEncoding['MS_SubClass'] DecisionTreeEncoderDecisionTreeEncoder() remainder['MS_Zoning', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"treeEncoding.*\")\n\n      treeEncoding__MS_SubClass\n0                    187355.694\n1                    187355.694\n2                    187355.694\n3                    187355.694\n4                    239364.285\n...                         ...\n2925                 168009.364\n2926                 187355.694\n2927                 138618.386\n2928                 187355.694\n2929                 239364.285\n\n[2930 rows x 1 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Leaf Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-glmm.html",
    "href": "categorical-glmm.html",
    "title": "27  GLMM Encoding",
    "section": "",
    "text": "27.1 GLMM Encoding\nGeneralized linear mixed models (GLMM) encoding Pargent et al. (2022) follows as an extension to target encoding which is laid out in detail in Chapter 23.\nA hierarchical generalized linear model is fit, using no intercept.\nWhen applying target encoding, some classes have different numbers of observations associated with them.\nThe \"horse\" class only has 1 observation in this data set, how confident are we that the mean calculated from this value is as valid as the mean that was calculated over the 3 values for the \"cat\" class?\nKnowing that the global mean of the target is our baseline when we have no information. We can combine the level mean with the global mean, in accordance with how many observations we observe. If we have a lot of observations at a level, we will let the global mean have little influence, and if there are fewer observations we will let the global mean have a higher influence.\nWe can visualize this effect in the following charts. First, we have an example of what happens with a smaller amount of smoothing. The points are mostly along the diagonal. Remember that if we didn’t do this, all the points would be along the diagonal regardless of their size.\nIn this next chart, we see the effect of a higher amount of smoothing, now the levels with fewer observations are pulled quite a bit closer to the global mean.\nThe exact way this is done will vary from method to method, and the strength of this smoothing can and should properly be tuned as there isn’t an empirical best way to choose it.\nThe big benefit is that by fitting a hierarchical generalized linear model is fit, using no intercept is that it will handle the amount of smoothing for us. Giving us a method that handles smoothing, using a sound statistical method, without needed a hyperparameter.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>GLMM Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-glmm.html#pros-and-cons",
    "href": "categorical-glmm.html#pros-and-cons",
    "title": "27  GLMM Encoding",
    "section": "27.2 Pros and Cons",
    "text": "27.2 Pros and Cons\n\n27.2.1 Pros\n\nNo hyperparameters to tune, as shrinkage is automatically done.\nCan deal with categorical variables with many levels\nCan deal with unseen levels in a sensible way\n\n\n\n27.2.2 Cons\n\nCan be prone to overfitting",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>GLMM Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-glmm.html#r-examples",
    "href": "categorical-glmm.html#r-examples",
    "title": "27  GLMM Encoding",
    "section": "27.3 R Examples",
    "text": "27.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nWe apply the smoothed GLMM encoder using the step_lencode_mixed() step.\n\nlibrary(recipes)\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrec_target_smooth &lt;- recipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) |&gt;\n  prep()\n\nrec_target_smooth |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Neighborhood Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1      145156.     215000\n 2      145156.     105000\n 3      145156.     172000\n 4      145156.     244000\n 5      190633.     189900\n 6      190633.     195500\n 7      322591.     213500\n 8      322591.     191500\n 9      322591.     236500\n10      190633.     189000\n# ℹ 2,920 more rows\n\n\nAnd we can pull out the values of the encoding like so.\n\nrec_target_smooth |&gt;\n  tidy(1)\n\n# A tibble: 29 × 4\n   level                value terms        id                 \n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;              \n 1 North_Ames         145156. Neighborhood lencode_mixed_Bp5vK\n 2 College_Creek      201769. Neighborhood lencode_mixed_Bp5vK\n 3 Old_Town           124154. Neighborhood lencode_mixed_Bp5vK\n 4 Edwards            131021. Neighborhood lencode_mixed_Bp5vK\n 5 Somerset           229563. Neighborhood lencode_mixed_Bp5vK\n 6 Northridge_Heights 321519. Neighborhood lencode_mixed_Bp5vK\n 7 Gilbert            190633. Neighborhood lencode_mixed_Bp5vK\n 8 Sawyer             136956. Neighborhood lencode_mixed_Bp5vK\n 9 Northwest_Ames     188401. Neighborhood lencode_mixed_Bp5vK\n10 Sawyer_West        184085. Neighborhood lencode_mixed_Bp5vK\n# ℹ 19 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>GLMM Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-glmm.html#python-examples",
    "href": "categorical-glmm.html#python-examples",
    "title": "27  GLMM Encoding",
    "section": "27.4 Python Examples",
    "text": "27.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the GLMMEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.glmm import GLMMEncoder\n\nct = ColumnTransformer(\n    [('glmm', GLMMEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('glmm', GLMMEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('glmm', GLMMEncoder(), ['MS_Zoning'])]) glmm['MS_Zoning'] GLMMEncoderGLMMEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      glmm__MS_Zoning  ... remainder__Latitude\n0           55180.830  ...              42.054\n1             338.691  ...              42.053\n2           55180.830  ...              42.053\n3           55180.830  ...              42.051\n4           55180.830  ...              42.061\n...               ...  ...                 ...\n2925        55180.830  ...              41.989\n2926        55180.830  ...              41.988\n2927        55180.830  ...              41.987\n2928        55180.830  ...              41.991\n2929        55180.830  ...              41.989\n\n[2930 rows x 74 columns]\n\n\n\n\n\n\nPargent, Florian, Florian Pfisterer, Janek Thomas, and Bernd Bischl. 2022. “Regularized Target Encoding Outperforms Traditional Methods in Supervised Machine Learning with High Cardinality Features.” Computational Statistics 37 (5): 2671–92. https://doi.org/10.1007/s00180-022-01207-6.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>GLMM Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-catboost.html",
    "href": "categorical-catboost.html",
    "title": "28  Catboost Encoding",
    "section": "",
    "text": "28.1 Catboost Encoding\nAlso known as ordered target encoding, is an extension of target encoding as seen in Chapter 23. First proposed as a part of CatBoost(Prokhorenkova et al. 2019).\nIn regular target encoding, we can calculate the encoding at once for each level on the predictor we are working with. Ordered target encoding, as the name suggests, imposes an ordering to the observations, Then the target statistics are calculated only for previous observations, in the hopes that this will reduce target leakage.\nThe general formula used to calculate the encoding is as follows:\n\\[\n\\dfrac{currentCount + prior}{totalCount + 1}\n\\]\nWhere \\(currentCount\\) is the number of times the target class has occured for this predictor level. \\(totalCount\\) is the number of times the predictor level has occured. \\(prior\\) is some constant value, typically defaulted to \\(0.05\\).\nNotice the above formulation assumes a classification setting, regression is usually done by running quantization on the numeric target inside the application.\nBelow we have a worked example. We are using color as the categorical variable we are going to encode, using target as the target, and using \"yes\" as the target class we are looking for. The first row is the trivial case since it is the first occurrence of \"red\". It will thus have the value of prior since currentCount and totalCount are both equal to 0. The next row is another \"red\", so we just count how many previous values of \"red\" we have, which is 1, and set totalCount as that value. Then we count how many times target is equal to \"yes\" in those instances, which is also 1, and we set currentCount to 1 as well. This gives us (1 + 0.05) / (1 + 1) = 525. The third row is another trivial case. The fourth row has totalCount = 1 and currentCount = 0 since the previous value of \"green\" didn’t have a target of \"yes\". Follow rows are calculated using the same principles.\nOne of the stated downsides of using this method outside of catboost itselt is that since the encoding happens on order, you end up with an encoding where the amount of information isn’t uniformally spread over the observation. Instead, you have that the first observations have low information and the last observations have high information. This is of cause after shuffling if that has taken place. This is not a problem inside catboost as the shuffling of the order before applying the encoding is part of what makes stochastic gradient descent work for catboost. It is worth keeping this in mind when you are doing performance checks of your fitted models.\nThe way we apply this method to new observations, such as in the testing set, is you pretend that each row would have been appended to the training set, apply the encoding, and then remove it. Rinse and repeat for the remaining observation. That is to say that we use the currentCount and totalCount but we do not update them.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Catboost Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-catboost.html#pros-and-cons",
    "href": "categorical-catboost.html#pros-and-cons",
    "title": "28  Catboost Encoding",
    "section": "28.2 Pros and Cons",
    "text": "28.2 Pros and Cons\n\n28.2.1 Pros\n\nCan deal with categorical variables with many levels\nCan deal with unseen levels in a sensible way\n\n\n\n28.2.2 Cons\n\nUneven effect for different observations",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Catboost Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-catboost.html#r-examples",
    "href": "categorical-catboost.html#r-examples",
    "title": "28  Catboost Encoding",
    "section": "28.3 R Examples",
    "text": "28.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Catboost Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-catboost.html#python-examples",
    "href": "categorical-catboost.html#python-examples",
    "title": "28  Catboost Encoding",
    "section": "28.4 Python Examples",
    "text": "28.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the CatBoostEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.cat_boost import CatBoostEncoder\n\nct = ColumnTransformer(\n    [('catboost', CatBoostEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('catboost', CatBoostEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('catboost', CatBoostEncoder(), ['MS_Zoning'])]) catboost['MS_Zoning'] CatBoostEncoderCatBoostEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"catboost.*\")\n\n      catboost__MS_Zoning\n0              191278.640\n1              138004.645\n2              191278.640\n3              191278.640\n4              191278.640\n...                   ...\n2925           191278.640\n2926           191278.640\n2927           191278.640\n2928           191278.640\n2929           191278.640\n\n[2930 rows x 1 columns]\n\n\n\n\n\n\nProkhorenkova, Liudmila, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. 2019. “CatBoost: Unbiased Boosting with Categorical Features.” https://arxiv.org/abs/1706.09516.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Catboost Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-woe.html",
    "href": "categorical-woe.html",
    "title": "29  Weight of Evidence Encoding",
    "section": "",
    "text": "29.1 Weight of Evidence Encoding\nThe Weight of Evidence (WOE) encoding method is a method that specifically works with a binary target variable and a categorical predictor. It has a background in the financial sector. Its main drawback is its reliance on a binary outcome, which is often common in that sector.\nIt works by taking calculating the logarithm of the odds ratio, as a quantification of the relationship between the categorical predictor and the binary target. The method assigns the target levels as the good outcome and one as the bad outcome. Or target and no target. This choice doesn’t matter beyond notation. Swapping these results in a sign change of the resulting numeric predictor. It uses the following formula:\n\\[\nWOE_c = log\\left( \\frac{P(X = c | Y = 1)}{P(X = c | Y = 0)} \\right)\n\\]\nWhere c represents a given level of the categorical predictor. We read it as the probability of a specific observation having a given level when the target has one level over the other level.\nWe can run into bad values with this formula if there aren’t enough counts. If \\(P(X = c | Y = 1) = 0\\) we get undefined behavior when we take the logarithm, and if \\(P(X = c | Y = 0) = 0\\) we get a division by 0 problem. Both of these issues are typically handled with the use of a Laplace value. This value, typically quite small is added to the numerator and denominator to avoid these issues.\nThe resulting single numeric predictor takes non-infinite values. 0 means that according to the training data set, the category doesn’t have any information one way or another. Positive values mean a stronger relationship between the predictor and the “good” outcome, and negative values mean a stronger relationship to the “bad” outcome. Missing values and unseen levels typically default to have WOE = 0 as we don’t have information about them. One could get information out of missing values, by treating it as another level.\nThe weight of evidence encoding has been reported to be effective for addressing imbalanced data sets, by capturing the minority class effectively.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Weight of Evidence Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-woe.html#pros-and-cons",
    "href": "categorical-woe.html#pros-and-cons",
    "title": "29  Weight of Evidence Encoding",
    "section": "29.2 Pros and Cons",
    "text": "29.2 Pros and Cons\n\n29.2.1 Pros\n\n\n29.2.2 Cons",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Weight of Evidence Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-woe.html#r-examples",
    "href": "categorical-woe.html#r-examples",
    "title": "29  Weight of Evidence Encoding",
    "section": "29.3 R Examples",
    "text": "29.3 R Examples\n\nlibrary(recipes)\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrec_target &lt;- recipe(Street ~ Neighborhood, data = ames) |&gt;\n  step_woe(Neighborhood, outcome = vars(Street)) |&gt;\n  prep()\n\nrec_target |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Street woe_Neighborhood\n   &lt;fct&gt;             &lt;dbl&gt;\n 1 Pave            -14.4  \n 2 Pave            -14.4  \n 3 Pave            -14.4  \n 4 Pave            -14.4  \n 5 Pave              0.394\n 6 Pave              0.394\n 7 Pave            -12.3  \n 8 Pave            -12.3  \n 9 Pave            -12.3  \n10 Pave              0.394\n# ℹ 2,920 more rows\n\n\nAnd we see that it works as intended, we can pull out the exact levels using the tidy() method\n\nrec_target |&gt;\n  tidy(1)\n\n# A tibble: 28 × 10\n   terms        value   n_tot n_Grvl n_Pave p_Grvl  p_Pave     woe outcome id   \n   &lt;chr&gt;        &lt;chr&gt;   &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;\n 1 Neighborhood Bloomi…    28      0     28 0      9.60e-3 -11.7   Street  woe_…\n 2 Neighborhood Blueste    10      0     10 0      3.43e-3 -10.6   Street  woe_…\n 3 Neighborhood Briard…    30      0     30 0      1.03e-2 -11.7   Street  woe_…\n 4 Neighborhood Brooks…   108      0    108 0      3.70e-2 -13.0   Street  woe_…\n 5 Neighborhood Clear_…    44      0     44 0      1.51e-2 -12.1   Street  woe_…\n 6 Neighborhood Colleg…   267      0    267 0      9.15e-2 -13.9   Street  woe_…\n 7 Neighborhood Crawfo…   103      0    103 0      3.53e-2 -13.0   Street  woe_…\n 8 Neighborhood Edwards   194      1    193 0.0833 6.61e-2   0.231 Street  woe_…\n 9 Neighborhood Gilbert   165      1    164 0.0833 5.62e-2   0.394 Street  woe_…\n10 Neighborhood Green_…     2      0      2 0      6.85e-4  -9.01  Street  woe_…\n# ℹ 18 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Weight of Evidence Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-woe.html#python-examples",
    "href": "categorical-woe.html#python-examples",
    "title": "29  Weight of Evidence Encoding",
    "section": "29.4 Python Examples",
    "text": "29.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the CatBoostEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.woe import WOEEncoder\n\nct = ColumnTransformer(\n    [('WOEEncoding', WOEEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Street\"]].values.flatten() == \"Pave\")\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('WOEEncoding', WOEEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('WOEEncoding', WOEEncoder(), ['MS_Zoning'])]) WOEEncoding['MS_Zoning'] WOEEncoderWOEEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"WOEEncoding.*\")\n\n      WOEEncoding__MS_Zoning\n0                      0.778\n1                     -2.008\n2                      0.778\n3                      0.778\n4                      0.778\n...                      ...\n2925                   0.778\n2926                   0.778\n2927                   0.778\n2928                   0.778\n2929                   0.778\n\n[2930 rows x 1 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Weight of Evidence Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-jamesstein.html",
    "href": "categorical-jamesstein.html",
    "title": "30  James-Stein Encoding",
    "section": "",
    "text": "30.1 James-Stein Encoding\nThe James-Stein encoding method is another variation of target encoding as seen in Chapter 23. This page will explain how the James-Stein encoding is different than target encoding, and it is thus encouraged to read that chapter first.\nThe main difference between James-Stein and target encoding is the way it handles the shrinkage. It uses the variance of the groups and the global variance to denote the amount of shrinkage to apply. If the variance of the group is larger than the whole, then we pull the estimate close to the overall mean, if the group variance is smaller than the whole then we don’t pull as much.\nFor the following equation\n\\[\nJS_i = (1 - B_i) * \\text{mean}(y_i) + B * \\text{mean}(y)\n\\]\nwe have that \\(JS_i\\) is the James-Stein estimate for the \\(i\\)’th group, with \\(\\text{mean}(y_i)\\) being the mean of the \\(i\\)’th group of the target, and \\(\\text{mean}(y)\\) is the overall mean of the target.\nWe now need to find \\(B_i\\) which is our amount of shrinkage for each group.\n\\[\nB_i = \\dfrac{\\text{var}(y_i)}{\\text{var}(y_i) + \\text{var}(y)}\n\\]\nWhich when put into words are expressed as so.\n\\[\nB_i = \\dfrac{\\text{group variance}}{\\text{group variance} + \\text{overall variance}}\n\\]\nSince variances are non-negative, the value of \\(B_i\\) is bounded between 0 and 1, with it being 0 when the group variance is 0 and tending towards 1 when the group variance is larger than the overall variance.\nAll of the other considerations we have with target encoding apply to this method. There is no clear-cut reason why you should pick James-Stein over target encoding or vice versa. Trying both and seeing how it does is recommended.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>James-Stein Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-jamesstein.html#pros-and-cons",
    "href": "categorical-jamesstein.html#pros-and-cons",
    "title": "30  James-Stein Encoding",
    "section": "30.2 Pros and Cons",
    "text": "30.2 Pros and Cons\n\n30.2.1 Pros\n\nCan deal with categorical variables with many levels\nCan deal with unseen levels in a sensible way\nRuns fast with sensible shrinkage\nLess prone to overfitting that unkinked target encoding\n\n\n\n30.2.2 Cons\n\nOnly defined for normal distributions. Unsure whether this matters",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>James-Stein Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-jamesstein.html#r-examples",
    "href": "categorical-jamesstein.html#r-examples",
    "title": "30  James-Stein Encoding",
    "section": "30.3 R Examples",
    "text": "30.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>James-Stein Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-jamesstein.html#python-examples",
    "href": "categorical-jamesstein.html#python-examples",
    "title": "30  James-Stein Encoding",
    "section": "30.4 Python Examples",
    "text": "30.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the JamesSteinEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.james_stein import JamesSteinEncoder\n\nct = ColumnTransformer(\n    [('jamesstein', JamesSteinEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('jamesstein', JamesSteinEncoder(),\n                                 ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('jamesstein', JamesSteinEncoder(),\n                                 ['MS_Zoning'])]) jamesstein['MS_Zoning'] JamesSteinEncoderJamesSteinEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      jamesstein__MS_Zoning  ... remainder__Latitude\n0                187726.407  ...              42.054\n1                141453.434  ...              42.053\n2                187726.407  ...              42.053\n3                187726.407  ...              42.051\n4                187726.407  ...              42.061\n...                     ...  ...                 ...\n2925             187726.407  ...              41.989\n2926             187726.407  ...              41.988\n2927             187726.407  ...              41.987\n2928             187726.407  ...              41.991\n2929             187726.407  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>James-Stein Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-mestimator.html",
    "href": "categorical-mestimator.html",
    "title": "31  M-Estimator Encoding",
    "section": "",
    "text": "31.1 M-Estimator Encoding\nThe M-estimator encoding method is another variation of Target Encoding. This page will explain how M-estimator encoding is different from target encoding, so it is encouraged to read that chapter first.\nThe idea behind M-estimator encoding is the same as the other target encoding methods. But we are using a different mean, namely M-estimator which is a statistical estimator that is less influenced by extreme values in the target value.\nWe use the following formula to calculate the effect of each level.\n\\[\nM_i = \\dfrac{\\text{count}(category_i) \\cdot \\text{mean}(category_i) + M \\cdot \\text{mean}(target)}{\\text{count}(category_i) + M}\n\\]\nNote that it contains a hyperparameter \\(M\\). This value has to be tuned, and will thus invite data leakage if not tuned correctly.\nThe method by itself doesn’t perform shrinkage so you run into issues associated with lack of shrinkage.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>M-Estimator Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-mestimator.html#pros-and-cons",
    "href": "categorical-mestimator.html#pros-and-cons",
    "title": "31  M-Estimator Encoding",
    "section": "31.2 Pros and Cons",
    "text": "31.2 Pros and Cons\n\n31.2.1 Pros\n\nRobust to extreme values in target\n\n\n\n31.2.2 Cons\n\nHas to be tuned",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>M-Estimator Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-mestimator.html#r-examples",
    "href": "categorical-mestimator.html#r-examples",
    "title": "31  M-Estimator Encoding",
    "section": "31.3 R Examples",
    "text": "31.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>M-Estimator Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-mestimator.html#python-examples",
    "href": "categorical-mestimator.html#python-examples",
    "title": "31  M-Estimator Encoding",
    "section": "31.4 Python Examples",
    "text": "31.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the MEstimateEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.m_estimate import MEstimateEncoder\n\nct = ColumnTransformer(\n    [('mestimate', MEstimateEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('mestimate', MEstimateEncoder(),\n                                 ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('mestimate', MEstimateEncoder(),\n                                 ['MS_Zoning'])]) mestimate['MS_Zoning'] MEstimateEncoderMEstimateEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      mestimate__MS_Zoning  ... remainder__Latitude\n0               191278.640  ...              42.054\n1               138004.645  ...              42.053\n2               191278.640  ...              42.053\n3               191278.640  ...              42.051\n4               191278.640  ...              42.061\n...                    ...  ...                 ...\n2925            191278.640  ...              41.989\n2926            191278.640  ...              41.988\n2927            191278.640  ...              41.987\n2928            191278.640  ...              41.991\n2929            191278.640  ...              41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>M-Estimator Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-thermometer.html",
    "href": "categorical-thermometer.html",
    "title": "32  Thermometer Encoding",
    "section": "",
    "text": "32.1 Thermometer Encoding\nThermometer encoding (also called Rank Hot Encoding) is a variation of Dummy Encoding. It is intended only for ordinal data.\nWhere one-hot encoding produces 1 for the current level and 0 for all other levels, thermometer encoding produces 1 for the current level and lesser levels and 0 for other levels.\nConsidering this short ordinal variable of emotions, we observe there are 3 unique values “sad” &lt; “neutral” &lt; “happy”. These values clearly have an order as listed.\nThere should be 3 columns one for each of the levels.\nNotice how the happy instances have 1s all across and sad only has 1. You can think of this encoding as making this cumulative. Asking the question “is this emotion at least this X”.\nWhile this method is often called rank hot encoding, you should use the dummy variant, since the first column produced by definition will be constant.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Thermometer Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-thermometer.html#pros-and-cons",
    "href": "categorical-thermometer.html#pros-and-cons",
    "title": "32  Thermometer Encoding",
    "section": "32.2 Pros and Cons",
    "text": "32.2 Pros and Cons\n\n32.2.1 Pros\n\nexplainable results\nfast calculations\n\n\n\n32.2.2 Cons\n\nshould only be used for ordinal data",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Thermometer Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-thermometer.html#r-examples",
    "href": "categorical-thermometer.html#r-examples",
    "title": "32  Thermometer Encoding",
    "section": "32.3 R Examples",
    "text": "32.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Thermometer Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-thermometer.html#python-examples",
    "href": "categorical-thermometer.html#python-examples",
    "title": "32  Thermometer Encoding",
    "section": "32.4 Python Examples",
    "text": "32.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the RankHotEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.rankhot import RankHotEncoder\n\nct = ColumnTransformer(\n    [('rankhot', RankHotEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('rankhot', RankHotEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('rankhot', RankHotEncoder(), ['MS_Zoning'])]) rankhot['MS_Zoning'] RankHotEncoderRankHotEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames).filter(regex=\"rankhot.*\")\n\n      rankhot__MS_Zoning_1  ...  rankhot__MS_Zoning_7\n0                        1  ...                     0\n1                        1  ...                     0\n2                        1  ...                     0\n3                        1  ...                     0\n4                        1  ...                     0\n...                    ...  ...                   ...\n2925                     1  ...                     0\n2926                     1  ...                     0\n2927                     1  ...                     0\n2928                     1  ...                     0\n2929                     1  ...                     0\n\n[2930 rows x 7 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Thermometer Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-quantile.html",
    "href": "categorical-quantile.html",
    "title": "33  Quantile Encoding",
    "section": "",
    "text": "33.1 Quantile Encoding\nQuantile encoding (Mougan et al. 2021), is a reimagined version of Target Encoding and M-estimator Encoding that uses quantiles instead of means and M regulatization from M-estimator.\nWhereas target encoding uses the mean as an aggregation function, quantile encoding uses any quantile as its aggregation function. Most of the things we know about target encoding are also true for quantile encoding. The differences come with how quantiles differ from means. Quantiles are generally more robust to outliers, for quantiles away from the end. This same pattern is mirrored in quantile encoding.\nQuantile encoding is suggested to be paired with M-estimator style regularization to deal with the issue of having smaller groups.\nThe following formula is used to calculate the quantile encodings.\n\\[\nQE_i = \\dfrac{q(category_i) \\cdot n_i + q(whole) \\cdot M}{n_i + M}\n\\]\n\\(QE_i\\) is the encoding value for the \\(i\\)’th category. \\(q(category_i)\\) is the quantile of the values within the \\(i\\)’th category, \\(q(whole)\\) is the quantile of the whole data set. \\(n_i\\) is the number of observations in the \\(i\\)’th category and \\(M\\) is the hyperparameter \\(M\\) that handles the regularization.\nIn essense we have 2 hyper parameters for this style on encoding, one is \\(M\\) which we very much has to tune, and the other one is the quantile of choice. We could set the quantile to specific values, such as 0.5 for median, but tuning it is likely to give better results. But this again is a trade-off between computational time and performance.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Quantile Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-quantile.html#pros-and-cons",
    "href": "categorical-quantile.html#pros-and-cons",
    "title": "33  Quantile Encoding",
    "section": "33.2 Pros and Cons",
    "text": "33.2 Pros and Cons\n\n33.2.1 Pros\n\nless prone to outliers compared to target encoding\n\n\n\n33.2.2 Cons\n\nhas hyperparameters in need of tuning",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Quantile Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-quantile.html#r-examples",
    "href": "categorical-quantile.html#r-examples",
    "title": "33  Quantile Encoding",
    "section": "33.3 R Examples",
    "text": "33.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Quantile Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-quantile.html#python-examples",
    "href": "categorical-quantile.html#python-examples",
    "title": "33  Quantile Encoding",
    "section": "33.4 Python Examples",
    "text": "33.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the QuantileEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.quantile_encoder import QuantileEncoder\n\nct = ColumnTransformer(\n    [('quantile', QuantileEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('quantile', QuantileEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('quantile', QuantileEncoder(), ['MS_Zoning'])]) quantile['MS_Zoning'] QuantileEncoderQuantileEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      quantile__MS_Zoning  ... remainder__Latitude\n0              171994.723  ...              42.054\n1              140714.286  ...              42.053\n2              171994.723  ...              42.053\n3              171994.723  ...              42.051\n4              171994.723  ...              42.061\n...                   ...  ...                 ...\n2925           171994.723  ...              41.989\n2926           171994.723  ...              41.988\n2927           171994.723  ...              41.987\n2928           171994.723  ...              41.991\n2929           171994.723  ...              41.989\n\n[2930 rows x 74 columns]\n\n\n\n\n\n\nMougan, Carlos, David Masip, Jordi Nin, and Oriol Pujol. 2021. “Quantile Encoder: Tackling High Cardinality Categorical Features in Regression Problems.” In Modeling Decisions for Artificial Intelligence, edited by Vicenç Torra and Yasuo Narukawa, 168–80. Cham: Springer International Publishing.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Quantile Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-summary.html",
    "href": "categorical-summary.html",
    "title": "34  Summary Encoding",
    "section": "",
    "text": "34.1 Summary Encoding\nYou can repeat Quantile Encoding, using using different quantiles for more information extraction, e.i. with 0.25, 0.5, and 0.75 quantile. This is called summary encoding.\nOne of the downsides of quantile encoding is that you need to pick or tune to find a good quantile. Summary encoding curcomvents this issue by calculating a lot of quantiles at the same time.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Summary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-summary.html#pros-and-cons",
    "href": "categorical-summary.html#pros-and-cons",
    "title": "34  Summary Encoding",
    "section": "34.2 Pros and Cons",
    "text": "34.2 Pros and Cons\n\n34.2.1 Pros\n\nLess tuning than quantile encoding\n\n\n\n34.2.2 Cons\n\nMore computational than quantile encoding\nchance of producing correlated or redundant features",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Summary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-summary.html#r-examples",
    "href": "categorical-summary.html#r-examples",
    "title": "34  Summary Encoding",
    "section": "34.3 R Examples",
    "text": "34.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Summary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-summary.html#python-examples",
    "href": "categorical-summary.html#python-examples",
    "title": "34  Summary Encoding",
    "section": "34.4 Python Examples",
    "text": "34.4 Python Examples\nWe are using the ames data set for examples. {category_encoders} provided the SummaryEncoder() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom category_encoders.quantile_encoder import SummaryEncoder\n\nct = ColumnTransformer(\n    [('summary', SummaryEncoder(), ['MS_Zoning'])], \n    remainder=\"passthrough\")\n\nct.fit(ames, y=ames[[\"Sale_Price\"]].values.flatten())\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('summary', SummaryEncoder(), ['MS_Zoning'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('summary', SummaryEncoder(), ['MS_Zoning'])]) summary['MS_Zoning'] SummaryEncoderSummaryEncoder() remainder['MS_SubClass', 'Lot_Frontage', 'Lot_Area', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Mas_Vnr_Area', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Wood_Deck_SF', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Sale_Price', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      summary__MS_Zoning_25  ...  remainder__Latitude\n0                137496.482  ...               42.054\n1                111612.500  ...               42.053\n2                137496.482  ...               42.053\n3                137496.482  ...               42.051\n4                137496.482  ...               42.061\n...                     ...  ...                  ...\n2925             137496.482  ...               41.989\n2926             137496.482  ...               41.988\n2927             137496.482  ...               41.987\n2928             137496.482  ...               41.991\n2929             137496.482  ...               41.989\n\n[2930 rows x 75 columns]",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Summary Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-collapse.html",
    "href": "categorical-collapse.html",
    "title": "35  Collapsing Categories",
    "section": "",
    "text": "35.1 Collapsing Categories\nThere are times, especially when you have a lot of levels in a categorical variable, that it will be beneficial for you to combine some of them. This practice is in some ways similar to what we saw in Chapter 16 about cleaning categorical variables, but here we are doing it for performance reasons.\nEssentially what we are working on, is trying to combine many levels to get higher performance and interpretability\nthis has two prongs\nThe first issue can be quite a common one. See the below distribution as an example\nThe proportion of how often each level appears is quite stark, to the point where 4 of them happen less than 10 times, which is not a lot considering the most frequent level occurs over 1000 times.\nFor some methods such as Dummy Encoding, having these infrequent levels would not do us much good, and may even make things worse. Having a level be so infrequent increases its likelihood of being uninformative. This is where collapsing can come into play. The method takes the most infrequent levels and combines them into one, typically called \"other\".\nAbove we see how that is done. We took all the levels that appeared less than 2.5% of the time and combined them into a new level called \"other\". This value threshold will off cause depend on many things and is a good candidate for tuning. And we don’t have to do it as a percentage, we might as well do it based on counts. Collapsing anything with less than 10 occurrences.\nThis method can give pretty good results. But is by nature very crude. We are more than likely to combine levels that have nothing to do with each other than their low frequency. This will sometimes be inefficient, and while it has straightforward explainability due to its simple nature it can be hard to argue for its approach to shareholders.\nThis is where the other type of collapsing comes in. These methods use a little more information about the data, in the hopes that the collapsing will be more sensible. We will describe two of them here. First is the model-based approach. You can imagine that we fit a small model, such as a decision tree on the categorical variable, using a sensible outcome. This outcome could the the real outcome of our larger modeling problem. then we let the decision tree run, and the way the tree splits the data is how we combine the levels.\nThis is done below as an example:\nAnd it isn’t super hard to see that these make sense. “Asbestos Shingles” and “Asphalt Shingles” got paired, as did “Metal Siding” and “Wood Siding”, and “Hard Board” and “Plywood”. These are of course only linked because they appear similar when looking at the sale price. But We are likely able to reason better about these new labels than before.\nWhen to stop growing the is still something that we can and should tune. Too small of a tree and we collapse levels that should be different, too large a tree and we don’t collapse things that should have been collapsed.\nAnother interesting method is when we use the levels themselves to decide how to do the collapsing. Using string distances between the levels. Hence similar levels will be collapsed together.\nThis doesn’t use the data at all, and the results reflect that. The distance will again have to be used for best results, and there are quite a few different types of ways to calculate string distance. The choices will sometimes matter a lot and should be investigated before use for best results.\nThe string distance collapsing method does in some ways feel like a cleaning method like we saw in Chapter 16 about cleaning categorical variables. Having many levels in a freeform state will often lead to misspellings or other slight variations. String distances are a great way to deal with those types of problems. It is recommended that you always look at the resulting mappings to make sure they are created correctly, or good enough.\nSome data is naturally hierarchical, and if we have this type of data and we want to collapse, we can take advantage of that and collapse according to the hierarchies.\nCollapsing categorical levels is not without risk, and each application should be monitored and checked to make sure we don’t do anything bad. In many ways, this is quite a manual process and luckily the results are very easy to validate.\nUnseen levels will have to be treated on a method-to-method basis. Tree-based collapsing will not be able to handle unseen levels as they are not part of the tree at all. On the other hand, it is possible to handle unseen levels on the string distance method as you just need to calculate the distances. With the caveat that some strings could be the same distance away from multiple levels.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Collapsing Categories</span>"
    ]
  },
  {
    "objectID": "categorical-collapse.html#pros-and-cons",
    "href": "categorical-collapse.html#pros-and-cons",
    "title": "35  Collapsing Categories",
    "section": "35.2 Pros and Cons",
    "text": "35.2 Pros and Cons\n\n35.2.1 Pros\n\nEasy to perform and verify\nComputationally fast\n\n\n\n35.2.2 Cons\n\nMust be tuned\nCan produce counterintuitive results",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Collapsing Categories</span>"
    ]
  },
  {
    "objectID": "categorical-collapse.html#r-examples",
    "href": "categorical-collapse.html#r-examples",
    "title": "35  Collapsing Categories",
    "section": "35.3 R Examples",
    "text": "35.3 R Examples\nMethods to collapse categorical levels can be found in the recipes package with step_other() and the embed package in step_collapse_cart() and step_collapse_stringdist().\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set for examples\n\n\nOthering can be done using the step_other() function, it uses the argument threshold to determine the cutoff used to turn levels into \"other\" or not.\n\nlibrary(recipes)\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrec_other &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_other(Exterior_2nd, threshold = 0.1) |&gt;\n  prep()\n\nrec_other |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 other            215000\n 2 VinylSd          105000\n 3 Wd Sdng          172000\n 4 other            244000\n 5 VinylSd          189900\n 6 VinylSd          195500\n 7 other            213500\n 8 HdBoard          191500\n 9 other            236500\n10 VinylSd          189000\n# ℹ 2,920 more rows\n\n\nselecting a higher threshold turns more levels into \"other\".\n\nrec_other &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_other(Exterior_2nd, threshold = 0.5) |&gt;\n  prep()\n\nrec_other |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 other            215000\n 2 VinylSd          105000\n 3 other            172000\n 4 other            244000\n 5 VinylSd          189900\n 6 VinylSd          195500\n 7 other            213500\n 8 other            191500\n 9 other            236500\n10 VinylSd          189000\n# ℹ 2,920 more rows\n\n\nfor the more advanced methods, we turn to the embed package. To collapse levels by their string distance, we use the step_collapse_stringdist(). By default, you control it with the distance argument.\n\nrec_stringdist &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_collapse_stringdist(Exterior_2nd, distance = 5) |&gt;\n  prep()\n\nrec_stringdist |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 Plywood          215000\n 2 MetalSd          105000\n 3 AsbShng          172000\n 4 Brk Cmn          244000\n 5 MetalSd          189900\n 6 MetalSd          195500\n 7 CmentBd          213500\n 8 HdBoard          191500\n 9 CmentBd          236500\n10 MetalSd          189000\n# ℹ 2,920 more rows\n\n\nUnsurprisingly, there are almost a dozen different ways to calculate the distance between two strings. Most are supported and can be changed using the method argument, and further controlled using the options argument.\n\nrec_stringdist &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_collapse_stringdist(Exterior_2nd, \n                           distance = 0.75, \n                           method = \"cosine\", \n                           options = list(q = 2)) |&gt;\n  prep()\n\nrec_stringdist |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 Plywood          215000\n 2 MetalSd          105000\n 3 AsbShng          172000\n 4 Brk Cmn          244000\n 5 MetalSd          189900\n 6 MetalSd          195500\n 7 CmentBd          213500\n 8 HdBoard          191500\n 9 CmentBd          236500\n10 MetalSd          189000\n# ℹ 2,920 more rows\n\n\nLastly, we have the tree-based method, this is done using the step_collapse_cart() function. For this to work, you need to select an outcome variable using the outcome argument. With cost_complexity and min_n as arguments to change the shape of the tree.\n\nrec_cart &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_collapse_cart(Exterior_2nd, outcome = vars(Sale_Price)) |&gt;\n  prep()\n\nrec_cart |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Exterior_2nd   Sale_Price\n   &lt;fct&gt;               &lt;int&gt;\n 1 Exterior_2nd_5     215000\n 2 Exterior_2nd_7     105000\n 3 Exterior_2nd_3     172000\n 4 Exterior_2nd_6     244000\n 5 Exterior_2nd_7     189900\n 6 Exterior_2nd_7     195500\n 7 Exterior_2nd_8     213500\n 8 Exterior_2nd_5     191500\n 9 Exterior_2nd_8     236500\n10 Exterior_2nd_7     189000\n# ℹ 2,920 more rows",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Collapsing Categories</span>"
    ]
  },
  {
    "objectID": "categorical-collapse.html#python-examples",
    "href": "categorical-collapse.html#python-examples",
    "title": "35  Collapsing Categories",
    "section": "35.4 Python Examples",
    "text": "35.4 Python Examples\nhttps://github.com/skrub-data/skrub",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Collapsing Categories</span>"
    ]
  },
  {
    "objectID": "categorical-combination.html",
    "href": "categorical-combination.html",
    "title": "36  Categorical Combination",
    "section": "",
    "text": "36.1 Categorical Combination\nWhen you have multiple categorical variables, it might be the case that you want to see whether certain levels co-occur. For the variables day_of_week and weather, finding \"rainy Monday\"s can be hard for some model types.\nCategorical combination does what the name suggests. We are combining categorical variables one to one. We would multiply two numeric variables much the same way. The following table shows how we would combine a day of the week and weather variable.\nWe noticed a couple of things during this interaction. The first thing is that the number of unique levels goes up multiplicatively, so the newly created variable will include many levels. Depending on what the original variables encode, you will get impossible combinations or rarely occurring levels. This can be handled partly by Collapsing Categories.\nIt is not guaranteed that the combination feature is going to be better than the individual features before. It can be beneficial to keep both the individual features as long as the combination feature. In the above example, you can imagine that Fridays and rainy Mondays are high signals. If that is the case, then keeping all variables would be the best decision.\nThis method is not confined to just 2 variables, you could combine as many variables as you want. But the problems described with increased number of levels will be exacerbated.\nThis method will be hard to automatically employ and will thus need to be done manually. Despite this, it can provide a good boost in performance when done right.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Categorical Combination</span>"
    ]
  },
  {
    "objectID": "categorical-combination.html#pros-and-cons",
    "href": "categorical-combination.html#pros-and-cons",
    "title": "36  Categorical Combination",
    "section": "36.2 Pros and Cons",
    "text": "36.2 Pros and Cons\n\n36.2.1 Pros\n\nEasy to perform\nProduces easily explanatory features\n\n\n\n36.2.2 Cons\n\nCan’t be done automatically\nProduces categorical variables with many levels",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Categorical Combination</span>"
    ]
  },
  {
    "objectID": "categorical-combination.html#r-examples",
    "href": "categorical-combination.html#r-examples",
    "title": "36  Categorical Combination",
    "section": "36.3 R Examples",
    "text": "36.3 R Examples\nHas not yet been implemented.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Categorical Combination</span>"
    ]
  },
  {
    "objectID": "categorical-combination.html#python-examples",
    "href": "categorical-combination.html#python-examples",
    "title": "36  Categorical Combination",
    "section": "36.4 Python Examples",
    "text": "36.4 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.\nSee https://github.com/EmilHvitfeldt/feature-engineering-az/issues/40 for progress.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Categorical Combination</span>"
    ]
  },
  {
    "objectID": "categorical-multi-dummy.html",
    "href": "categorical-multi-dummy.html",
    "title": "37  Multi-Dummy Encoding",
    "section": "",
    "text": "37.1 Multi-Dummy Encoding\nThis chapter will cover what I like to call multi-dummy encoding. When you are able to extract multiple entries from one categorical variable or combine entries from multiple categorical variables.\nI find that this is best explained by example. We will start with combining entries from multiple variables. Imagine that you see this subsection in your data, perhaps denoting some individual’s language proficiencies. How would you encode it?\nOne could apply dummy variables on each of them individually, but that could potentially create a lot of columns as you would expect a near-equal number of levels produced from each variable. You could apply Target Encoding to the variables, but that too feels insufficient. What these methods don’t take into account is that there is some shared information between these variables that isn’t being picked up.\nWe can use the shared information to our advantage. The levels used in these categorical variables are likely the same, so we can treat them combined. In practice, this means that we count over all selected columns, and add dummies or counts accordingly.\nThis style of transformation often provides zero-one indicators rather than counts purely because of the construction of the data. But it doesn’t mean that counts can’t happen. Make sure that the implementation you are using matches the expectations you have for the data.\nOne thing that is lost in the method is the potential ordering of the variables. The above example has a natural ordering, indicative by the names of the variables. Depending on how important you think the ordering is, one could add a weighting scheme like below.\nNext, we look at extracting multiple entries. This can be seen as a convenient shorthand for text extraction, as is discussed in Chapter 47. I find that this pattern emerges enough by itself that it is worth denoting it as its own method.\nThe above example could instead be structured so\nWe can pull out the same counts as before, using regular expressions. One could pull out the entries in two main ways, by splitting or extraction. We could either split by , or extract sequences of characters [a-z]*.\nWith splitting, you can sometimes extract a lot of signal if you have a carefully crafted regular expression. Consider the following list of materials and mediums for a number of art pieces.\nAt first glance, they appear quite unstructured, but by splitting on (, )|( and )|( on ) you get the following mediums and techniques [canvas, etching, lithograph, oil paint, painted steel, paper, salt].",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Multi-Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-multi-dummy.html#pros-and-cons",
    "href": "categorical-multi-dummy.html#pros-and-cons",
    "title": "37  Multi-Dummy Encoding",
    "section": "37.2 Pros and Cons",
    "text": "37.2 Pros and Cons\n\n37.2.1 Pros\n\nCan provide increased signal\n\n\n\n37.2.2 Cons\n\nLess commonly occurring\nRequires careful eye and hand tuning",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Multi-Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-multi-dummy.html#r-examples",
    "href": "categorical-multi-dummy.html#r-examples",
    "title": "37  Multi-Dummy Encoding",
    "section": "37.3 R Examples",
    "text": "37.3 R Examples\n\nhttps://recipes.tidymodels.org/reference/step_dummy_multi_choice.html\nhttps://recipes.tidymodels.org/reference/step_dummy_extract.html\n\nWait for adoption data set",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Multi-Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "categorical-multi-dummy.html#python-examples",
    "href": "categorical-multi-dummy.html#python-examples",
    "title": "37  Multi-Dummy Encoding",
    "section": "37.4 Python Examples",
    "text": "37.4 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.",
    "crumbs": [
      "Categorical Features",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>Multi-Dummy Encoding</span>"
    ]
  },
  {
    "objectID": "datetime.html",
    "href": "datetime.html",
    "title": "38  Datetime Overview",
    "section": "",
    "text": "38.1 Datetime Overview\nDate and datetime variables are another type of data that can be quite common. This is different than time as we talked about in the Time Series section, as when we talk about time series data, it is typically a series of data points that are related, and we use that inherent structure of the data as the basis for the modeling problem. Here we are talking about predictors, that happen be to expressed as a date or datetime field.\nThese types of data could be the day where the sale took place, or when the taxi started and ended its trip. In some of these cases, it wouldn’t make sense to treat it as a time-series model, but we still want to be able to pull out valuable information. In many cases, date and datetime variables will be treated as text fields if they are unparsed, and as fancy integer representations. When encoded they typically use integers to denote time since a specific reference point.\nIf the date and datetime variables were used directly in a modeling function it would at best use the underlying integer representation, which is unlikely to be useful since it just denotes chronological time. At worst the modeling function will error and complain.\nThe chapters in this section are going to assume that you have parsed the date and datetime variables and that we are working with those directly.\nWhen we talk about extraction in Chapter 39, what we will be doing is extracting components of the data. This can be things like year, month, day, hour, minutes and seconds. There are more complicated things like “Is this a holiday?” or “Is it a weekend?”. After that, we will go over some more complicated features in Chapter 40. These features will mostly be based on the extraction features from earlier. But it can be things like “closest holiday” and “how long since last Monday”. Lastly, we will talk about how many of these features work in a very periodic way in Chapter 41. Naturally, if we were to model using hours of the day, 1 hour before midnight and 1 hour after midnight are close to the clock but not numerically.",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Datetime Overview</span>"
    ]
  },
  {
    "objectID": "datetime-extraction.html",
    "href": "datetime-extraction.html",
    "title": "39  Value Extraction",
    "section": "",
    "text": "39.1 Value Extraction\nSometimes date variables come in many data sets and problems. The presence of a date time variable doesn’t mean that we are working with a time series data set. If you are, then read more about how to handle that in the Time Series section. All the extract methods are not learned, as there isn’t anything to estimate.\nDates and date times are typically stored as integers. With dates represented as the number of days since 1970-01-01, with negative values for earlier dates. with date times representing the number of seconds since 1970-01-01.\nThis means that if you are lucky and the model you were trying to fit knew to convert dates and date times into integers, that “time since 1970-01-01” would be a helpful predictor. If not then we need to do some more advanced work.\nWe have assumed for this chapter that we already have the date variables in the right format. This would typically be\nfor dates, and\nfor date times.\nThere is also the wrinkle concerning time zones, leap years and leap seconds. Many rules can mess you up. It is for this reason that we recommend that you use a trusted datetime library to do the following calculations.\nMost libraries allow us to pull out standard measurements like\nbut we can include a couple more. quarter, semester, week. These are highly related to the above list, they are just in a different format. season is another possibly nice feature, but we have to be careful as the seasons flip depending on where on the globe we are.\nIn our above list of features, each of them counts up from 1, until we reach the level level. So minutes stop at 60 and start over. This might not be what we want, so we can include finer detail by extracting seconds in day, days in week, and days in year. It will be up to you to figure out if these are useful for you. Generally, these become useful in the broader sense. seconds in day is more finely grained than hour, so if you want to figure out the time of day, then the smaller measurements are helpful. Likewise, some of these methods are better expressed as decimals, as periods such as month have different lengths, and will thus be different. Talking about being 0.9 through the month will be more precise.\nFrom these we can also do things like weekends, weekdays, morning and holidays such as Christmas and Easter. Remember that there are libraries to extract these for you.\nSome of the measurements here can be extracted as categorical rather than numeric, things like day of week can either be extracted as 1, 2, 3 or Monday, Tuesday, Wednesday or month that can be extracted. We can think of the numeric extract as being an automatic Label Encoding of the categorical version. Sometimes it will be worth extracting the categorical version and using it directly or embedding it with other methods in the Categorical section. It is worth noting that the categorical version of these variables is ordinal.\nThe talk in this chapter is very Euro and US-focused. Many cultures around the world divide the “year” and “day” up differently. Always use the conventions that are most appropriate to the culture you are working with.",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Value Extraction</span>"
    ]
  },
  {
    "objectID": "datetime-extraction.html#pros-and-cons",
    "href": "datetime-extraction.html#pros-and-cons",
    "title": "39  Value Extraction",
    "section": "39.2 Pros and Cons",
    "text": "39.2 Pros and Cons\n\n39.2.1 Pros\n\nFast and easy computations\nCan provide good results\n\n\n\n39.2.2 Cons\n\nThe numerical features generated are all increasing with time linearly\nThere are a lot of extractions, and they correlate quite a bit",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Value Extraction</span>"
    ]
  },
  {
    "objectID": "datetime-extraction.html#r-examples",
    "href": "datetime-extraction.html#r-examples",
    "title": "39  Value Extraction",
    "section": "39.3 R Examples",
    "text": "39.3 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\nlibrary(recipes)\n\nhotel_bookings |&gt;\n  select(reservation_status_date)\n\n# A tibble: 119,390 × 1\n   reservation_status_date\n   &lt;date&gt;                 \n 1 2015-07-01             \n 2 2015-07-01             \n 3 2015-07-02             \n 4 2015-07-02             \n 5 2015-07-03             \n 6 2015-07-03             \n 7 2015-07-03             \n 8 2015-07-03             \n 9 2015-05-06             \n10 2015-04-22             \n# ℹ 119,380 more rows\n\n\n{recipes} provide two steps for date time extraction. step_date() handles dates, and step_time() handles the sub-day time features. The steps work the same way, so we will only show how step_date() works here. A couple of features are selected by default,\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date)\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 5\n$ reservation_status_date       &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, 201…\n$ is_canceled                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0…\n$ reservation_status_date_dow   &lt;fct&gt; Wed, Wed, Thu, Thu, Fri, Fri, Fri, Fri, …\n$ reservation_status_date_month &lt;fct&gt; Jul, Jul, Jul, Jul, Jul, Jul, Jul, Jul, …\n$ reservation_status_date_year  &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015…\n\n\nBut you can use the features argument to specify other types as well\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date, \n            features = c( \"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                          \"quarter\", \"dow\", \"month\"))\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 10\n$ reservation_status_date          &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, …\n$ is_canceled                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0…\n$ reservation_status_date_year     &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ reservation_status_date_doy      &lt;int&gt; 182, 182, 183, 183, 184, 184, 184, 18…\n$ reservation_status_date_week     &lt;int&gt; 26, 26, 27, 27, 27, 27, 27, 27, 18, 1…\n$ reservation_status_date_decimal  &lt;dbl&gt; 2015.496, 2015.496, 2015.499, 2015.49…\n$ reservation_status_date_semester &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2…\n$ reservation_status_date_quarter  &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3…\n$ reservation_status_date_dow      &lt;fct&gt; Wed, Wed, Thu, Thu, Fri, Fri, Fri, Fr…\n$ reservation_status_date_month    &lt;fct&gt; Jul, Jul, Jul, Jul, Jul, Jul, Jul, Ju…\n\n\nfeatures that can be categorical will be so by default, but can be turned off by setting label = FALSE.\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date, \n            features = c( \"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                          \"quarter\", \"dow\", \"month\"), label = FALSE)\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 10\n$ reservation_status_date          &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, …\n$ is_canceled                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0…\n$ reservation_status_date_year     &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ reservation_status_date_doy      &lt;int&gt; 182, 182, 183, 183, 184, 184, 184, 18…\n$ reservation_status_date_week     &lt;int&gt; 26, 26, 27, 27, 27, 27, 27, 27, 18, 1…\n$ reservation_status_date_decimal  &lt;dbl&gt; 2015.496, 2015.496, 2015.499, 2015.49…\n$ reservation_status_date_semester &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2…\n$ reservation_status_date_quarter  &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3…\n$ reservation_status_date_dow      &lt;int&gt; 4, 4, 5, 5, 6, 6, 6, 6, 4, 4, 3, 1, 1…\n$ reservation_status_date_month    &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 5, 4, 6, 7, 7…\n\n\nIf we want to extract holiday features, we can use the step_holiday() function, which uses the {timeDate} library. With known holidays listed in timeDate::listHolidays().\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_holiday(reservation_status_date, \n               holidays = c(\"BoxingDay\", \"CAFamilyDay\", \"JPConstitutionDay\"))\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 5\n$ reservation_status_date                   &lt;date&gt; 2015-07-01, 2015-07-01, 201…\n$ is_canceled                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ reservation_status_date_BoxingDay         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reservation_status_date_CAFamilyDay       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ reservation_status_date_JPConstitutionDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Value Extraction</span>"
    ]
  },
  {
    "objectID": "datetime-extraction.html#python-examples",
    "href": "datetime-extraction.html#python-examples",
    "title": "39  Value Extraction",
    "section": "39.4 Python Examples",
    "text": "39.4 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Value Extraction</span>"
    ]
  },
  {
    "objectID": "datetime-advanced.html",
    "href": "datetime-advanced.html",
    "title": "40  Advanced Features",
    "section": "",
    "text": "40.1 Advanced Features\nAll the features we were able to extract were related to what day or time it was for a given observation. Or numbers on the form “how many since the start of the month” or “how many days since the start of the week”. And while this information can be useful, there will often be times when we want to do slight modifications that can result in huge payoffs.\nConsider merchandise sale-related data. The mere indication of specific dates might become useful, but the sale amount is not likely to be affected just on the sale days, but on the surrounding days as well. Consider the American Black Friday. This day is predetermined to come every year at an easily recognized day, namely the last Friday of November. Considering its close time to Christmas and other gift-giving holidays, it is a common day for thrifty people to start buying presents.\nIn the extraction since we have a single indicator for the day of Black Friday\nBut it would make sense that since we know the day of Black Friday, that the sales will see a drop on the previous days, we can incorporate that as well.\nOn the other hand, once the sale has started happening the sales to pick up again. Since this is the last big sale before the Holidays, shoppers are free to buy their remaining presents as they don’t have to fear the item going on sale.\nThe exact effects shown here are just approximate to our story at hand. But they provide a useful illustration. There is a lot of bandwidth to be given if we look at date times from a distance perspective. We can play around with “distance from” and “distance to”, different numerical transformations we saw in Numeric Overview, and signs and indicators we talked about in Chapter 39 to tailor our feature engineering to our problem.\nWhat all these methods have in common is a reference point. For an extracted day feature, the reference point is “first of the month” and the after-function is x, or in other words “days since the time of day”. We see this in the following chart. Almost all extracted functions follow this formula\nwe could just as well do the inverse and look at how many days are left in the month. This would have a before-function of x as well.\nWe can do a both-sided formula by looking at “how many days are we away from a weekend”. This would have both the before and after functions be x and look like so. Here it isn’t too interesting as it is quite periodic, but using the same measure with “sale” instead of “weekend” and suddenly you have something different.\nThere are many other functions you can use, they will depend entirely on your task at hand. A few examples are shown below for inspiration.\nWhat makes these calculations so neat is that they can be tailored to our task at hand and that they work with irregular events such as holidays and signup dates. These methods are not circular by definition, but they will work in many ways it. We will cover explicit circular methods in Chapter 41.",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Advanced Features</span>"
    ]
  },
  {
    "objectID": "datetime-advanced.html#pros-and-cons",
    "href": "datetime-advanced.html#pros-and-cons",
    "title": "40  Advanced Features",
    "section": "40.2 Pros and Cons",
    "text": "40.2 Pros and Cons\n\n40.2.1 Pros\n\n\n40.2.2 Cons",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Advanced Features</span>"
    ]
  },
  {
    "objectID": "datetime-advanced.html#r-examples",
    "href": "datetime-advanced.html#r-examples",
    "title": "40  Advanced Features",
    "section": "40.3 R Examples",
    "text": "40.3 R Examples",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Advanced Features</span>"
    ]
  },
  {
    "objectID": "datetime-advanced.html#python-examples",
    "href": "datetime-advanced.html#python-examples",
    "title": "40  Advanced Features",
    "section": "40.4 Python Examples",
    "text": "40.4 Python Examples",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Advanced Features</span>"
    ]
  },
  {
    "objectID": "datetime-periodic.html",
    "href": "datetime-periodic.html",
    "title": "41  Periodic Features",
    "section": "",
    "text": "41.1 Periodic Features\nWhen working with datetime features, there are a couple of instances where it is useful to work with them in a purely periodic sense. We will cover those instances and why we would want to do it in this chapter. For a broader and more comprehensive look at how to work with periodic features, see the periodic section.\nTo make things perfectly periodic we need to work with regular units of seasonality. This is quite nice as we have the units that are going to guide the periodic nature of the effects. This is unlike other types of periodic data where we need to find the scale of the period.\nWe can work with things like hours, days and weeks directly. But we need to take extra care when working with uneven periods such as months and years, since the months are not all the same length, and leap years exist. One way to deal with this is to work with the decimal representation.\nOnce we have the datetime variables in the right format, all we have to do is find an appropriate periodic method in the periodic section and apply it.",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Periodic Features</span>"
    ]
  },
  {
    "objectID": "datetime-periodic.html#pros-and-cons",
    "href": "datetime-periodic.html#pros-and-cons",
    "title": "41  Periodic Features",
    "section": "41.2 Pros and Cons",
    "text": "41.2 Pros and Cons\n\n41.2.1 Pros\n\nThe nature of datetime variables naturally leads to periodic patterns\n\n\n\n41.2.2 Cons\n\nExtra care and work need to be done to find the right period",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Periodic Features</span>"
    ]
  },
  {
    "objectID": "datetime-periodic.html#r-examples",
    "href": "datetime-periodic.html#r-examples",
    "title": "41  Periodic Features",
    "section": "41.3 R Examples",
    "text": "41.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nAdd recipes steps",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Periodic Features</span>"
    ]
  },
  {
    "objectID": "datetime-periodic.html#python-examples",
    "href": "datetime-periodic.html#python-examples",
    "title": "41  Periodic Features",
    "section": "41.4 Python Examples",
    "text": "41.4 Python Examples",
    "crumbs": [
      "Datetime Features",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>Periodic Features</span>"
    ]
  },
  {
    "objectID": "missing.html",
    "href": "missing.html",
    "title": "42  Missing Overview",
    "section": "",
    "text": "42.1 Missing Overview\nMissing data is sometimes hard to avoid. So we will have to work to deal with them. This is something we can’t avoid since many implemented models are not set up to work with missing data and will error if it encounters them.\nWe have different ways data can be missing. It is important to identify which type it is as it will change how we should deal with it. The 3 types we will consider are Missing Completely At Random (MCAR), Missing At Random (MAR), and Missing Not At Random (MNAR) (RUBIN 1976).\nWhen data is MCAR, then we have that the missingness is unrelated to anything else in our data set or other relevant information. This is the cleanest form of missingness as if a truly random die was rolled for each value to determine whether a value should be marked as missing or not. This type of missing data doesn’t contain any information, and our method of remedying it is about causing the least destruction. Data are very rarely missing completely at random.\nWhen data is MAR, then the missingness is related to the observed data but not the unobserved data. We can find this type of missingness by looking at the relationship between the missingness and the observed data we have. If we are collecting reviews from hotel stays, we might imagine that satisfaction with the stay is related to how much information we get in other fields. This is a much bigger class of missingness than MCAR and something we can find good remedies for. Most methods that try to deal with missing data assume the data is MAR.\nLastly, we have MNAR. This happens when neither MCAR nor MAR holds. Rephrased, this happens when the missingness isn’t related to anything we have observed. An example of this is an instrument that wears over time, giving higher and higher probabilities of producing missing values. If we don’t know and measure this, then we have data that are MNAR. MNAR might feel like MCAR, so our main goal is to figure out if it is or not. This should all be done at the EDA stage.\nAnother wrinkle to missingness is that we can have a combination of the above methods. This is happening because we see missingness in two ways, observation-wise and column-wise. For observation missingness, the whole observation is missing. E.I. all the values for the observation are missing, maybe except for the ID. For column missingness, it is the values of a given feature that can be missing or not. You could imagine a data set where some columns are MCAR while others are MAR. We need to treat the columns accordingly.\nHow and why the data might be missing will require investigation and domain knowledge, both of which this book won’t be able to give you. What the following chapters will show you, is the ways to deal with the missingness, once you find out what measure is appropriate.\nDepending on what type of missing data you have, and the access to the data-generating process. You might be able to improve your collected data to fill in the blanks. Suppose that some of your data comes from OCR1 from some documents. If the original images are still available and the missing values could be removed by changing the settings then we can deal with the missing values directly. You might even be able to manually correct missing data.\nThe following chapters assume that the reader has identified missing values. Sometimes they are easily labeled as NA in your code and database. but creativity knows no bounds, which sadly has influenced some users in their way of identifying missing values. Common values are -9, 9999, and 9000. If you are lucky, then you can find this information in the data dictionary, but sometimes you need to be in contact with the people in charge of data collection. To make matters more complicated, there will also be times when different types of missingness will be encoded differently, such as 9000 for machine failure, and 9001 for missing sheet.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Missing Overview</span>"
    ]
  },
  {
    "objectID": "missing.html#imputation",
    "href": "missing.html#imputation",
    "title": "42  Missing Overview",
    "section": "42.2 Imputation",
    "text": "42.2 Imputation\nOne of the most common ways of dealing with missing values is to fill them in with some values. The types of methods that do this can be split into two groups. Simple Imputation is when you use the values in the variable to impute its missing values, which is where mean and mode imputation are found. Anything more complicated than this will be found in the Model Based Imputation. This is where multiple columns are used to determine the type of imputation needed.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Missing Overview</span>"
    ]
  },
  {
    "objectID": "missing.html#indication",
    "href": "missing.html#indication",
    "title": "42  Missing Overview",
    "section": "42.3 Indication",
    "text": "42.3 Indication\nIf you suspect that the data is not missing at random, it might be worthwhile to include the missingness as an indicator in your data. We will see how we can do that in Chapter 45.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Missing Overview</span>"
    ]
  },
  {
    "objectID": "missing.html#removal",
    "href": "missing.html#removal",
    "title": "42  Missing Overview",
    "section": "42.4 Removal",
    "text": "42.4 Removal\nAs a last resort, you might want to remove variables or rows with missing data, we will see how that is done in Chapter 46. This chapter is put last in this section, as it is generally not the preferred action, and all other avenues should be considered before removal is done.\n\n\n\n\nRUBIN, DONALD B. 1976. “Inference and missing data.” Biometrika 63 (3): 581–92. https://doi.org/10.1093/biomet/63.3.581.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Missing Overview</span>"
    ]
  },
  {
    "objectID": "missing.html#footnotes",
    "href": "missing.html#footnotes",
    "title": "42  Missing Overview",
    "section": "",
    "text": "Optical Character Recognition is used to extract text from images.↩︎",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Missing Overview</span>"
    ]
  },
  {
    "objectID": "missing-simple.html",
    "href": "missing-simple.html",
    "title": "43  Simple Imputation",
    "section": "",
    "text": "43.1 Simple Imputation\nWhen dealing with missing data, the first suggestion is often imputation. Simple imputation as covered in this chapter refers to the method where for each variable a single value is selected, and all missing values will be replaced by that value. It is important to remind ourselves why we impute missing values. Because the model we are using isn’t able to handle missing values natively. Simple imputation is the simplest way to handle missing values.\nThe way we find this value will depend on the variable type, and within each type of variable, we find multiple ways to decide on a value. The main categories are categorical and numeric variables.\nFor categorical we have 2 main options. We can replace the value of NA with \"missing\" or some other unused value. This would denote that the value was missing, without trying to pick a value. It would change the cardinality of the predictors as variables with levels “new”, “used”, and “old” would go from 3 levels to 4. Which may be undesirable. Especially with ordinal variables as it is unclear where the \"missing\" should go in the ordering.\nThe simple imputation way to deal with categorization is to find the value that we want to impute with. Typically this will be calculated with the mode, e.i. the most frequent value. It doesn’t have to be the most frequent value, you could set up the imputation to pick the 5th most common value or the least common value. But this is quite uncommon, and the mode is used by far the most. These methods also work for low cardinality integers.\nFor numeric values, we don’t have the option to add a new type of value. One option is to manually select the imputed value. If this approach is taken, then it should be done with utmost caution! It would also make it an unlearned imputation method. What is typically done is that some value is picked as the replacement. Be it the median, mean or even mode.\nDatetime variables will be a different story. One could use the mode, or an adjusted mean or median. Another way is to let the value extraction work first, and then apply imputation to the extracted variables. Time series data is different enough that it has its chapter in Chapter 116.\nOne of the main downsides to simple imputation is that it can lead to impossible configurations in the data. Imagine that the total square area is missing, but we know the number of rooms and number of bedrooms. Certain combinations are more likely than others. Below is the classic ames data set\nThere can’t be more bedrooms than the total number of rooms. And we see that in the data. The average number of bedrooms is 2.85, and if we round, then it will be 3. That is perfectly fine for a house with an average number of rooms, but it will be impossible for small houses and quite inadequate for large houses. This is bad but can be seen as an improvement to the situation where the model didn’t fit because a missing value was present. This scenario is part of the motivation for Model Based Imputation.\nOther drawbacks of simple include; reducing the variance and standard deviation of the data. This happens because we are adding zero-variance information to the variables. In the same vein, we are changing the distribution of our variables, which can also affect downstream modeling and feature engineering.\nBelow we see this in effect, as more and more missing data, leads to a larger peak of the mean of the distribution.\nAnother thing we can do, while we stay in this domain of only using a single variable to impute itself, is to impute using the original distribution. So instead of imputing by the mode, a sample is drawn from the non-missing values, and that value is used. This is then done for each observation. Each observation won’t get the same values, but it will preserve the distribution, variance and standard deviation. It won’t help with the relationship between variables and as an added downside, it adds noise into imputation, making it a seeded feature engineering method.\nOn the implementation side, we need to be careful about how we extract the original distribution. This distribution needs to be saved for later reapply. Imagine a numeric predictor, if it is a non-integer, it will likely take many unique values, if not all unique values. We might want to bin the data to create a distribution that way to the same memory.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Simple Imputation</span>"
    ]
  },
  {
    "objectID": "missing-simple.html#pros-and-cons",
    "href": "missing-simple.html#pros-and-cons",
    "title": "43  Simple Imputation",
    "section": "43.2 Pros and Cons",
    "text": "43.2 Pros and Cons\n\n43.2.1 Pros\n\nFast computationally\nEasy to explain what was done\n\n\n\n43.2.2 Cons\n\nDoesn’t preserve relationship between predictors\nreducing the variance and standard deviation of the data\nunlikely to help performance",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Simple Imputation</span>"
    ]
  },
  {
    "objectID": "missing-simple.html#r-examples",
    "href": "missing-simple.html#r-examples",
    "title": "43  Simple Imputation",
    "section": "43.3 R Examples",
    "text": "43.3 R Examples\nThe recipes package contains several steps. It includes the steps step_impute_mean(), step_impute_median() and step_impute_mode() which imputes by the mean, median and mode respectively.\n\n\n\n\n\n\nTODO\n\n\n\nfind a good data set with missing values\n\n\n\nlibrary(recipes)\nlibrary(modeldata)\n\nimpute_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_impute_mean(contains(\"Area\")) |&gt;\n  step_impute_median(contains(\"_SF\")) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  prep()\n\nWe can use the tidy() function to find the estimated mean\n\nimpute_rec |&gt;\n  tidy(1)\n\n# A tibble: 5 × 3\n  terms         value id               \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;            \n1 Lot_Area     10148  impute_mean_CnWw4\n2 Mas_Vnr_Area   101. impute_mean_CnWw4\n3 Gr_Liv_Area   1500  impute_mean_CnWw4\n4 Garage_Area    473. impute_mean_CnWw4\n5 Pool_Area        2  impute_mean_CnWw4\n\n\nestimated median\n\nimpute_rec |&gt;\n  tidy(2)\n\n# A tibble: 8 × 3\n  terms         value id                 \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;              \n1 BsmtFin_SF_1     3  impute_median_UoO9a\n2 BsmtFin_SF_2     0  impute_median_UoO9a\n3 Bsmt_Unf_SF    466. impute_median_UoO9a\n4 Total_Bsmt_SF  990  impute_median_UoO9a\n5 First_Flr_SF  1084  impute_median_UoO9a\n6 Second_Flr_SF    0  impute_median_UoO9a\n7 Wood_Deck_SF     0  impute_median_UoO9a\n8 Open_Porch_SF   27  impute_median_UoO9a\n\n\nand estimated mode\n\nimpute_rec |&gt;#| \n  tidy(3)\n\n# A tibble: 40 × 3\n   terms        value                               id               \n   &lt;chr&gt;        &lt;chr&gt;                               &lt;chr&gt;            \n 1 MS_SubClass  One_Story_1946_and_Newer_All_Styles impute_mode_I3pVC\n 2 MS_Zoning    Residential_Low_Density             impute_mode_I3pVC\n 3 Street       Pave                                impute_mode_I3pVC\n 4 Alley        No_Alley_Access                     impute_mode_I3pVC\n 5 Lot_Shape    Regular                             impute_mode_I3pVC\n 6 Land_Contour Lvl                                 impute_mode_I3pVC\n 7 Utilities    AllPub                              impute_mode_I3pVC\n 8 Lot_Config   Inside                              impute_mode_I3pVC\n 9 Land_Slope   Gtl                                 impute_mode_I3pVC\n10 Neighborhood North_Ames                          impute_mode_I3pVC\n# ℹ 30 more rows\n\n\n\n\n\n\n\n\nTODO\n\n\n\nwait for the distribution step",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Simple Imputation</span>"
    ]
  },
  {
    "objectID": "missing-simple.html#python-examples",
    "href": "missing-simple.html#python-examples",
    "title": "43  Simple Imputation",
    "section": "43.4 Python Examples",
    "text": "43.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the SimpleImputer() method we can use. The main argument we will use is strategy which we can set to determine the type of imputing.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\nct = ColumnTransformer(\n    [('mean_impute', SimpleImputer(strategy='mean'), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('mean_impute', SimpleImputer(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('mean_impute', SimpleImputer(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) mean_impute['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  SimpleImputer?Documentation for SimpleImputerSimpleImputer() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      mean_impute__Sale_Price  ...  remainder__Latitude\n0                    215000.0  ...               42.054\n1                    105000.0  ...               42.053\n2                    172000.0  ...               42.053\n3                    244000.0  ...               42.051\n4                    189900.0  ...               42.061\n...                       ...  ...                  ...\n2925                 142500.0  ...               41.989\n2926                 131000.0  ...               41.988\n2927                 132000.0  ...               41.987\n2928                 170000.0  ...               41.991\n2929                 188000.0  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nSetting strategy='median' switches the imputer to do median imputing.\n\nct = ColumnTransformer(\n    [('median_impute', SimpleImputer(strategy='median'), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('median_impute',\n                                 SimpleImputer(strategy='median'),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('median_impute',\n                                 SimpleImputer(strategy='median'),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) median_impute['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  SimpleImputer?Documentation for SimpleImputerSimpleImputer(strategy='median') remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      median_impute__Sale_Price  ...  remainder__Latitude\n0                      215000.0  ...               42.054\n1                      105000.0  ...               42.053\n2                      172000.0  ...               42.053\n3                      244000.0  ...               42.051\n4                      189900.0  ...               42.061\n...                         ...  ...                  ...\n2925                   142500.0  ...               41.989\n2926                   131000.0  ...               41.988\n2927                   132000.0  ...               41.987\n2928                   170000.0  ...               41.991\n2929                   188000.0  ...               41.989\n\n[2930 rows x 74 columns]",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>43</span>  <span class='chapter-title'>Simple Imputation</span>"
    ]
  },
  {
    "objectID": "missing-model.html",
    "href": "missing-model.html",
    "title": "44  Model Based Imputation",
    "section": "",
    "text": "44.1 Model Based Imputation\nIn model-based imputation, this is where we get the remaining types of imputation that we can use. It is quite a big and broad topic. This chapter will try to do it justice.\nWe start with simpler methods. Remember, this chapter specifically refers to methods where more than one variable is being used for the imputation. So we could do grouped versions of the simple imputation methods seen in Simple Imputation chapter. Instead of imputing with the mean, you impute with the mean within a given group as defined by another categorical variable.\nYou could also fit a linear regression model with the target variable as the variable you intend to impute, and other complete variables as predictors.\nThis idea will extend into most other types of models. K-nearest neighbors and trees are common models for this task. For these models, you need to make sure that the predictors can be used. So they will need to not have any missing values themselves. You could in theory use a series of models you impute variables with missing data, which then will be used as predictors to predict another variable.\nMethods such as Multivariate Imputation by Chained Equations(Buuren 2012) also fall into this category of imputation as well.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Model Based Imputation</span>"
    ]
  },
  {
    "objectID": "missing-model.html#pros-and-cons",
    "href": "missing-model.html#pros-and-cons",
    "title": "44  Model Based Imputation",
    "section": "44.2 Pros and Cons",
    "text": "44.2 Pros and Cons\n\n44.2.1 Pros\n\nLikely get better performance than simple imputation\n\n\n\n44.2.2 Cons\n\nMore complex model\nlower interpretability",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Model Based Imputation</span>"
    ]
  },
  {
    "objectID": "missing-model.html#r-examples",
    "href": "missing-model.html#r-examples",
    "title": "44  Model Based Imputation",
    "section": "44.3 R Examples",
    "text": "44.3 R Examples\nThere are a number of steps in the recipes package that fall under this category. Within that, we have step_impute_bag(), step_impute_knn(), and step_impute_linear().\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nBelow we are showing how we can impute using a K-nearest neighbor model using step_impute_knn(). We specify the variable to impute on first, and then with impute_with we specify which variables are used as predictors in the model.\n\nlibrary(recipes)\n\nimpute_knn_rec &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_impute_knn(disp, neighbors = 1, impute_with = imp_vars(vs, am, hp, drat))\n\nimpute_knn_rec |&gt;\n  prep() |&gt;\n  juice()\n\n# A tibble: 32 × 11\n     cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb   mpg\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     6  160    110  3.9   2.62  16.5     0     1     4     4  21  \n 2     6  160    110  3.9   2.88  17.0     0     1     4     4  21  \n 3     4  108     93  3.85  2.32  18.6     1     1     4     1  22.8\n 4     6  258    110  3.08  3.22  19.4     1     0     3     1  21.4\n 5     8  360    175  3.15  3.44  17.0     0     0     3     2  18.7\n 6     6  225    105  2.76  3.46  20.2     1     0     3     1  18.1\n 7     8  360    245  3.21  3.57  15.8     0     0     3     4  14.3\n 8     4  147.    62  3.69  3.19  20       1     0     4     2  24.4\n 9     4  141.    95  3.92  3.15  22.9     1     0     4     2  22.8\n10     6  168.   123  3.92  3.44  18.3     1     0     4     4  19.2\n# ℹ 22 more rows",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Model Based Imputation</span>"
    ]
  },
  {
    "objectID": "missing-model.html#python-examples",
    "href": "missing-model.html#python-examples",
    "title": "44  Model Based Imputation",
    "section": "44.4 Python Examples",
    "text": "44.4 Python Examples\nI’m not aware of a good way to do this for models other than KNN in a scikit-learn way. Please file an issue on github if you know of a good way.\nWe are using the ames data set for examples. {sklearn} provided the KNNImputer() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import KNNImputer\n\nct = ColumnTransformer(\n    [('na_indicator', KNNImputer(), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('na_indicator', KNNImputer(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('na_indicator', KNNImputer(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) na_indicator['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  KNNImputer?Documentation for KNNImputerKNNImputer() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      na_indicator__Sale_Price  ...  remainder__Latitude\n0                     215000.0  ...               42.054\n1                     105000.0  ...               42.053\n2                     172000.0  ...               42.053\n3                     244000.0  ...               42.051\n4                     189900.0  ...               42.061\n...                        ...  ...                  ...\n2925                  142500.0  ...               41.989\n2926                  131000.0  ...               41.988\n2927                  132000.0  ...               41.987\n2928                  170000.0  ...               41.991\n2929                  188000.0  ...               41.989\n\n[2930 rows x 74 columns]\n\n\nThe argument n_neighbors is something you might have to tune to get good performance for this type of imputing method.\n\nct = ColumnTransformer(\n    [('na_indicator', KNNImputer(n_neighbors=15), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('na_indicator', KNNImputer(n_neighbors=15),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('na_indicator', KNNImputer(n_neighbors=15),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) na_indicator['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  KNNImputer?Documentation for KNNImputerKNNImputer(n_neighbors=15) remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n      na_indicator__Sale_Price  ...  remainder__Latitude\n0                     215000.0  ...               42.054\n1                     105000.0  ...               42.053\n2                     172000.0  ...               42.053\n3                     244000.0  ...               42.051\n4                     189900.0  ...               42.061\n...                        ...  ...                  ...\n2925                  142500.0  ...               41.989\n2926                  131000.0  ...               41.988\n2927                  132000.0  ...               41.987\n2928                  170000.0  ...               41.991\n2929                  188000.0  ...               41.989\n\n[2930 rows x 74 columns]\n\n\n\n\n\n\nBuuren, S. van. 2012. Flexible Imputation of Missing Data. Chapman & Hall/CRC Interdisciplinary Statistics. CRC Press. https://books.google.com/books?id=elDNBQAAQBAJ.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>44</span>  <span class='chapter-title'>Model Based Imputation</span>"
    ]
  },
  {
    "objectID": "missing-indicator.html",
    "href": "missing-indicator.html",
    "title": "45  Missing Values Indicators",
    "section": "",
    "text": "45.1 Missing Values Indicators\nWhile imputation can be useful, as we saw in the Simple Imputation and Model Based Imputation chapters. That by itself isn’t always enough to extract all the information. As was described in Missing section, missing values can come in different variants, and depending on the variant, imputation might not give enough information. Suppose you are working with non-MCAR data (non Missing Completely At Random). Then we have some mechanism that determines when missing values occur. This mechanism might be known or unknown. From a predictive standpoint whether or not it is known doesn’t matter as much, what matters is whether the mechanism is related to the outcome or not.\nThis is where missing value indicators come in. Used in combination with imputation, missing value indicators will try to capture that signal. For each chosen variable, create another Boolean variable that is 1 when a missing value is seen, and 0 otherwise.\nThe following sample data set\nWill look like the data set below, once missing value indicators have been added.\nFrom here on, you are potentially adding information, otherwise we are adding a lot of noise. The noise here can be filtered by other methods seen in this book. If variables with no missing data were used, then we create zero variance predictors, which we can deal with as seen in Zero Variance chapter.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Missing Values Indicators</span>"
    ]
  },
  {
    "objectID": "missing-indicator.html#pros-and-cons",
    "href": "missing-indicator.html#pros-and-cons",
    "title": "45  Missing Values Indicators",
    "section": "45.2 Pros and Cons",
    "text": "45.2 Pros and Cons\n\n45.2.1 Pros\n\nNo performance harm when added to variables with no missing data\nSimple and interpretable\n\n\n\n45.2.2 Cons\n\nWill produce zero variance columns when used on data with no missing values\nCan create a sizable increase in data set size",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Missing Values Indicators</span>"
    ]
  },
  {
    "objectID": "missing-indicator.html#r-examples",
    "href": "missing-indicator.html#r-examples",
    "title": "45  Missing Values Indicators",
    "section": "45.3 R Examples",
    "text": "45.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nFrom the recipes package, can we use the step_indicate_na() function to create indicator variables based on missing data\n\nlibrary(recipes)\n\nna_ind_rec &lt;- recipe(mpg ~ disp + vs + am, data = mtcars) |&gt;\n  step_indicate_na(all_predictors()) |&gt;\n  prep()\n\n\nna_ind_rec |&gt;\n  bake(new_data = mtcars)\n\n# A tibble: 32 × 7\n    disp    vs    am   mpg na_ind_disp na_ind_vs na_ind_am\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n 1  160      0     1  21             0         0         0\n 2  160      0     1  21             0         0         0\n 3  108      1     1  22.8           0         0         0\n 4  258      1     0  21.4           0         0         0\n 5  360      0     0  18.7           0         0         0\n 6  225      1     0  18.1           0         0         0\n 7  360      0     0  14.3           0         0         0\n 8  147.     1     0  24.4           0         0         0\n 9  141.     1     0  22.8           0         0         0\n10  168.     1     0  19.2           0         0         0\n# ℹ 22 more rows",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Missing Values Indicators</span>"
    ]
  },
  {
    "objectID": "missing-indicator.html#python-examples",
    "href": "missing-indicator.html#python-examples",
    "title": "45  Missing Values Indicators",
    "section": "45.4 Python Examples",
    "text": "45.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the MissingIndicator() method we can use.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import MissingIndicator\n\nct = ColumnTransformer(\n    [('na_indicator', MissingIndicator(), ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',  'Mas_Vnr_Area'])], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('na_indicator', MissingIndicator(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('na_indicator', MissingIndicator(),\n                                 ['Sale_Price', 'Lot_Area', 'Wood_Deck_SF',\n                                  'Mas_Vnr_Area'])]) na_indicator['Sale_Price', 'Lot_Area', 'Wood_Deck_SF', 'Mas_Vnr_Area']  MissingIndicator?Documentation for MissingIndicatorMissingIndicator() remainder['MS_SubClass', 'MS_Zoning', 'Lot_Frontage', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Year_Built', 'Year_Remod_Add', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_SF_1', 'BsmtFin_Type_2', 'BsmtFin_SF_2', 'Bsmt_Unf_SF', 'Total_Bsmt_SF', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Bsmt_Full_Bath', 'Bsmt_Half_Bath', 'Full_Bath', 'Half_Bath', 'Bedroom_AbvGr', 'Kitchen_AbvGr', 'TotRms_AbvGrd', 'Functional', 'Fireplaces', 'Garage_Type', 'Garage_Finish', 'Garage_Cars', 'Garage_Area', 'Garage_Cond', 'Paved_Drive', 'Open_Porch_SF', 'Enclosed_Porch', 'Three_season_porch', 'Screen_Porch', 'Pool_Area', 'Pool_QC', 'Fence', 'Misc_Feature', 'Misc_Val', 'Mo_Sold', 'Year_Sold', 'Sale_Type', 'Sale_Condition', 'Longitude', 'Latitude'] passthroughpassthrough \n\nct.transform(ames)\n\n                   remainder__MS_SubClass  ... remainder__Latitude\n0     One_Story_1946_and_Newer_All_Styles  ...              42.054\n1     One_Story_1946_and_Newer_All_Styles  ...              42.053\n2     One_Story_1946_and_Newer_All_Styles  ...              42.053\n3     One_Story_1946_and_Newer_All_Styles  ...              42.051\n4                Two_Story_1946_and_Newer  ...              42.061\n...                                   ...  ...                 ...\n2925                  Split_or_Multilevel  ...              41.989\n2926  One_Story_1946_and_Newer_All_Styles  ...              41.988\n2927                          Split_Foyer  ...              41.987\n2928  One_Story_1946_and_Newer_All_Styles  ...              41.991\n2929             Two_Story_1946_and_Newer  ...              41.989\n\n[2930 rows x 70 columns]",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>45</span>  <span class='chapter-title'>Missing Values Indicators</span>"
    ]
  },
  {
    "objectID": "missing-remove.html",
    "href": "missing-remove.html",
    "title": "46  Remove Missing Values",
    "section": "",
    "text": "46.1 Remove Missing Values\nRemoving observations with missing values should generally be considered the last resort. This will of course depend on your data and practices, and there are cases where it is appropriate.\nNevertheless, the removal of observations should be done with care. Especially since the removal of observations means that no prediction will happen to them. This might be unacceptable, and you need to make sure proper procedure is taken to guarantee that all observations are predicted. Even if removed observations are given default values for predictions.\nThe most simple way to do missing value removal is to remove observations that experience any number of missing values. It is straightforward to do and implemented in any modeling software you use. The downside to this is that if you are seeing a little bit of missing data, across your predictors, you end up throwing away a lot of data once you have a lot of columns.\nThis can also happen with missingness that clusters in certain groups of variables. Remember to always look at how many observations you are losing by doing the removals.\nWe are removing observations for one of two reasons. The first reason is that it messes with our model’s ability to fit correctly. If this is the issue we should think about other methods in the Missing section than removal. Another reason why we want to remove observations is because they don’t have enough information in them because of their missingness.\nImagine an observation where every single field is missing. There is no information about the observation other than the fact that it is fully missing. On the other end of the spectrum, we have observations with no missing values at all. Somewhere between these two extremes is a threshold that we imagine defines “too much missing”. This is the basis behind threshold-based missing value removal we define a threshold and then remove observations once they have more than, say 50% missing values. This method by itself doesn’t deal with all the missing values and would need to be accompanied by other methods as described in the Missing section. This threshold will need to be carefully selected, as you are still removing observations.\nIn the same vein as above, we can look at removing predictors that have too many missing values. This is a less controversial option that you can explore. As for its use. It is still recommended that you do it in a thresholded fashion. So you can set up your preprocessing to remove predictors that have more than 30% or 50% missing values. Again this value will likely have to be tuned. The reason why this method isn’t as bad is that we aren’t removing observations, we are instead trying to remove low information predictors. This is not a foolproof method, as the missingness could be non-MCAR and be informative.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Remove Missing Values</span>"
    ]
  },
  {
    "objectID": "missing-remove.html#pros-and-cons",
    "href": "missing-remove.html#pros-and-cons",
    "title": "46  Remove Missing Values",
    "section": "46.2 Pros and Cons",
    "text": "46.2 Pros and Cons\n\n46.2.1 Pros\n\nWill sometimes be necessary\n\n\n\n46.2.2 Cons\n\nExtreme care has to be taken\nLoss of data",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Remove Missing Values</span>"
    ]
  },
  {
    "objectID": "missing-remove.html#r-examples",
    "href": "missing-remove.html#r-examples",
    "title": "46  Remove Missing Values",
    "section": "46.3 R Examples",
    "text": "46.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind data set\n\n\nWe can use the step_naomit() function from the recipes package to remove any observation that contains missing values.\n\nlibrary(recipes)\n\nnaomit_rec &lt;- recipe(~., data = mtcars) |&gt;\n  step_naomit(all_predictors()) |&gt;\n  prep()\n\nbake(naomit_rec, new_data = NULL)\n\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows\n\n\n\n\n\n\n\n\nTODO\n\n\n\nwait for thresholded observation removal\n\n\nThere is the step_filter_missing() function from the recipes package that removes predictors with more missing values than the specified threshold\n\nlibrary(recipes)\n\nfilter_missing_rec &lt;- recipe(~., data = mtcars) |&gt;\n  step_filter_missing(all_predictors()) |&gt;\n  prep()\n\nbake(filter_missing_rec, new_data = NULL)\n\n# A tibble: 32 × 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ℹ 22 more rows",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Remove Missing Values</span>"
    ]
  },
  {
    "objectID": "missing-remove.html#python-examples",
    "href": "missing-remove.html#python-examples",
    "title": "46  Remove Missing Values",
    "section": "46.4 Python Examples",
    "text": "46.4 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.",
    "crumbs": [
      "Missing Data",
      "<span class='chapter-number'>46</span>  <span class='chapter-title'>Remove Missing Values</span>"
    ]
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "47  Text Overview",
    "section": "",
    "text": "47.1 Text Overview\nSometimes you have fields that are free long-form text. This cannot be dealt with using the methods we saw in the Categorical secton, as the natural text will have enough variation that counting the full-text fields is not going to be fruitful. Imagine this fictitious short book reviews\nOn a character level, these are all uniquely different. But to us English speakers, the first 3 are positive, and the last 2 are negative. We will in the upcoming chapters look at all the methods and their details, to see how we can modify and extract as much information out of this as we can.\nWe will adhere to the #BenderRule1 and make it clear that while the examples you will see in this book are going to be in English, it doesn’t mean that the results you see can be replicated in other languages. All languages have unique things about them that make these types of calculations harder or easier to use.\nIn these chapters, we will be studying methods to work with text. Notably, text and language are different things, with many languages starting as spoken words, and a smaller number of these develop a writing system. This is important to remember, as the written text will almost always have less information than the language it is transcribing. You have seen examples of this when you send a text message that wasn’t received correctly, because of the missing non-verbal information.\nText data, like all other types of data, isn’t guaranteed to contain information that helps your modeling data at hand. This is especially true with shorter text fields but can be true with just about anything.\nThe types of operations you do to get text data into numeric data are various, but they can usually be split into several different tasks. You don’t have to do everything in this list to get good results, and some off-the-shelf methods will sometimes combine 2 or more of these steps into one. This is another reason why it is important to read the documentation of the implementation you are using.\nThere is also the possibility that you know exactly what type of information is important in your text field. In that case, you will be better served by manually extracting the information with regular expression or related text processing tools, we look at examples of that in the Manual Text Features chapter.\nThe coding portions of the following chapters will be focused on using text as features in addition to other features. This is unlike other modeling tasks where text is the whole input, such as in translation or text2image tasks. Despite this, they would still be a worthwhile read for practitioners working on those types of tasks.\nThe chapters in this section read quite differently than the other chapters, as many of the chapters assume that preview chapters are used. This is because some of the chapters describe part of the text handling rather than a specific method. See the diagram below for possible workflows.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#text-cleaning",
    "href": "text.html#text-cleaning",
    "title": "47  Text Overview",
    "section": "47.2 Text cleaning",
    "text": "47.2 Text cleaning\nIn the Text Cleaning chapter, we will look over the ways we take raw text and get it ready for later tasks. This work deals with encoding issues, standardization, and cases and sometimes you need to get rid of a lot of unwanted chunks.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#tokenization",
    "href": "text.html#tokenization",
    "title": "47  Text Overview",
    "section": "47.3 Tokenization",
    "text": "47.3 Tokenization\nOnce the text is cleaned, we need to split it into a smaller unit of information such that we can count it, this is called tokenization and we will be explored in detail.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#modifying-tokens",
    "href": "text.html#modifying-tokens",
    "title": "47  Text Overview",
    "section": "47.4 Modifying tokens",
    "text": "47.4 Modifying tokens\nOnce you have the data as tokens, one of the things you might want to do is modify them in various ways. This could be things like changing the endings to words or changing the words entirely. We see examples of this in the Stemming and N-grams chapters.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#filtering-tokens",
    "href": "text.html#filtering-tokens",
    "title": "47  Text Overview",
    "section": "47.5 Filtering tokens",
    "text": "47.5 Filtering tokens\nThe tokens you create might not all be of the same quality. Depending on your choice of tokenizer, there will be reasons for you to remove some of the tokens you have created. We see examples of this in the Stop words and Token filter chapters.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#counting-tokens",
    "href": "text.html#counting-tokens",
    "title": "47  Text Overview",
    "section": "47.6 Counting tokens",
    "text": "47.6 Counting tokens\nWe have gotten to the end of the line and we are ready to turn the tokens into numeric variables we can use. There are many different ways we look at them in the TF, TF-IDF, Token Hashing, Sequence Encoding, and LDA chapters.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#embeddings",
    "href": "text.html#embeddings",
    "title": "47  Text Overview",
    "section": "47.7 Embeddings",
    "text": "47.7 Embeddings\nAnother way to use text is to work with embeddings, this is another powerful tool that can give you good performance. We look at some of them in the word2vec and BERT chapters.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text.html#footnotes",
    "href": "text.html#footnotes",
    "title": "47  Text Overview",
    "section": "",
    "text": "https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/↩︎",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>47</span>  <span class='chapter-title'>Text Overview</span>"
    ]
  },
  {
    "objectID": "text-manual.html",
    "href": "text-manual.html",
    "title": "48  Manual Text Features",
    "section": "",
    "text": "48.1 Manual Text Features\nWhen talking about manual text features, we are talking about hand-crafted metrics or counts based on the text. you will be able to find some off-the-shelf features that fit into this category. But generally, this is where you can use your domain knowledge to extract useful information.\nTypical of-the-shelf counts are generally counted. So it will be counts of words, sentences, linebreaks, commas, hashtags, emojis, and punctuation. A lot of these will be proxies for text length so some kind of normalization will be useful here. Normalizing in this setting is typically done by dividing by the text length, which then gives different interpretations as we are no longer looking at the “number of words”, and now finding “the inverse of average word length”.\nThe above features are easy to calculate and will therefore not be hard to include in your model. But this is where creativity and domain knowledge shine!\nOne thing you might need to do when working with these hand-crafted features is knowledge about working with regular expressions.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Manual Text Features</span>"
    ]
  },
  {
    "objectID": "text-manual.html#pros-and-cons",
    "href": "text-manual.html#pros-and-cons",
    "title": "48  Manual Text Features",
    "section": "48.2 Pros and Cons",
    "text": "48.2 Pros and Cons\n\n48.2.1 Pros\n\nClear and actionable features\nHigh interpretability\n\n\n\n48.2.2 Cons\n\nCan be time-consuming to create\nComputational speed depends on the feature\nWill likely need to",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Manual Text Features</span>"
    ]
  },
  {
    "objectID": "text-manual.html#r-examples",
    "href": "text-manual.html#r-examples",
    "title": "48  Manual Text Features",
    "section": "48.3 R Examples",
    "text": "48.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nThe textfeatures package is one package in R that contains a bunch of general features that may or may not be useful.\n\nlibrary(textfeatures)\nlibrary(modeldata)\n\ntextfeatures(modeldata::tate_text$medium, word_dims = 0, \n             verbose = FALSE) |&gt;\n  dplyr::glimpse()\n\nRows: 4,284\nColumns: 34\n$ n_urls           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_uq_urls        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_hashtags       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_uq_hashtags    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_mentions       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_uq_mentions    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_chars          &lt;dbl&gt; 1.39951429, -0.91391731, -0.91391731, -0.91391731, -0…\n$ n_uq_chars       &lt;dbl&gt; 1.3463720, -0.2656168, -0.2656168, -0.2656168, -0.585…\n$ n_commas         &lt;dbl&gt; 1.2867430, -0.6470182, -0.6470182, -0.6470182, -0.647…\n$ n_digits         &lt;dbl&gt; -0.2800874, -0.2800874, -0.2800874, -0.2800874, -0.28…\n$ n_exclaims       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_extraspaces    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_lowers         &lt;dbl&gt; 1.348546506, -0.912127069, -0.912127069, -0.912127069…\n$ n_lowersp        &lt;dbl&gt; -1.0518721, 0.1014681, 0.1014681, 0.1014681, 0.352584…\n$ n_periods        &lt;dbl&gt; -0.04324894, -0.04324894, -0.04324894, -0.04324894, -…\n$ n_words          &lt;dbl&gt; 1.3593949, -0.7937823, -0.7937823, -0.7937823, -0.162…\n$ n_uq_words       &lt;dbl&gt; 1.4230658, -0.7920930, -0.7920930, -0.7920930, -0.142…\n$ n_caps           &lt;dbl&gt; -0.04050572, -0.04050572, -0.04050572, -0.04050572, -…\n$ n_nonasciis      &lt;dbl&gt; -0.02646899, -0.02646899, -0.02646899, -0.02646899, -…\n$ n_puncts         &lt;dbl&gt; 5.6233563, -0.2031327, -0.2031327, -0.2031327, -0.203…\n$ n_capsp          &lt;dbl&gt; -1.2508524, 0.8890397, 0.8890397, 0.8890397, 0.538919…\n$ n_charsperword   &lt;dbl&gt; 1.09976675, -0.87544061, -0.87544061, -0.87544061, -1…\n$ sent_afinn       &lt;dbl&gt; 0.01511448, 0.01511448, 0.01511448, 0.01511448, 0.015…\n$ sent_bing        &lt;dbl&gt; -0.07864915, -0.07864915, -0.07864915, -0.07864915, -…\n$ sent_syuzhet     &lt;dbl&gt; -0.1334035, -0.1334035, -0.1334035, -0.1334035, -0.13…\n$ sent_vader       &lt;dbl&gt; -0.06711618, -0.06711618, -0.06711618, -0.06711618, -…\n$ n_polite         &lt;dbl&gt; 0.05597655, 0.05597655, 0.05597655, 0.05597655, 0.055…\n$ n_first_person   &lt;dbl&gt; -0.01527831, -0.01527831, -0.01527831, -0.01527831, -…\n$ n_first_personp  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_second_person  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_second_personp &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_third_person   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_tobe           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ n_prepositions   &lt;dbl&gt; -2.3324219, 0.3482094, 0.3482094, 0.3482094, 0.348209…\n\n\n\n\n\n\n\n\nTODO\n\n\n\nCome up with domain-specific examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Manual Text Features</span>"
    ]
  },
  {
    "objectID": "text-manual.html#python-examples",
    "href": "text-manual.html#python-examples",
    "title": "48  Manual Text Features",
    "section": "48.4 Python Examples",
    "text": "48.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>48</span>  <span class='chapter-title'>Manual Text Features</span>"
    ]
  },
  {
    "objectID": "text-cleaning.html",
    "href": "text-cleaning.html",
    "title": "49  Text Cleaning",
    "section": "",
    "text": "49.1 Text Cleaning\nWhen working with unstructured data such as free-form text. You likely will need to do some cleaning. As with basically every method in this book, not all the steps in this chapter should be applied, and it will be up to you the practitioner to evaluate what is best for your data. It is also important that you formalize what these steps should be such that you can apply them consistently to new data that comes in.\nI’ll split this chapter into 2 major categories. It doesn’t mean that they are the only ones, but it helps to keep things clear in our minds. These two categories are removing things and transforming things.\nRemoving things should be quite self-explanatory. This group of actions can be quite lengthy depending on how clean your data is when you get it. HTML tags, Markdown, and other artifacts are prime candidates for removal as they have less information about the words written and more about how they are displaced. You can also get malformed text for different reasons, with the most common being speech-to-text and optical character recognition (OCR). If you get this kind of malformed text, you should try your best to have that part of the process improved, to hopefully avoid the malformed text altogether, but the escape-hatch would be to remove it. Sometimes people will remove all non-alphanumeric values from the text. This is a quick and crude method of getting something decent and easy to work with. This is also popular because it can be done using a small regular expression. The crudeness because a problem as it removes indiscriminately such that periods, question marks, emojis, and non-Latin characters are removed.\nThe crudeness of the last approach is what inspires some of the transformation actions we can take. If we are going to count different parts of the text for analysis, then we might benefit from some bucketing. We could replace all emojis with EMOJI to denote that an emoji was present. The same could be done with emails, hashtags, user names, dates, datetimes, and URLs. Doing this will rob us of the information about which emojis were used, but it lets us count how many times and where they are used instead.\nAnother case of bucketing is done by lower-casing all the text. By making all the characters lowercase we lose some information. In English, we lose proper nouns and the starts of sentences and in German, we lose all nouns. We do this when we work under the assumption that Today and today isn’t a meaningful difference to our analysis.\nOne thing that can happen when we lowercase is that some works we don’t want to combine are combined, it and IT make a good example. replacing IT with information technology is one way to solve this issue. As you can see, there are a lot of different ways to improve our data quality, and spending some time at this part of the process tends to give better results as it affects every part of the rest of the pipeline.\nThe choices you make on the actions above will depend on where the text is supposed to end up. If we want to use the data as part of a supervised machine learning model then we will likely be counting tokens, and keeping periods will not be helpful. if we want text generation, we need the periods as it will try to replicate text.\nWhen working with text data you will inevitably run into encodings and their associated irregularities.\nWhat this means in practice is that you can run into issues by reading data in as the wrong encoding, or if multiple pieces of text read the same, but constructed by different characters. You are less and less likely to run into encoding confusion as more and more text is encoded with UTF-8. These issues appear as glitched text, if you see that you can use one of the many encoding detection methods online to try to narrow down how to read the text correctly.\nThe other issue is much harder to detect unless you know what you are looking for. Consider the two strings \"schön\" and \"schön\", are they the same? the reasonable reader would say yes. But the computer disagrees, if you look at the Unicode codes you see that they are different \\u0073\\u0063\\u0068\\u00f6\\u006e and \\u0073\\u0063\\u0068\\u006f\\u0308\\u006e. The difference comes in the o with the umlaut. In the first string, 0f6 is a “latin small letter o with diaeresis” where 06f is a “latin small letter o”m and 308 is a “combining diaeresis”. This means the first string uses one symbol to represent \"ö\" and the second string uses two symbols to represent it, but combining a \"0\" with an umlaut symbol. And they appear similar to the naked eye.\nThis is an issue because the computer considers these strings as different strings, so for counting operations they are counted differently. Luckily there are solutions to this problem, and the method is known as unicode normalization. The ways these methods work could be a chapter by itself and will be excluded. But the methods by themselves are easy to find and use on your data. Unless you are sure that you won’t have these issues, or you are on a big performance crush, it is recommended to use Unicode normalization at all times.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Text Cleaning</span>"
    ]
  },
  {
    "objectID": "text-cleaning.html#pros-and-cons",
    "href": "text-cleaning.html#pros-and-cons",
    "title": "49  Text Cleaning",
    "section": "49.2 Pros and Cons",
    "text": "49.2 Pros and Cons\n\n49.2.1 Pros\n\nWhen applied correctly, can lead to boosts in insights into the data\n\n\n\n49.2.2 Cons\n\nCan be a quite manual process which will likely be domain specific",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Text Cleaning</span>"
    ]
  },
  {
    "objectID": "text-cleaning.html#r-examples",
    "href": "text-cleaning.html#r-examples",
    "title": "49  Text Cleaning",
    "section": "49.3 R Examples",
    "text": "49.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nFind a data set that isn’t clean\n\n\nWe will use the step_text_normalization() function from the {textrecipes} package to perform unicode normalization, which defaults to the NFC normalization form.\n\nlibrary(textrecipes)\n\nLoading required package: recipes\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nsample_data &lt;- tibble(text = c(\"sch\\U00f6n\", \"scho\\U0308n\"))\n\nsample_data |&gt;\n  count(text)\n\n# A tibble: 2 × 2\n  text      n\n  &lt;chr&gt; &lt;int&gt;\n1 schön     1\n2 schön     1\n\nrec &lt;- recipe(~., data = sample_data) |&gt;\n  step_text_normalization(text) |&gt;\n  prep()\n\nrec |&gt;\n  bake(new_data = NULL) |&gt;\n  count(text)\n\n# A tibble: 1 × 2\n  text      n\n  &lt;fct&gt; &lt;int&gt;\n1 schön     2",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Text Cleaning</span>"
    ]
  },
  {
    "objectID": "text-cleaning.html#python-examples",
    "href": "text-cleaning.html#python-examples",
    "title": "49  Text Cleaning",
    "section": "49.4 Python Examples",
    "text": "49.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>49</span>  <span class='chapter-title'>Text Cleaning</span>"
    ]
  },
  {
    "objectID": "text-tokenization.html",
    "href": "text-tokenization.html",
    "title": "50  Tokenization",
    "section": "",
    "text": "50.1 Tokenization\nThe main issue with text, when working with machines is that they don’t have a way to natively deal with it in a way that makes sense. Machines like numbers, and text are far from that. tokenization is the act of turning long strings of text into many smaller units of text (called tokens) which can then be worked with. The method that performs the tokenization is called a tokenizer.\nBefore we turn to the technical task of defining the right size of units, I to make it clear that tokenization is always used when working with text data in machine learning and AI tasks. It might be hidden, but it is always there. This book tries to make a clear distinction between the preprocessing techniques that are applied to text. However it is not uncommon for multiple of these methods to be combined into one function call, where text has non-ascii characters removed, text lower-cased and then tokenized. You just have to hope that the documentation is thorough enough.\nThe first and arguably simplest type of tokenization is a character tokenizer. This tokenization is done by splitting the text into the smallest unit possible, which in most cases will be letters. This is likely not the best way. For counting procedures knowing the distributions of letters of a text is in most cases not enough to help us extract insights. The main upside to this tokenizer is that it produces the smallest number of unique tokens for any of the tokenizers we have seen. \"Don't Blame Me\" becomes \"D\", \"o\", \"n\", \"'\", \"t\", \" \", \"B\", \"l\", \"a\", \"m\", \"e\", \" \", \"M\", \"e\" under this type of tokenization.\nThe next tokenizer is more practical but is harder to define. This is the word tokenizer. The idea behind it is solid. Words appear to be a good small unit to break a longer text into. The problem is that we don’t have a good definition of what a word is. In English, the definition “anything that is separated by spaces” isn’t that bad. This is the first type of word tokenizer you will find. with this you have that \"Don't Blame Me\" becomes \"Don't\", \"Blame\", \"Me\".\nThere are more advanced methods of tokenizing into words. One of them is done by finding word boundaries according to specifications made by International Components for Unicode (ICU). This method is much more advanced and will likely give you better word tokens than what you would get by using the space tokenizer. At least for English text. The method is going to be slower than the amount of calculations that are being done, but the speed hit is likely to be outweighed by the performance increase.\nBoth space tokenizers and word tokenizers are useful, they are performant and interpretable. If you want to dial up on performance, then this next tokenizer is for you. It is called the Byte-Pair Encoding tokenizer, or BPE tokenizer for short. It is also the first tokenizer that we have to train to the text corpus. The mechanics behind it borrows from compression. Imagine that we want to store some text as a sequence of tokens.\neach token is going to be assigned a unique integer. Then we take a look at the whole corpus and find the most frequent pair of adjacent integers. In this case, it is 2 4 and we merge them into a new token er and give them a new integer value 11\nand this process is repeated, many times, creating longer and longer tokens.\nthis process will go on until there are no more duplicate pairs of tokens, which is far too long. Typically the vocabulary size is fixed beforehand, in the orders of 1000s. The good thing about this tokenizer is that it is more tailored toward the data you are working with, hence words like \"credit\" and \"load\" will self-populate very fast in banking text, and other words won’t come until later in the vocabulary. It is in essence less dependent on language since it tries to find frequent sequences of characters. For some language tasks, you will use this tokenizer without removing spaces and punctuation, since the tokenizer then will be able to “learn” the beginning and ends of words. One of the downsides is that the tokens themselves are harder to reason about after the fact. The token \"teach\" might be found by itself or as part of \"teacher\" and the later models likely won’t know, unless both \"teach\" and \"teacher\" is in the vocabulary.\nWe have talked a lot about Latin alphabet tokenization. As with any study of data, it is important to work with domain experts. Tokenization of languages such as Chinese, Japanese and Korean, where each glyph contains much more information, is almost a different problem than the tokenization we have seen so far, since each “word” a lot of times is contained in 1 to 3 glyphs. Many different CJK tokenizers exist to best tackle the language construction specific to each language. Many of the techniques stem from the methods we have discussed above.\nAbove all else, it is important to make sure that you know that the tokenization choice is one we have to make, and to make sure it matches the other things we are doing. If you look up a model on a site like Hugging Face, you will see that for each model, a specific tokenizer is explicitly stated. This is done because we want to make sure future use of the model uses the same tokenizer. If you were to want to change the tokenizer, you would need to retrain the model from scratch using the new tokenizer.\nIf you were to look at some of these tokenizers you will see options not discussed here in this chapter. This is partly because it is hard to keep up, and because many of the options are very specific to the text cleaning tasks. Some cleaning tasks involve having specific tokens to represent things like unknown tokens, the beginning of the sequence, and the end of the sequence. The choice doesn’t really matter, as not as it doesn’t overlap with your vocabulary and the data is cleaned in such a way that they are used correctly.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "text-tokenization.html#pros-and-cons",
    "href": "text-tokenization.html#pros-and-cons",
    "title": "50  Tokenization",
    "section": "50.2 Pros and Cons",
    "text": "50.2 Pros and Cons\n\n50.2.1 Pros\n\nMany established methods are implemented\nYou rarely have to think about performance in terms of speed\n\n\n\n50.2.2 Cons\n\nThere is often not a best answer, and experimentation will be needed\nsomething a handcrafted tokenizer will go better than an off-the-shelf version",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "text-tokenization.html#r-examples",
    "href": "text-tokenization.html#r-examples",
    "title": "50  Tokenization",
    "section": "50.3 R Examples",
    "text": "50.3 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind text data set to use",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "text-tokenization.html#python-examples",
    "href": "text-tokenization.html#python-examples",
    "title": "50  Tokenization",
    "section": "50.4 Python Examples",
    "text": "50.4 Python Examples\n\n\n\n\n\n\nTODO\n\n\n\nfigure out the best approach to teach in Python. Should we stay in scikit-learn? move somewhere else? how should this be taught since it is likely not separate as it is in {textrecipes}",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>50</span>  <span class='chapter-title'>Tokenization</span>"
    ]
  },
  {
    "objectID": "text-stemming.html",
    "href": "text-stemming.html",
    "title": "51  Stemming",
    "section": "",
    "text": "51.1 Stemming\nWhen we tokenize text we will end up with a lot of different types of tokens. More importantly, we will end up with different tokens, that feel very similar. Take the tokens \"house\", \"houses\", and \"housing\". Whether or not those tokens are similar or not depends on your task. If you think these tokens are the same, then you should consider applying stemming.\nIn its simplest form, stemming is the task of modifying tokens to collapse dissimilar tokens. Often this is seen done, by modifying the ends of words. In practice, this is done using rule-based algorithms or using regular expressions.\nThe simplest English stemmer that has some useful applications is the S-Stemmer. This stemmer works by removing ending s’s on tokens. Especially turning plural versions of words into their non-plural version. \"houses\" to \"house\", \"Happiness\" to \"Happine\" and \"is\" to \"i\". We don’t mind that the results aren’t real words. What is important is that we are reducing the total number of tokens, in a hopefully useful way. If we took Jane Austen’s 6 published novels, tokenized them and applied the S-Stemmer, we would go front around 14520 to 12860 tokens, representing a drop of 11.4%.\nWe see above that some failures do appear with stemmers. Such as the \"is\" to \"i\" example. And it is a choice for us as the practitioner to determine if the non-informative collapsing is worth it. A simple script can be created to showcase all the collapsed tokens and whether we can tolerate them or not.\nA well-used stemmer is the Porter Stemmer (Porter 1980). Porter himself released the implementation in the Snowball framework. The stemmer itself has implementations in almost any language you would need. The The English (Porter2) stemming algorithm is fully described on the website, as it is a little more advanced than the S-Stemmer, and there is little advantage of copying it into these pages. The last update to the algorithm happened in October 2023, as of the time of writing.\nLemmatization is a similar method to stemming. But with a few differences. The main similarity is that both methods aim to find a shorter representation of the token. Stemming does this by working on each token in isolation, using a set of rules to carry out an algorithm. Lemmatization on the other hand is linguistics-based, operating on the word in its context. Using Part of Speech and word meaning, you can find the lemma of words we are looking for. So a good implementation of a lemmatization would change the word to \"say\" from \"said\" and to \"come\" from \"came\". These methods are orders of magnitude slower than stemming but can be useful when time isn’t a limiting factor as the results tend to be better. One of the better software implementations that do lemmatization is spacy (Honnibal et al. 2020).",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Stemming</span>"
    ]
  },
  {
    "objectID": "text-stemming.html#pros-and-cons",
    "href": "text-stemming.html#pros-and-cons",
    "title": "51  Stemming",
    "section": "51.2 Pros and Cons",
    "text": "51.2 Pros and Cons\n\n51.2.1 Pros\n\nCan be a fast and easy way to reduce the number of tokens\n\n\n\n51.2.2 Cons\n\nDoes require thorough inspection to avoid wrongful collapse",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Stemming</span>"
    ]
  },
  {
    "objectID": "text-stemming.html#r-examples",
    "href": "text-stemming.html#r-examples",
    "title": "51  Stemming",
    "section": "51.3 R Examples",
    "text": "51.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Stemming</span>"
    ]
  },
  {
    "objectID": "text-stemming.html#python-examples",
    "href": "text-stemming.html#python-examples",
    "title": "51  Stemming",
    "section": "51.4 Python Examples",
    "text": "51.4 Python Examples\n\n\n\n\nHonnibal, Matthew, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. “spaCy: Industrial-strength Natural Language Processing in Python.” https://doi.org/10.5281/zenodo.1212303.\n\n\nPorter, Martin F. 1980. “An Algorithm for Suffix Stripping.” Program 14 (3): 130–37. https://doi.org/10.1108/eb046814.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>51</span>  <span class='chapter-title'>Stemming</span>"
    ]
  },
  {
    "objectID": "text-ngrams.html",
    "href": "text-ngrams.html",
    "title": "52  N-grams",
    "section": "",
    "text": "52.1 N-grams\nSo far we have thought of tokens as a single unit. This is a useful technique for a lot of modeling workflows. It is a simple way to represent the text in a way that we can turn into numbers later down the line. The main shortcoming of this approach is that it doesn’t capture any order or positioning of the tokens at all.\nN-grams are one way of dealing with this shortcoming. Consider the sequence of tokens \"we\", \"are\", \"all\", \"learning\", \"today\". If we were to count the tokens, it would not be any different than the sequence \"today\", \"all\", \"we\", \"are\", \"learning\". The idea being n-grams is to look at sequences of tokens as tokens themselves. So by considering the first set of tokens, and finding 2-grams (also called bi-grams) we get the bi-grams\"we are\", \"are all\", \"all learning\", \"learning today\". These bi-grams are different than the bigrams we would find with the second set of tokens.\nWe can let n be any integer value, and often we want to collect multiple at the same time. So we could collect all n-grams for n=1,2,3 and get \"we\", \"are\", \"all\", \"learning\", \"today\", \"we are\", \"are all\", \"all learning\", \"learning today\", \"we are all\", \"we are learning\" \"all learning today\". Generating these n-grams leads us to our first observation. Generating n-grams of higher values of n, leads to many more unique tokens, each with lower counts. The word \"truck\" might appear in the text a lot of times, and even \"fire truck\" might appear a lot of times. But the n-gram \"he ran after the fire truck\" is quite likely to be a singleton in the data set.\nEach value of n provides many hopefully useful tokens, with a drop-off at one point as the count of the n-grams drops off. The challenge for the practitioner is to find the right number of n-grams to collect. m unigrams give rise to m-1 bigrams, m-2 trigrams and so on. Many of the tokens will not be useful, so we have a balance between performance and speed.\nThere are also cases where n-grams don’t give us enough information to be worth doing. After all, it is not a free operation. This chapter talks about n-grams as a process that happens independently from tokenization. And in theory that should be possible. However many software implementations connect the tokenization action with the n-gramming procedure. If your tool allows you to create n-grams independently, you can create n-grams based on stemmed tokens.\nN-grams work by looking at neighboring tokens. skip-grams on the other hand work by looking at nearby tokens. For the set of tokens \"we\", \"are\", \"all\", \"learning\", \"today\", the skip-grams generated from \"are\" are \"are we\", \"are all\", \"are learning\", \"are today\". We have a number to determine how far away from the target word we look. Skip-grams produce a lot of unique tokens but are not as focused on the ordering as we saw in the n-gram case. And we see that there are cases where one is more useful than the other.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>N-grams</span>"
    ]
  },
  {
    "objectID": "text-ngrams.html#pros-and-cons",
    "href": "text-ngrams.html#pros-and-cons",
    "title": "52  N-grams",
    "section": "52.2 Pros and Cons",
    "text": "52.2 Pros and Cons\n\n52.2.1 Pros\n\nCan uncover patterns in the data not otherwise seen\n\n\n\n52.2.2 Cons\n\nCan lead to longer computation times",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>N-grams</span>"
    ]
  },
  {
    "objectID": "text-ngrams.html#r-examples",
    "href": "text-ngrams.html#r-examples",
    "title": "52  N-grams",
    "section": "52.3 R Examples",
    "text": "52.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>N-grams</span>"
    ]
  },
  {
    "objectID": "text-ngrams.html#python-examples",
    "href": "text-ngrams.html#python-examples",
    "title": "52  N-grams",
    "section": "52.4 Python Examples",
    "text": "52.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>52</span>  <span class='chapter-title'>N-grams</span>"
    ]
  },
  {
    "objectID": "text-stopwords.html",
    "href": "text-stopwords.html",
    "title": "53  Stop words",
    "section": "",
    "text": "53.1 Stop words\nThe term stop words is a very loaded term, that is often under-explained and therefore misunderstood by practitioners. The original definition is a set of words that contains no information and are safe to remove. This is too simplistic of a view. Instead, we can think of words as having an amount of signal in them related to the task we are trying to accomplish. Removing low-signal words can be fruitful, if we are able to identify them. The term was first coined in 1960 (Luhn 1960).\nUnder this definition, we need to find a way to quantify how important each word is. One can think about the scopes as global, domain, and document. Global stop words are words that are almost almost low information words. These are the words like \"and\" and \"or\". They are highly unlikely to provide a signal to the modeling task at hand, especially when using a counting method. This list is inherently going to be quite small. The domain-level stop words are more interesting but can be harder to precisely define. Suppose you are looking at hotel listings. The words \"bed\" and \"room\" are likely stop words as they don’t provide much value to any talk that you would imagine. Hotel listings are all but guaranteed to be talking about the style of bed, and the room they are offering. The word \"beach\" would generally be a word of interest when talking about hotel listings as a whole, but could be a stop word when we are scoped to beach hotel listings. There is a lot of grey area and domain knowledge happening in this type of stop words. Lastly, we have document-level stop words. These are words in a document that don’t contain information, and they are really hard to figure out effectively because it is determined on a document-by-document basis. These stop words technically exist but will likely not be picked up on the methods we will be using.\nThe first application of stop words that people encounter is premade stop word lists. The list of stop word lists is very long and we won’t bother looking over all of them. But some of the well-known and popular ones are: SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System, an information retrieval system developed at Cornell University in the 1960s (Lewis et al. 2004) and the English Snowball stop word list (Porter 2001).\nThe SMART stop word list contains 571 words, and the Snowball list contains length(stopwords::data_stopwords_smart$en). The first 25 stop words alphabetically from the SMART list are shown here\nwith the first 25 words from the Snowball list shown here.\nWe notice that the SMART list contains a lot more words, some of which may feel on the verge of importance in certain cases. And there may be a reason for that. The Snowball list is meticulously constructed by looking at words by their classes as seen here. On the other hand, is it not known how the SMART list was constructed. But there are hints, we can look at the words that are included as well as those which aren’t included. By digging around for a little bit, we notice that the word \"he's\" is in the list but \"she's\" isn’t. One explanation for this is that this list is partly frequency-based. And it wouldn’t be surprising to see, especially with some of the other words in the list like \"wish\" and \"thanx\". But there is some evidence of this list being manually curated as well, as all the letters of the alphabet are in there, which is quite unlikely to all have been frequent enough in the original corpus.\nAll of this is to say that it is important to thoroughly investigate the stop word list you are using, as they can have unexpected results. Nothman, Qin, and Yurchak (2018) explores a selection of 52 stop word lists with alarming results. Among some of the more grave issues were misspellings (“fify” instead of “fifty”), the inclusion of clearly informative words such as “computer” and “cry” and internal inconsistencies, such as including the word “has” but not the word “does”. Some of these mistakes have crept into popular libraries without notice for several years.\nWe can create homemade lists in a few different ways. One of which is to take the training data set, count the tokens and sort the results. Then we can go through them and manually select which ones to put on the list. You don’t have to pick the top 50, you can leave some in and some out, as some frequent words could be informative. We will talk about TF-IDF, but that can also be helpful in finding noninformative words. Calculating the IDF of all the words and sorting them lets us find tokens with low IDF that are good candidates for removal.\nThe final advice is to combine the two approaches you see above. Pick a premade list, and make modifications to it, based on your domain expertise and the data set you are working with.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Stop words</span>"
    ]
  },
  {
    "objectID": "text-stopwords.html#pros-and-cons",
    "href": "text-stopwords.html#pros-and-cons",
    "title": "53  Stop words",
    "section": "53.2 Pros and Cons",
    "text": "53.2 Pros and Cons\n\n53.2.1 Pros\n\nis a way to cut down on computation times and improve performance\n\n\n\n53.2.2 Cons\n\ncan be quite time-consuming to get right\nusing off the shelf and default options are often not ideal",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Stop words</span>"
    ]
  },
  {
    "objectID": "text-stopwords.html#r-examples",
    "href": "text-stopwords.html#r-examples",
    "title": "53  Stop words",
    "section": "53.3 R Examples",
    "text": "53.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Stop words</span>"
    ]
  },
  {
    "objectID": "text-stopwords.html#python-examples",
    "href": "text-stopwords.html#python-examples",
    "title": "53  Stop words",
    "section": "53.4 Python Examples",
    "text": "53.4 Python Examples\n\n\n\n\nLewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004. “RCV1: A New Benchmark Collection for Text Categorization Research.” Journal of Machine Learning Research 5: 361–97. https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf.\n\n\nLuhn, H. P. 1960. “Key Word-in-Context Index for Technical Literature (Kwic Index).” American Documentation 11 (4): 288–95. https://doi.org/https://doi.org/10.1002/asi.5090110403.\n\n\nNothman, Joel, Hanmin Qin, and Roman Yurchak. 2018. “Stop Word Lists in Free Open-Source Software Packages.” In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), edited by Eunjeong L. Park, Masato Hagiwara, Dmitrijs Milajevs, and Liling Tan, 7–12. Melbourne, Australia: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-2502.\n\n\nPorter, Martin F. 2001. “Snowball: A Language for Stemming Algorithms.” https://snowballstem.org.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>53</span>  <span class='chapter-title'>Stop words</span>"
    ]
  },
  {
    "objectID": "text-filter.html",
    "href": "text-filter.html",
    "title": "54  Token Filter",
    "section": "",
    "text": "54.1 Token Filter\nThis chapter is going to be quite similar to the stop word removal chapter. The difference is going to be how we go about doing things. When removing stop words we are hoping to remove words that don’t contain enough information. Depending on our model, we might want to further reduce the number of tokens we are considering. For counting methods such as TF and TF-IDF, each unique token leads to 1 column in the output. Depending on your data set and model type, you can end up with tens of thousands of different tokens, which can be too much.\nWe can do this filtering in 2 ways, thresholded and counted. In the thresholded, we say “Keep all tokens that satisfy this condition”, and in the counted way we say “Keep the n tokens with the highest value of x”. Each has a pro and a con. Thresholded methods are precisely defined, but don’t return a fixed number of tokens. Counted methods are imprecisely defined, because of potential ties, but return a fixed number of tokens. Both of these concepts can be used for the criterias explained below.\nThe frequency of a token in the training data set is going to be an important factor as to whether we should keep it or not. Some tokens will only appear less than a handful of times, they are likely not to appear enough times to provide a signal to a model. On the same side, you could also filter away the most common words. This is very similar to what some people do as a proxy for stop word removal. This is less likely to be a success, especially if you have already done stop word removal.\nOn the same coin, we could use IDF values to determine the cut-off. We will talk about TF-IDF, but that can also be helpful in finding noninformative words. Calculating the IDF of all the words and sorting them lets us find tokens with low IDF that are good candidates for removal.\nRemoval of tokens is rarely done to improve performance but is done as a tradeoff between speed and accuracy. Removing too many tokens will yield faster models, but risk removing tokens with signal, removing too few tokens yields slower models, that may or may not contain noise.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Token Filter</span>"
    ]
  },
  {
    "objectID": "text-filter.html#pros-and-cons",
    "href": "text-filter.html#pros-and-cons",
    "title": "54  Token Filter",
    "section": "54.2 Pros and Cons",
    "text": "54.2 Pros and Cons\n\n54.2.1 Pros\n\ndone as a tradeoff between speed\n\n\n\n54.2.2 Cons\n\nHas to be done with care to avoid removing signal",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Token Filter</span>"
    ]
  },
  {
    "objectID": "text-filter.html#r-examples",
    "href": "text-filter.html#r-examples",
    "title": "54  Token Filter",
    "section": "54.3 R Examples",
    "text": "54.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Token Filter</span>"
    ]
  },
  {
    "objectID": "text-filter.html#python-examples",
    "href": "text-filter.html#python-examples",
    "title": "54  Token Filter",
    "section": "54.4 Python Examples",
    "text": "54.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>54</span>  <span class='chapter-title'>Token Filter</span>"
    ]
  },
  {
    "objectID": "text-tf.html",
    "href": "text-tf.html",
    "title": "55  Term Frequency",
    "section": "",
    "text": "55.1 Term Frequency\nThe main reason why we do these text feature engineering steps is to produce numeric variables. Once we have a selection of tokens we are happy with it is time to turn them into these numeric variables, and term frequency (TF) is the first and simplest method we will showcase to accomplish this.\nIn essence, it works by taking the tokens we have generated, and counting how many times they appear for each observation. If there are 500 unique tokens, then 500 columns are generated. If there are 50,000 unique tokens, then 50,000 columns are created.\nThe traditional way is to count how many times each token appears. But it is not the only way. We could also create an indicator of whether the token was there or not. This is another way of encoding x &gt; 0. We can use this if we think that the number of times a token appears doesn’t matter that much, just that it appears. If you have a mix of long and short text fields, you might be interested in calculating the frequency instead of the raw counts. Here we take the number of times a token appears, divided by the number of tokens in the observation. This is similar but not exactly the same as Range Scaling, as we are making it so the values across the row sum to 1, not making it so each column has values between 0 and 1. Since we are working with counts, there are also times when log normalization is done, which means that we take the columns produced here, and take the logarithm of them, Typically with an offset because this method tends to produce a lot of zeroes. Lastly, we also see people perform double normalization, which is done by taking the raw frequency divided by the raw frequency of the most occurring token in the observation. This is then multiplied by some offset and the offset is added to the result. This is again done to prevent a bias towards longer documents. The offset can be anything, with 0.5 being an option some prefer as it doesn’t have more influence than a single count.\nOne thing that is going to happen is that there will be a high degree of sparsity. Generally speaking, each document contains far fewer words, than the total amount of used words. It will be less so if you limit the number of tokens you count, but you will generally see around sparsity rates from 90% to 99.99% in practice. Another thing that happens when we represent our data in this way, is that each resulting column is going to be orthogonal to each other. If you were to add another token to a document, you would only see it being affected in 1 column. This is great for interpretability, but it isn’t exactly a space-efficient way to store the information.\nYou might run into what is known as unseen tokens. They typically won’t show up in term frequency solutions. You could, in theory, add a column with \"&lt;unseen token&gt;\", but for counting methods like this, it doesn’t make much sense as it would only take the value 0. Any such variable should properly be removed anyway as we further talk about in the Zero Variance chapter. An efficient software solution wouldn’t generate this column to begin with.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Term Frequency</span>"
    ]
  },
  {
    "objectID": "text-tf.html#pros-and-cons",
    "href": "text-tf.html#pros-and-cons",
    "title": "55  Term Frequency",
    "section": "55.2 Pros and Cons",
    "text": "55.2 Pros and Cons\n\n55.2.1 Pros\n\nEasy calculations\nIntepretable\n\n\n\n55.2.2 Cons\n\nCan have problems with long and short documents\nnot very space efficient results",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Term Frequency</span>"
    ]
  },
  {
    "objectID": "text-tf.html#r-examples",
    "href": "text-tf.html#r-examples",
    "title": "55  Term Frequency",
    "section": "55.3 R Examples",
    "text": "55.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Term Frequency</span>"
    ]
  },
  {
    "objectID": "text-tf.html#python-examples",
    "href": "text-tf.html#python-examples",
    "title": "55  Term Frequency",
    "section": "55.4 Python Examples",
    "text": "55.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>55</span>  <span class='chapter-title'>Term Frequency</span>"
    ]
  },
  {
    "objectID": "text-tfidf.html",
    "href": "text-tfidf.html",
    "title": "56  TF-IDF",
    "section": "",
    "text": "56.1 TF-IDF\nTerm Frequency inverse document frequency is the next iteration based on term frequency we explored in the TF chapter. As the name implies, it is what happens when you take the term frequencies and multiply them by the inverse document frequency.\n\\[\nTF-IDF(t, d) = TF(t, d) \\times IDF(t)\n\\]\nConceptually, we start by creating the term frequency matrix we created in TF. Then we Look at each term/token. We calculate the inverse document frequency by dividing the number of observations we have, by how many observations that token appears in. Once we have that number we take the logarithm of it. There will thus be one IDF value for each token. These values will be multiplied column-wise to the term frequency matrix.\nYou can think of IDF as a dampening value. If a given token appears in 1 out of 100 document, then the IDF of that token is r round(log(100 / 1), 2), if it is 1 out of 10 the value is r round(log(10 / 1), 2), if the token appears in 90% of the documents the IDF is r round(log(10 / 1), 2) and if the token appears in every document then the IDF is r 0. We reward tokens that appear in a few documents and punish tokens that appear in every document.\nIt is important to note that TF-IDF is a trained method. So we need to save the IDF values so we can apply them to new incoming observations.\nAbove is explained the main idea behind TF-IDF calculations. In practice, we see a couple of modifications and options one might take. Smoothing of the IDF values is commonly done as a default to avoid division by zero issues. It is done by (TODO).\nWe could also do probabilistic inverse document frequency. We calculate this by having the numerator be the number of documents minus the number of occurrences instead of just the number of documents.\nThe idea of TF-IDF was first proposed in 1972 (SPARCK JONES 1972), while it works well in practice, it wasn’t based on much theory. Efforts have been made to see how we can ground IDF calculations in theory (Robertson 2004), but it doesn’t change the uses.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>TF-IDF</span>"
    ]
  },
  {
    "objectID": "text-tfidf.html#pros-and-cons",
    "href": "text-tfidf.html#pros-and-cons",
    "title": "56  TF-IDF",
    "section": "56.2 Pros and Cons",
    "text": "56.2 Pros and Cons\n\n56.2.1 Pros\n\nCan lead to performance gains if the IDF capture relevant words\n\n\n\n56.2.2 Cons\n\nLeads to harder interpretations than term frequencies",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>TF-IDF</span>"
    ]
  },
  {
    "objectID": "text-tfidf.html#r-examples",
    "href": "text-tfidf.html#r-examples",
    "title": "56  TF-IDF",
    "section": "56.3 R Examples",
    "text": "56.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>TF-IDF</span>"
    ]
  },
  {
    "objectID": "text-tfidf.html#python-examples",
    "href": "text-tfidf.html#python-examples",
    "title": "56  TF-IDF",
    "section": "56.4 Python Examples",
    "text": "56.4 Python Examples\n\n\n\n\nRobertson, Stephen. 2004. “Understanding Inverse Document Frequency: On Theoretical Arguments for IDF.” Journal of Documentation 60 (5): 503–20.\n\n\nSPARCK JONES, K. 1972. “A STATISTICAL INTERPRETATION OF TERM SPECIFICITY AND ITS APPLICATION IN RETRIEVAL.” Journal of Documentation 28 (1): 11–21. https://doi.org/https://doi.org/10.1108/eb026526.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>56</span>  <span class='chapter-title'>TF-IDF</span>"
    ]
  },
  {
    "objectID": "text-hashing.html",
    "href": "text-hashing.html",
    "title": "57  Token Hashing",
    "section": "",
    "text": "57.1 Token Hashing\nToken hashing is quite similar to Hashing Encoding. Instead of hashing the whole text string, we instead apply the hashing function to each token. This means that Token Hashing is to term frequency as hashing encoding is to dummy encoding.\nOne mental model is that we are performing term frequency, and then combining the columns in a deterministically random way. For this, we are using a hashing function. A hashing function is a function that takes an input, and returns a integer. The same value will always return the same output. It is computationally easy to perform this transformation, but it is hard to reverse it. This is not a downside as we don’t need to perform the reverse transformation. Furthermore a good hashing function outputs values evenly across their supported range, and similar values such as cat and cats will produce vastly different hashes, 1751422759 and 2517493566 respectively for the MurmurHash3 hashing function.\nThe MurmurHash3, which is commonly used for its speed, produces values 32-but hash values, which gives us integers between 1 and 2^32 = 4294967296. Producing 4294967296 columns would not help us, so what is typically done is to round these values down to a more manageable range. Specifically rounding by a power of 2 is common since that can be archived by bit manipulation. Suppose we round such that we only keep the 6 significant digits, then we are left with 2^6 = 64 values. And the hashes for cat is now 39 and cats is 62. They are still different, but now they take up a smaller space of possible values.\nOne thing that will happen when you use these hashing functions is that different tokens hash to the same value. This is called a collision. And are technically a bad thing, as the model will be unable to distinguish between the influence of those two tokens. This is much more likely to happen than with hashing encoding since each observation will contain many different tokens, however it is not something to avoid at all costs. One of the main tenets of token hashing is that we are getting a trade-off between storage size and information.\nOne thing that is used to combat collisions is the use of a hashed sign function. Much like we are using a hashing function to generate integers, we will use a different hashing function to give us one of two values -1 and 1. This will determine the sign of each hashed word. This is done to lessen the negative effects of collisions as there is a 50% chance that a pair of strings that hash to the same values will have different signs and thus cancel each other out.\nThe main downside to this method is the lack of explainability, as the collisions make it so we can’t know for sure which level contributed to the effect of that variable. On the other hand, we get the added benefit of being able to handle unseen tokens directly. These will not be of use directly, but they are handled in the sense that we don’t have to keep track of levels, as the hashing function just does its job.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Token Hashing</span>"
    ]
  },
  {
    "objectID": "text-hashing.html#pros-and-cons",
    "href": "text-hashing.html#pros-and-cons",
    "title": "57  Token Hashing",
    "section": "57.2 Pros and Cons",
    "text": "57.2 Pros and Cons\n\n57.2.1 Pros\n\nComputationally fast\nAllows for a fixed number of output columns\ngives less sparse output than term frequencies\n\n\n\n57.2.2 Cons\n\nLoss of interpretability\nStill gives quite sparse output",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Token Hashing</span>"
    ]
  },
  {
    "objectID": "text-hashing.html#r-examples",
    "href": "text-hashing.html#r-examples",
    "title": "57  Token Hashing",
    "section": "57.3 R Examples",
    "text": "57.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Token Hashing</span>"
    ]
  },
  {
    "objectID": "text-hashing.html#python-examples",
    "href": "text-hashing.html#python-examples",
    "title": "57  Token Hashing",
    "section": "57.4 Python Examples",
    "text": "57.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>57</span>  <span class='chapter-title'>Token Hashing</span>"
    ]
  },
  {
    "objectID": "text-onehot.html",
    "href": "text-onehot.html",
    "title": "58  Sequence Encoding",
    "section": "",
    "text": "58.1 Sequence Encoding\nIn TF, TF-IDF, and Token Hashing we took a bag of words approach to encoding the tokens into numeric columns. This approach strips out the positional information of the text. For some tasks that is perfectly fine to do. One way to encode the sequence itself is presented in this chapter. Known as sequence encoding or one-hot encoding.\nEach token in the dictionary is assigned an integer value. The tokens are then replaced with their dictionary value at their location. The sentence \"he\", \"was\", \"very\", \"happy\" could be encoded as the sequence 5, 205, 361, 663.\nWith this setup in mind, we need to set some parameters as to how we do it. They can not easily be inferred by the data so we have to use our expert knowledge, and their nature makes them harder to put into a hyperparameter optimization scheme. The parameters are sequence length, truncating, and padding.\nThe sequence length refers to the number of tokens we consider in the sequence. This number needs to be constant as it dictates the number of columns that are produced. A sequence with 10 tokens can not be put into the same number of columns as a sequence with 1000 tokens without some hard choices to be made. The choice should be informed based on the distribution of the number of tokens in your observations. The longer the sequence length you select, the larger the model becomes, but picking a shorter sequence length risks losing valuable signal in the data. This can be especially hard if you have a mixture of long and short text fields.\nFor a sequence of tokens that are shorter than the selected sequence length, we need to decide how it should be placed inside the output values. Typically we wanna do the padding with zeros to indicate that no token appeared. Should the padding with zeros happen before or after? This is a question we need to answer. What it asks of us, is how to align the sequences. If we do padding after the sequence, then all the beginnings pair up.\nA similar question is what happens when the sequence is too long. Where should truncation occur? This is the same style of decision as with padding we just went over. This choice makes a big difference when some of the sequences are much longer than the sequence length. The choice of type of truncating can be the difference between keeping the beginning of a sequence or the end.\nOne of the nice things about this method is that you aren’t restricted to a certain number of unique tokens as they just influence the number of unique values the columns can take, not the number of columns. That doesn’t mean that filtering tokens shouldn’t be done, just that it isn’t a requirement as it often is for methods such as TF and TF-IDF.\nThis method of embedding tokens relies on the positional information that we have. For this reason, we should have a way to encode unseen tokens. This should be done as another integer value not used in the dictionary already.\nThis is likely to work better with neural networks such as LSTMs as they can use this representation efficiently.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Sequence Encoding</span>"
    ]
  },
  {
    "objectID": "text-onehot.html#pros-and-cons",
    "href": "text-onehot.html#pros-and-cons",
    "title": "58  Sequence Encoding",
    "section": "58.2 Pros and Cons",
    "text": "58.2 Pros and Cons\n\n58.2.1 Pros\n\nTakes order of tokens into account when modeling\n\n\n\n58.2.2 Cons\n\nA lot of decisions in the form of hyperparameters to make things work correctly\nWill require a specific type of model to get the most out of this data format",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Sequence Encoding</span>"
    ]
  },
  {
    "objectID": "text-onehot.html#r-examples",
    "href": "text-onehot.html#r-examples",
    "title": "58  Sequence Encoding",
    "section": "58.3 R Examples",
    "text": "58.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Sequence Encoding</span>"
    ]
  },
  {
    "objectID": "text-onehot.html#python-examples",
    "href": "text-onehot.html#python-examples",
    "title": "58  Sequence Encoding",
    "section": "58.4 Python Examples",
    "text": "58.4 Python Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>58</span>  <span class='chapter-title'>Sequence Encoding</span>"
    ]
  },
  {
    "objectID": "text-lda.html",
    "href": "text-lda.html",
    "title": "59  LDA",
    "section": "",
    "text": "59.1 LDA\nThe feature engineering methods we have seen so far, have been counting based or variations thereof as seen in TF, TF-IDF, and Token Hashing. Latent Dirichlet Allocation (LDA) is different because it is done by first fitting a model on the data, and then using this fitted model to define features for us.\nTo understand this method, we should first learn what Latent Dirichlet AllocationChen et al. (2016) is, and how it is used for topic modeling1.\nLatent Dirichlet Allocation is a probabilistic generative model which in simple terms assumes that each document (observation in our case) is made up of a mixture of topics, and each topic is a distribution of words (tokens). Since we have the distributions of tokens for each document, we can use the LDA method to try to find the topics. The features we pull out of this are the topic memberships for each observation.\nWe are in essence turning our tokens into n numerical features where n is the number of topics. The number of topics is not set in stone and can act as a hyperparameter.\nDocuments, which are the established terminology for the observations, are seen as a bag-of-words representation of tokens. Meaning that much like TF-IDF, the order doesn’t matter. We are simply looking at token counts.\nWords are the common term used, but we know that it is referring to tokens. Each token has a given probability for each topic, and it is assumed that when a document is generated, it samples the topics associated with it, and then samples a token from that topic. This is obviously not how writing is done, but together with the other assumptions in the model gives decent results at times.\nTopics, and latent structures over the documents. They are characterized by their distribution of words (tokens). the same word can appear in multiple topics, with the same or different weights. A document is associated with a given topic, proportional to how the document’s distribution of words aligns with the topic’s word distribution.\nThis procedure doesn’t natively have a way to determine the number of topics. But by itself, that isn’t the worst thing. Finding the “right” number of topics will depend on your modeling goal. If you are interested in having an interpretable model, then you want the topics to align with the perceived structure of the documents. This will be hard to do, as it is akin to finding the “right” number of clusters. On the other hand, if your problem is finding a very predictive model, then you will likely have an easier time as you could tune it as a hyper-parameter.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>LDA</span>"
    ]
  },
  {
    "objectID": "text-lda.html#pros-and-cons",
    "href": "text-lda.html#pros-and-cons",
    "title": "59  LDA",
    "section": "59.2 Pros and Cons",
    "text": "59.2 Pros and Cons\n\n59.2.1 Pros\n\nReduces text to few number of numerical features\n\n\n\n59.2.2 Cons\n\nMay not extract information in an interpretable way",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>LDA</span>"
    ]
  },
  {
    "objectID": "text-lda.html#r-examples",
    "href": "text-lda.html#r-examples",
    "title": "59  LDA",
    "section": "59.3 R Examples",
    "text": "59.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>LDA</span>"
    ]
  },
  {
    "objectID": "text-lda.html#python-examples",
    "href": "text-lda.html#python-examples",
    "title": "59  LDA",
    "section": "59.4 Python Examples",
    "text": "59.4 Python Examples\n\n\n\n\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent Dirichlet Allocation.” J. Mach. Learn. Res. 3 (null): 993–1022.\n\n\nChen, Jianfei, Kaiwei Li, Jun Zhu, and Wenguang Chen. 2016. “WarpLDA: A Cache Efficient o(1) Algorithm for Latent Dirichlet Allocation.” https://arxiv.org/abs/1510.08628.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>LDA</span>"
    ]
  },
  {
    "objectID": "text-lda.html#footnotes",
    "href": "text-lda.html#footnotes",
    "title": "59  LDA",
    "section": "",
    "text": "https://msaxton.github.io/topic-model-best-practices/ provides a nice overview of topic models and their best practices.↩︎",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>59</span>  <span class='chapter-title'>LDA</span>"
    ]
  },
  {
    "objectID": "text-word2vec.html",
    "href": "text-word2vec.html",
    "title": "60  word2vec",
    "section": "",
    "text": "60.1 word2vec\nword2vec (Mikolov et al. 2013) is one of the most popular ways to learn word embeddings of text. Each word, or token, will be represented by a numeric vector of values.\nWe would ideally construct these embeddings in such a way that similar words are positioned closely together in the multidimentional space. So words like “happy” and “smiling” would be similar, but “happy” and “house” wouldn’t be similar because they aren’t related.\nword2vec can be done using 2 main methods; Continuous Bag Of Words (CBOW) and Skip-Gram. The way they work mirrors each other.\ntodo: add a diagram like https://swimm.io/learn/large-language-models/what-is-word2vec-and-how-does-it-work\nWe make sure all the text is tokenized to our liking, making sure that the ordering of the tokens is preserved.\nIn the continuous bag of words model, we have a neural network that tries to predict what the word is, given the surrounding words. In essence what ends up happening is that it takes all the surrounding words, aggregates them, and uses the resulting vector for prediction.\nIn the Skip-Gram model, we try to predict the surrounding words given a target word. This is essentially the reverse talk as we saw before, and it is slightly harder, requiring more training.\nA continuous bag of words tends to work better for small data sets, with the added benefit that it is generally faster than the Skip-Gram model. Skip-gram models tend to be able to capture more semantic relationships and handle rare words better.\nWhen these models are trained, each word is given a numeric vector, typically of length 100-300 with random numbers. Then as the training progresses, these values are updated. Hopefully ending with meaningful representation at the end. The randomness helps eliminate symmetry. However, it also means that the absolute value a vector has isn’t important, but its location relative to the vectors is what matters.\nAnother thing we have to take into account is the window size. This changes the number of surrounding words we are considering. Smaller window sizes give more value to local words, whereas larger window sizes give more broader semantic meaning. This tradeoff is compounded by the fact that bigger window sizes result in longer computational time.\nWord2Vec, as with any word embedding procedure has many extensions and variants.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>word2vec</span>"
    ]
  },
  {
    "objectID": "text-word2vec.html#pros-and-cons",
    "href": "text-word2vec.html#pros-and-cons",
    "title": "60  word2vec",
    "section": "60.2 Pros and Cons",
    "text": "60.2 Pros and Cons\n\n60.2.1 Pros\n\nCan give more information than term frequency models\n\n\n\n60.2.2 Cons\n\nRequires computationally intensive training",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>word2vec</span>"
    ]
  },
  {
    "objectID": "text-word2vec.html#r-examples",
    "href": "text-word2vec.html#r-examples",
    "title": "60  word2vec",
    "section": "60.3 R Examples",
    "text": "60.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>word2vec</span>"
    ]
  },
  {
    "objectID": "text-word2vec.html#python-examples",
    "href": "text-word2vec.html#python-examples",
    "title": "60  word2vec",
    "section": "60.4 Python Examples",
    "text": "60.4 Python Examples\n\n\n\n\nAngelov, Dimo. 2020. “Top2Vec: Distributed Representations of Topics.” https://arxiv.org/abs/2008.09470.\n\n\nAsgari, Mohammad R. K., Ehsaneddin AND Mofrad. 2015. “Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics.” PLOS ONE 10 (11): 1–15. https://doi.org/10.1371/journal.pone.0141287.\n\n\nLe, Quoc V., and Tomas Mikolov. 2014. “Distributed Representations of Sentences and Documents.” https://arxiv.org/abs/1405.4053.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” https://arxiv.org/abs/1301.3781.\n\n\nNg, Patrick. 2017. “Dna2vec: Consistent Vector Representations of Variable-Length k-Mers.” https://arxiv.org/abs/1701.06279.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>60</span>  <span class='chapter-title'>word2vec</span>"
    ]
  },
  {
    "objectID": "text-bert.html",
    "href": "text-bert.html",
    "title": "61  BERT",
    "section": "",
    "text": "61.1 BERT\nBidirectional Encoder Representations from Transformers (BERT)(Devlin et al. 2019) is a different way to represent text as data than what we have seen before in this section. In the bag-of-words method, we counted the occurance of each token. This was done regardless of the order of the tokens. BERT on the other hand looks at the previous and following works, hence the name Bidirectional.\nOne of the reasons why BERT excels is that since it looks at the surrounding tokens when trying to give a token meaning, it can better distinguish between “duck” the animal and “duck” the movement. This added nuance to tokens in their context is what will give it its edge at times. This is of course much more complicated than counting tokens by themselves, but the added complexity pays off in many ways as BERT models have been used in quite a lot of places such as search, translation, etc etc.\nBERTs work by using a transformer architecture. In simple terms, a transformer is comprised of an encoder and a decoder. The encoder maps the tokens to numeric arrays, and the decoder makes the numeric arrays to predictions. The BERT model in question is just the encoder part of the transformer.\nBERTs are trained on ordered tokens. Special tokens are added to allow the model more information about the sentence structure. [SEP] stands for seperation and is used to denote that a sentence is ending, [EOS] stands for “end of sentence”. These tokens when added doing preprocessing allow us to introduce structure to the data, that the model is able to pick up on.\nOnce the data is properly structured, we can fit the model. It is a Masked Language Model (MLM), meaning that doing the training loop a token is masked, and the model is trying to predict the masked token. Remember that this is done using an encoder and a decoder. Where the decoder tries to predict the masked token. We only need the encoder which maps tokens into numeric arrays.\nWhen we apply a BERT model, we are actually looking up the tokens in the embedding. These embeddings can be created from scratch, but it is much more common to use pre-trained embeddings, and fine-tuning them as needed. Fine-tuning the model involves more training using our specific data, to update the weights in the model.\nBERT as a model is very useful, and a number of spin-off models have been added. (Lan et al. 2020) and DistilBERT(Sanh et al. 2020) are a smaller version that has comparable speed, with a smaller computational cost. RoBERTa(Liu et al. 2019) is trained using more data and other data structures allowing for more capabilities.\nSince we are able to finetune a BERT to a specific data set. People have also been doing that to release domain and language-specific models:\nWith all of this in mind, there are a couple of downsides. The first is the increased computational cost. This is noticed twice, once for the initial training of the corpus and again for the application. You can alliviate this but use smaller. Which is in line with the general advice to start small and only add complixity as needed.\nThe second problem you can run into is that BERT has a maximum token limit. One way to deal with this is to chunk up text that is over the limit into smaller manageable sizes, process them separately, and combine results.\nLastly, it is known that the performance of any given BERT depends very much on the data it was trained on. This means that a general BERT model is unlikely to work well on a domain-specific problem without finetuning.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>BERT</span>"
    ]
  },
  {
    "objectID": "text-bert.html#pros-and-cons",
    "href": "text-bert.html#pros-and-cons",
    "title": "61  BERT",
    "section": "61.2 Pros and Cons",
    "text": "61.2 Pros and Cons\n\n61.2.1 Pros\n\nPerformance increases\nMore nuanced token information that takes surrounding context into consideration\n\n\n\n61.2.2 Cons\n\nComputationally expensive\nHas token input limits\nWill need to be finetuned for domain-specific tasks",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>BERT</span>"
    ]
  },
  {
    "objectID": "text-bert.html#r-examples",
    "href": "text-bert.html#r-examples",
    "title": "61  BERT",
    "section": "61.3 R Examples",
    "text": "61.3 R Examples",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>BERT</span>"
    ]
  },
  {
    "objectID": "text-bert.html#python-examples",
    "href": "text-bert.html#python-examples",
    "title": "61  BERT",
    "section": "61.4 Python Examples",
    "text": "61.4 Python Examples\n\n\n\n\nAraci, Dogu. 2019. “FinBERT: Financial Sentiment Analysis with Pre-Trained Language Models.” https://arxiv.org/abs/1908.10063.\n\n\nBeltagy, Iz, Kyle Lo, and Arman Cohan. 2019. “SciBERT: A Pretrained Language Model for Scientific Text.” https://arxiv.org/abs/1903.10676.\n\n\nCañete, José, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang, and Jorge Pérez. 2023. “Spanish Pre-Trained BERT Model and Evaluation Data.” https://arxiv.org/abs/2308.02976.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), edited by Jill Burstein, Christy Doran, and Thamar Solorio, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.\n\n\nHuang, Kexin, Jaan Altosaar, and Rajesh Ranganath. 2020. “ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission.” https://arxiv.org/abs/1904.05342.\n\n\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations.” https://arxiv.org/abs/1909.11942.\n\n\nLee, Jieh-Sheng, and Jieh Hsiang. 2019. “PatentBERT: Patent Classification with Fine-Tuning a Pre-Trained BERT Model.” https://arxiv.org/abs/1906.02124.\n\n\nLee, Jinhyuk, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. “BioBERT: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining.” Edited by Jonathan Wren. Bioinformatics 36 (4): 1234–40. https://doi.org/10.1093/bioinformatics/btz682.\n\n\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “RoBERTa: A Robustly Optimized BERT Pretraining Approach.” https://arxiv.org/abs/1907.11692.\n\n\nSanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. “DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter.” https://arxiv.org/abs/1910.01108.",
    "crumbs": [
      "Text Features",
      "<span class='chapter-number'>61</span>  <span class='chapter-title'>BERT</span>"
    ]
  },
  {
    "objectID": "periodic.html",
    "href": "periodic.html",
    "title": "62  Periodic Overview",
    "section": "",
    "text": "62.1 Periodic Overview\nWhen working with numeric variables, you see many different kinds of relationships. Positive, negative, non-linear. But a special type of relationship is the Periodic type. It is in essence a non-linear relationship, but specifically it relies on the assumption that the beginning of the domain behaves the same way as the end. Another assumption in this type of data is that the domain of values is restricted on the left and right sides.\nTypical examples of this are type-based, such as time of day, day of the week, day of the month, and day of the year. If we have an effect, we would imagine that the end and beginning are similar. Another example is directions, 1° of a circle is very close to 359° in reality. The goal of the chapters in this section is to use transformations to make sure they are close numerically in your model.\nThere are 2 main ways we can handle this. Harmonic calculations using trigonometric functions will be showcased in Chapter 63, Essentially what happens is that we are mapping the features to the unit circle in 2 dimensions. Another intriguing type of method is using periodic indicators such as splines or other methods. It doesn’t have to be splines, but if you carefully set the range of the indicators, you can get good performance. Splines are covered in Chapter 64, and the other more general methods are covered in Chapter 65.",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>62</span>  <span class='chapter-title'>Periodic Overview</span>"
    ]
  },
  {
    "objectID": "periodic-trig.html",
    "href": "periodic-trig.html",
    "title": "63  Trigonometric",
    "section": "",
    "text": "63.1 Trigonometric\nOne of the most common ways of dealing with periodic or cyclical data is by applying the trigonometric functions sin() and cos() to these variables. This is not a trained method.\nYou will note that many occurrences of periodic data are datetime variables in nature. Please read the Datetime Circular chapter, for the additional considerations to take into account when working with datetime variables specifically.\nThe terminology we will be using in this chapter when talking about sine curves is period and offset. A sine curve normally has a period of \\(2\\pi\\). A period of \\(2\\pi\\) is unlikely to be useful, we can set this value to be an arbitrarily different value by dividing by both that value and by \\(2\\pi\\).\n\\[\n\\sin\\left(\\dfrac{x}{\\text{period} \\cdot 2 \\pi}\\right)\n\\]\nThis sine curve has a period of period. The second thing we might wanna do is to slide this curve one way or another. The offset handles that by sliding the sine curve that many units back or forwards.\n\\[\n\\sin\\left(\\dfrac{x - \\text{offset}}{\\text{period} \\cdot 2 \\pi}\\right)\n\\]\nI think that this is easier to see with some toy visualizations. The following toy data set contains a highly significant periodic effect.\nIf we were to take the sin() and cos() of the predictor, using appropriate period and offset we can overlay them on the previous chart\nWe notice that the sin() transformation somehow captures the periodic nature of the data. It obviously can’t fully capture the effect since the relationship isn’t a perfect sine curve.\nThe effect might be more noticeable if we change the way we visualize this effect.\nIn this simple case, the predictor_sin_1 becomes an ideal predictor. On the other hand, predictor_cos_1 doesn’t do as well since it doesn’t allow for a clean separation between high and low target values. All the high values appear when predictor_cos_1 is close to 0. However, not all observations where predictor_cos_1 is close to 0 have high target values.\nAbove we got lucky that we picked the period and offset. if you are offeven by a bit you can get uninformative transformations like the one below.\nThe offset is also important. We got lucky that it gave 1 good predictor and one bad predictor. Even if the period is good, but the offset is off you end up with two decent predictors. These two predictors are still good enough, but they are less ideal than the first example we got.\nOne of the issues with this approach is that each variable contains double information. E.i. you need both variables to detect “summer”.\nOne of the downsides to taking both the sin() and cos() of a predictor, is that it is unlikely that both of them capture the effect well. You might get one good and one bad, or both decent. The good thing about calling both sin() and cos() is that you don’t have to worry about the offset too much. As long as you get the period right you get a signal in at least one of the curves.\nIf you spend some manual time finding the right period and offset, then you only need to calculate sin(). For a sufficiently periodic effect and proper placement, a single sin() is going to be more effective than both sin() and cos(). This is especially true since cos() is a shift of sin() by definition.\nPropose now we have multiple periodic signals. They could be shifted versions of each other, think Halloween, Christmas, and Valentine’s day. Or they could have different periods. This would be hard to handle using a single set of curves, and we thus have to create one for each periodic pattern. This is also where the decision to only do sin() instead of both helps, as we are trying to limit redundant or ineffective predictors from being created.\nThese two newly created predictors now hit the peaks of each of the periods.\nA glaring issue right now is that all signals are not perfect sine curves. This method explained in this chapter will thus work less well for sharp spikes and asymmetric signals. The more advanced methods such as periodic splines and periodic indicators handle these types of data better.",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Trigonometric</span>"
    ]
  },
  {
    "objectID": "periodic-trig.html#pros-and-cons",
    "href": "periodic-trig.html#pros-and-cons",
    "title": "63  Trigonometric",
    "section": "63.2 Pros and Cons",
    "text": "63.2 Pros and Cons\n\n63.2.1 Pros\n\nEasy and fast to calculate\nCalculating both sin() and cos() is fail-safe if you have the right period\n\n\n\n63.2.2 Cons\n\nDoesn’t fit all periodic data shapes\nSensitive to should of period",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Trigonometric</span>"
    ]
  },
  {
    "objectID": "periodic-trig.html#r-examples",
    "href": "periodic-trig.html#r-examples",
    "title": "63  Trigonometric",
    "section": "63.3 R Examples",
    "text": "63.3 R Examples\nWe will be using the animalshelter data set for this.\n\nlibrary(recipes)\nlibrary(animalshelter)\n\nlongbeach |&gt;\n  select(outcome_type, intake_date)\n\n# A tibble: 29,787 × 2\n   outcome_type intake_date\n   &lt;chr&gt;        &lt;date&gt;     \n 1 euthanasia   2023-02-20 \n 2 rescue       2023-10-03 \n 3 euthanasia   2020-01-01 \n 4 transfer     2020-02-02 \n 5 rescue       2018-12-18 \n 6 adoption     2024-10-18 \n 7 euthanasia   2020-07-25 \n 8 rescue       2019-06-12 \n 9 rescue       2017-09-21 \n10 rescue       2024-12-15 \n# ℹ 29,777 more rows\n\n\nThe step_harmonic() will calculate the\n\ntrig_rec &lt;- recipe(outcome_type ~ intake_date, data = longbeach) |&gt;\n  step_harmonic(intake_date, \n                cycle_size = 365, \n                frequency = 1,\n                starting_val = 0,\n                keep_original_cols = TRUE)\ntrig_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 29,787 × 4\n   intake_date outcome_type intake_date_sin_1 intake_date_cos_1\n   &lt;date&gt;      &lt;fct&gt;                    &lt;dbl&gt;             &lt;dbl&gt;\n 1 2023-02-20  euthanasia              0.884             0.467 \n 2 2023-10-03  rescue                 -0.970             0.243 \n 3 2020-01-01  euthanasia              0.205             0.979 \n 4 2020-02-02  transfer                0.687             0.727 \n 5 2018-12-18  rescue                 -0.0344            0.999 \n 6 2024-10-18  adoption               -0.867             0.498 \n 7 2020-07-25  euthanasia             -0.574            -0.819 \n 8 2019-06-12  rescue                  0.146            -0.989 \n 9 2017-09-21  rescue                 -1.00              0.0215\n10 2024-12-15  rescue                 -0.0516            0.999 \n# ℹ 29,777 more rows",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Trigonometric</span>"
    ]
  },
  {
    "objectID": "periodic-trig.html#python-examples",
    "href": "periodic-trig.html#python-examples",
    "title": "63  Trigonometric",
    "section": "63.4 Python Examples",
    "text": "63.4 Python Examples\nWe are using the shelter_cats data set for examples. Since there isn’t a built-in transformer for this transformation, we can create our own using FunctionTransformer() and numpy.sin() and numpy.cos().\n\nfrom feazdata import shelter_cats\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nimport numpy as np\nimport pandas as pd\n\n# transforming `intake_date` to integer.\n# Should properly be done in sin_transformer() and cos_transformer()\nshelter_cats[\"intake_date\"] = pd.to_datetime(shelter_cats[\"intake_date\"])\nshelter_cats[\"intake_date\"] = shelter_cats[\"intake_date\"].astype(int)\n\ndef sin_transformer(period, offset):\n    return FunctionTransformer(lambda x: np.sin((x - offset) / period * 2 * np.pi))\n\ndef cos_transformer(period, offset):\n    return FunctionTransformer(lambda x: np.cos((x - offset) / period * 2 * np.pi))\n\nct = ColumnTransformer(\n    transformers=[\n        (\"intake_sin\", sin_transformer(365, 0), [\"intake_date\"]),\n        (\"intake_cos\", cos_transformer(365, 0), [\"intake_date\"]),\n    ],\n    remainder=\"passthrough\"\n)\n\nct.fit(shelter_cats)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('intake_sin',\n                                 FunctionTransformer(func=&lt;function sin_transformer.&lt;locals&gt;.&lt;lambda&gt; at 0x34c428360&gt;),\n                                 ['intake_date']),\n                                ('intake_cos',\n                                 FunctionTransformer(func=&lt;function cos_transformer.&lt;locals&gt;.&lt;lambda&gt; at 0x34c429620&gt;),\n                                 ['intake_date'])])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('intake_sin',\n                                 FunctionTransformer(func=&lt;function sin_transformer.&lt;locals&gt;.&lt;lambda&gt; at 0x34c428360&gt;),\n                                 ['intake_date']),\n                                ('intake_cos',\n                                 FunctionTransformer(func=&lt;function cos_transformer.&lt;locals&gt;.&lt;lambda&gt; at 0x34c429620&gt;),\n                                 ['intake_date'])]) intake_sin['intake_date']  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(func=&lt;function sin_transformer.&lt;locals&gt;.&lt;lambda&gt; at 0x34c428360&gt;) intake_cos['intake_date']  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(func=&lt;function cos_transformer.&lt;locals&gt;.&lt;lambda&gt; at 0x34c429620&gt;) remainder['animal_id', 'animal_name', 'animal_type', 'primary_color', 'secondary_color', 'sex', 'dob', 'intake_condition', 'intake_type', 'intake_subtype', 'reason_for_intake', 'outcome_date', 'crossing', 'jurisdiction', 'outcome_type', 'outcome_subtype', 'latitude', 'longitude', 'intake_is_dead', 'outcome_is_dead', 'was_outcome_alive', 'geopoint'] passthroughpassthrough \n\nct.transform(shelter_cats).filter(regex=(\"^intake_(sin|cos).*\"))\n\n       intake_cos__intake_date  intake_cos__intake_date\n0                        0.515                    0.857\n1                       -0.713                   -0.701\n2                        0.722                    0.692\n3                        0.944                    0.331\n4                       -0.918                    0.397\n...                        ...                      ...\n13809                   -0.543                   -0.840\n13810                   -0.905                   -0.424\n13811                   -0.068                   -0.998\n13812                   -0.068                   -0.998\n13813                   -0.630                   -0.777\n\n[13814 rows x 2 columns]",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>63</span>  <span class='chapter-title'>Trigonometric</span>"
    ]
  },
  {
    "objectID": "periodic-splines.html",
    "href": "periodic-splines.html",
    "title": "64  Periodic Splines",
    "section": "",
    "text": "64.1 Periodic Splines\nThis chapter expands the idea we saw in the last chapter with the use of splines. These splines allow for different shapes of activation than we saw with trigonomic functions.\nWe will be using the same toy data set and see if we can improve on it.\nThis data has a very specific shape, and we will see if we can approcimate it with our splines.\nFirst we fit a number of spline terms to our data using default arguments.\nWhile it produces some fine splines they are neither well fitting or periodic. Let us make spline periodic and try to approcimate the period.\nWe already see that something good is happening, The width of each bump is related to the number of degrees of freedom we have, lowering this value creates more wider bumps.\nNow we got some pretty good traction. Pulling out the well performing spline term, we can translate it a bit to show how well it overlaps with our signal.\nThere are obviously some signals that can’t be captured using splines. Compared to sine curves they are much more flexible, with a number of different kinds, each with some room for customization. Any purely periodic signal can be captured in the next chapter.",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Periodic Splines</span>"
    ]
  },
  {
    "objectID": "periodic-splines.html#pros-and-cons",
    "href": "periodic-splines.html#pros-and-cons",
    "title": "64  Periodic Splines",
    "section": "64.2 Pros and Cons",
    "text": "64.2 Pros and Cons\n\n64.2.1 Pros\n\nMore flexible than sine curves\nFairly interpretable\n\n\n\n64.2.2 Cons\n\nRequires that you know the period\nWill create some unnecessary features\nCan’t capture all types of signal",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Periodic Splines</span>"
    ]
  },
  {
    "objectID": "periodic-splines.html#r-examples",
    "href": "periodic-splines.html#r-examples",
    "title": "64  Periodic Splines",
    "section": "64.3 R Examples",
    "text": "64.3 R Examples\nWe will be using the animalshelter data set for this.\n\nlibrary(recipes)\nlibrary(animalshelter)\n\nlongbeach |&gt;\n  select(outcome_type, intake_date)\n\n# A tibble: 29,787 × 2\n   outcome_type intake_date\n   &lt;chr&gt;        &lt;date&gt;     \n 1 euthanasia   2023-02-20 \n 2 rescue       2023-10-03 \n 3 euthanasia   2020-01-01 \n 4 transfer     2020-02-02 \n 5 rescue       2018-12-18 \n 6 adoption     2024-10-18 \n 7 euthanasia   2020-07-25 \n 8 rescue       2019-06-12 \n 9 rescue       2017-09-21 \n10 rescue       2024-12-15 \n# ℹ 29,777 more rows\n\n\nThere are two steps in the recipes package that support periodic splines. Those are step_spline_b() and step_spline_nonnegative(), used for B-splines and Non-negative splines (also called M-Splines) respectively.\nThese functions have 2 main arguments controlling the spline itself, and 2 main arguments controlling its periodic behavior.\ndeg_free and degree controls the spline, changing the number of spline terms that are created, and the degrees of the piecewise polynomials respectively. The defaults for these functions tend to be a good starting point. To make these steps periodic, we need to set periodic = TRUE in options. Lastly, we can control the period and its shift with Boundary.knots in options. I find the easiest way to set this like this: c(0, period) + shift.\n\nspline_rec &lt;- recipe(outcome_type ~ intake_date, data = longbeach) |&gt;\n  step_mutate(intake_date = as.integer(intake_date)) |&gt;\n  step_spline_b(\n    intake_date,\n    options = list(periodic = TRUE, Boundary.knots = c(0, 365) + 50)\n  )\n\nspline_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 29,787 × 11\n   outcome_type intake_date_01 intake_date_02 intake_date_03 intake_date_04\n   &lt;fct&gt;                 &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n 1 euthanasia            0             0               0             0     \n 2 rescue                0             0               0             0     \n 3 euthanasia            0             0               0             0     \n 4 transfer              0             0               0             0     \n 5 rescue                0             0               0             0     \n 6 adoption              0             0               0             0     \n 7 euthanasia            0             0.0154          0.437         0.512 \n 8 rescue                0.176         0.682           0.142         0     \n 9 rescue                0             0               0             0.0131\n10 rescue                0             0               0             0     \n# ℹ 29,777 more rows\n# ℹ 6 more variables: intake_date_05 &lt;dbl&gt;, intake_date_06 &lt;dbl&gt;,\n#   intake_date_07 &lt;dbl&gt;, intake_date_08 &lt;dbl&gt;, intake_date_09 &lt;dbl&gt;,\n#   intake_date_10 &lt;dbl&gt;\n\n\n\n\n\n\n\n\nTODO\n\n\n\nFind dataset where the predictor is naturally numeric.",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Periodic Splines</span>"
    ]
  },
  {
    "objectID": "periodic-splines.html#python-examples",
    "href": "periodic-splines.html#python-examples",
    "title": "64  Periodic Splines",
    "section": "64.4 Python Examples",
    "text": "64.4 Python Examples",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>64</span>  <span class='chapter-title'>Periodic Splines</span>"
    ]
  },
  {
    "objectID": "periodic-indicators.html",
    "href": "periodic-indicators.html",
    "title": "65  Periodic Indicators",
    "section": "",
    "text": "65.1 Periodic Indicators\nThere will be periodic signals that can’t be neatly modeled using sine curves or splines. Our last chance to deal with this type of data is by writing custom indicator functions and applying them.\nThis chapter is very similar in style to the Advanced Datetime Features chapter. With this chapter being the simpler case we don’t have to deal with the added complexity that comes with datetime variables.\nAbove we see a type of activation we might see in some data.\nCreating these features is mostly done by handcrafting. Making them periodic is done using a modulo operator, using the period.",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Periodic Indicators</span>"
    ]
  },
  {
    "objectID": "periodic-indicators.html#pros-and-cons",
    "href": "periodic-indicators.html#pros-and-cons",
    "title": "65  Periodic Indicators",
    "section": "65.2 Pros and Cons",
    "text": "65.2 Pros and Cons\n\n65.2.1 Pros\n\nCan capture most a periodic signal\n\n\n\n65.2.2 Cons\n\nRequires hand-crafted features",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Periodic Indicators</span>"
    ]
  },
  {
    "objectID": "periodic-indicators.html#r-examples",
    "href": "periodic-indicators.html#r-examples",
    "title": "65  Periodic Indicators",
    "section": "65.3 R Examples",
    "text": "65.3 R Examples",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Periodic Indicators</span>"
    ]
  },
  {
    "objectID": "periodic-indicators.html#python-examples",
    "href": "periodic-indicators.html#python-examples",
    "title": "65  Periodic Indicators",
    "section": "65.4 Python Examples",
    "text": "65.4 Python Examples",
    "crumbs": [
      "Periodic Features",
      "<span class='chapter-number'>65</span>  <span class='chapter-title'>Periodic Indicators</span>"
    ]
  },
  {
    "objectID": "too-many.html",
    "href": "too-many.html",
    "title": "66  Too Many Overview",
    "section": "",
    "text": "66.1 Too Many Overview\nThere will be times were you are given a lot of variables in your data set. This by itself may not be a problem. but it could come with some drawbacks. Many models will have slower fit times at best, and worse performance at worst. Not all your variables are likely beneficial in your model. They could be uninformative, correlated or contain redundant information. We look at ways to deal with correlated features, but there will also be methods here to accomplish similar goals.\nSuppose the same variable is included twice in your model. Both will not be able to be used in your model at the same time. Once one is included, the other becomes irrelevant. In essence, these two variables are completely correlated, thus we need to deal with this type of problem as well.\nThe overarching names for these types of methods are dimensionality reduction and feature selection, and we will cover most of these methods here.",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Too Many Overview</span>"
    ]
  },
  {
    "objectID": "too-many.html#non-zero-variance-filtering",
    "href": "too-many.html#non-zero-variance-filtering",
    "title": "66  Too Many Overview",
    "section": "66.2 Non-zero Variance filtering",
    "text": "66.2 Non-zero Variance filtering\nThese types of methods are quite simple, we remove variables that take a few number of values. If the value is always 1 then it doesn’t have any information in it and we should remove it. If the variables are almost always the same we might want to remove them. We look at these methods in the Non-zero Variance filtering chapter.\n\n\n\n\n\n\nTODO\n\n\n\nFigure out where to best write about de-duplication",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Too Many Overview</span>"
    ]
  },
  {
    "objectID": "too-many.html#dimensionality-reduction",
    "href": "too-many.html#dimensionality-reduction",
    "title": "66  Too Many Overview",
    "section": "66.3 Dimensionality reduction",
    "text": "66.3 Dimensionality reduction\nThe bulk of the chapter will be in this category. This book categorizes dimensionality reduction methods as methods where a calculation is done on several features, with the same or fewer features being returned. Remember that we only look at methods that can be used in predictive settings, hence we won’t be talking about t-distributed stochastic neighbor embedding t-SNE.\n\nPrincipal Component Analysis (PCA)\nPCA variants\nIndependent Component Analysis (ICA)\nNon-negative matrix factorization (NMF)\nLinear discriminant analysis (LDA)\nGeneralized discriminant analysis (GDA)\nAutoencoders\nUniform Manifold Approximation and Projection (UMAP)\nISOMAP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Too Many Overview</span>"
    ]
  },
  {
    "objectID": "too-many.html#feature-selection",
    "href": "too-many.html#feature-selection",
    "title": "66  Too Many Overview",
    "section": "66.4 Feature selection",
    "text": "66.4 Feature selection\nFeature selection on the other hand finds which variables to keep or remove. And then you act on that. This can be done in a couple of different ways. Filter-based approaches, these methods give each feature a score or rank, and then you use this information to select variables. There are many different ways to get these rankings and many will be covered in the chapter. Wrapper-based approaches. These methods iteratively look at subsets of data to try to find the best set. Their main downside is they tend to add a lot of computational overhead as you need to fit your model many times. Lastly, we have embedded methods. These methods use more advanced methods, sometimes other models, to do the feature selection.",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>66</span>  <span class='chapter-title'>Too Many Overview</span>"
    ]
  },
  {
    "objectID": "too-many-zv.html",
    "href": "too-many-zv.html",
    "title": "67  Zero Variance Filter",
    "section": "",
    "text": "67.1 Zero Variance Filter\nZero-variance predictors is a fancy way of saying that a predictor only takes 1 value. Another word for this is constant predictors. A zero variance predictor by definition contains no information as there isn’t a relationship between the outcome and the predictor. These types of predictors come in many different data sets. And are sometimes created in the course of the feature engineering process, such as when we do Dummy Encoding on categorical predictors with known possible levels.\nThe reason why this chapter exists is two-fold. Firstly, since these predictors have zero information in them, they are safe to remove which would lead to simpler and faster models. Secondly, many model implementations will error if zero-variance predictors are present in the data. Even some methods in this book don’t handle zero-variance predictors gracefully. Take the normalization methods in the Normalization chapter, some of these require division with the standard deviation, which is zero thus resulting in division by 0. Other methods like PCA can get in trouble as zero variance predictors can yield non-invertible matrices that they can’t normally handle.\nThe solution to this problem is very simple. For each variable in the data set, count the number of unique values. If the number is 1, then mark the variable for removal.\nThe zero-variance only matters on the training data set. So you could be in a situation where the testing data contained other values. This doesn’t matter as zero-variance predictors only affect the fitting of the model, which is done on the training data set.\nThere are a couple of variants to this problem. Some models require multiple values for predictors across groups. And we need to handle that accordingly. Another more complicated problem is working with predictors that have almost zero variance but not quite. Say a predictor has 999 instances of 10 and 1 instance of 15. According to the above definition, it doesn’t have zero variance. But it feels very close to it. These might be considered so low in information that they would be worth removing as well.\nMore care has to be taken as these predictors could have information in them, but they have low evidence. The way we flag near-zero variance predictors isn’t going to be as straightforward as how we did it above. We can’t just look at the number of unique values, as having 2 unique values by itself isn’t bad, as a 50/50 split of a variable is far from constant. We need to find a way to indicate that the variable takes few values. One metric could be looking at the percentage that the most common value is taken, if this is high it would be a prime candidate for near-zero variance. One could calculate the variance and pick a threshold. This would be harder to do since the calculated variance depends on scale. We could look at the ratio of the frequency of the most common value to the frequency of the second most common value. If this value is large then we have another contender for near-zero variance.\nThese different characteristics can be combined in different ways to suit your need for your data. you will likely need to tune the threshold values as well.",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Zero Variance Filter</span>"
    ]
  },
  {
    "objectID": "too-many-zv.html#pros-and-cons",
    "href": "too-many-zv.html#pros-and-cons",
    "title": "67  Zero Variance Filter",
    "section": "67.2 Pros and Cons",
    "text": "67.2 Pros and Cons\n\n67.2.1 Pros\n\nRemoving zero variance predictors should provide no downside\nFaster and smaller models\nEasy to explain and execute\n\n\n\n67.2.2 Cons\n\nRemoval of near-zero predictors requires care to avoid removing useful predictors",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Zero Variance Filter</span>"
    ]
  },
  {
    "objectID": "too-many-zv.html#r-examples",
    "href": "too-many-zv.html#r-examples",
    "title": "67  Zero Variance Filter",
    "section": "67.3 R Examples",
    "text": "67.3 R Examples\nWe will use the step_zv() and step_nzv() steps which are used to remove zero variance and near-zero variance preditors respectively.\n\n\n\n\n\n\nTODO\n\n\n\nfind a good data set\n\n\nBelow we are using the step_zv() function to remove\n\nlibrary(recipes)\n\ndata(\"ames\", package = \"modeldata\")\n\nzv_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep()\n\nWe can use the tidy() method to find out which variables were removed\n\nzv_rec |&gt;\n  tidy(1)\n\n# A tibble: 0 × 2\n# ℹ 2 variables: terms &lt;chr&gt;, id &lt;chr&gt;\n\n\nWe can remove non-zero variance predictors in the same manner using step_nzv()\n\nnzv_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_nzv(all_predictors()) |&gt;\n  prep()\n\nnzv_rec |&gt;\n  tidy(1)\n\n# A tibble: 21 × 2\n   terms          id       \n   &lt;chr&gt;          &lt;chr&gt;    \n 1 Street         nzv_RUieL\n 2 Alley          nzv_RUieL\n 3 Land_Contour   nzv_RUieL\n 4 Utilities      nzv_RUieL\n 5 Land_Slope     nzv_RUieL\n 6 Condition_2    nzv_RUieL\n 7 Roof_Matl      nzv_RUieL\n 8 Bsmt_Cond      nzv_RUieL\n 9 BsmtFin_Type_2 nzv_RUieL\n10 BsmtFin_SF_2   nzv_RUieL\n# ℹ 11 more rows",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Zero Variance Filter</span>"
    ]
  },
  {
    "objectID": "too-many-zv.html#python-examples",
    "href": "too-many-zv.html#python-examples",
    "title": "67  Zero Variance Filter",
    "section": "67.4 Python Examples",
    "text": "67.4 Python Examples\nWe are using the ames data set for examples. {sklearn} provided the VarianceThreshold() method we can use. With this, we can set the threshold argument to specify the threshold of when to remove. The default 0 will remove zero-variance columns.\n\nfrom feazdata import ames\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.compose import make_column_selector\nimport numpy as np\n\nct = ColumnTransformer(\n    [('onehot', VarianceThreshold(threshold=0), make_column_selector(dtype_include=np.number))], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('onehot', VarianceThreshold(threshold=0),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x3593c22d0&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('onehot', VarianceThreshold(threshold=0),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x3593c22d0&gt;)]) onehot&lt;sklearn.compose._column_transformer.make_column_selector object at 0x3593c22d0&gt;  VarianceThreshold?Documentation for VarianceThresholdVarianceThreshold(threshold=0) remainder['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'Functional', 'Garage_Type', 'Garage_Finish', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence', 'Misc_Feature', 'Sale_Type', 'Sale_Condition'] passthroughpassthrough \n\nct.transform(ames)\n\n      onehot__Lot_Frontage  ...  remainder__Sale_Condition\n0                      141  ...                     Normal\n1                       80  ...                     Normal\n2                       81  ...                     Normal\n3                       93  ...                     Normal\n4                       74  ...                     Normal\n...                    ...  ...                        ...\n2925                    37  ...                     Normal\n2926                     0  ...                     Normal\n2927                    62  ...                     Normal\n2928                    77  ...                     Normal\n2929                    74  ...                     Normal\n\n[2930 rows x 74 columns]\n\n\nbut we can change that threshold to remove near-zero variance columns.\n\nct = ColumnTransformer(\n    [('onehot', VarianceThreshold(threshold=0.2), make_column_selector(dtype_include=np.number))], \n    remainder=\"passthrough\")\n\nct.fit(ames)\n\nColumnTransformer(remainder='passthrough',\n                  transformers=[('onehot', VarianceThreshold(threshold=0.2),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x3593de180&gt;)])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ColumnTransformer?Documentation for ColumnTransformeriFittedColumnTransformer(remainder='passthrough',\n                  transformers=[('onehot', VarianceThreshold(threshold=0.2),\n                                 &lt;sklearn.compose._column_transformer.make_column_selector object at 0x3593de180&gt;)]) onehot&lt;sklearn.compose._column_transformer.make_column_selector object at 0x3593de180&gt;  VarianceThreshold?Documentation for VarianceThresholdVarianceThreshold(threshold=0.2) remainder['MS_SubClass', 'MS_Zoning', 'Street', 'Alley', 'Lot_Shape', 'Land_Contour', 'Utilities', 'Lot_Config', 'Land_Slope', 'Neighborhood', 'Condition_1', 'Condition_2', 'Bldg_Type', 'House_Style', 'Overall_Cond', 'Roof_Style', 'Roof_Matl', 'Exterior_1st', 'Exterior_2nd', 'Mas_Vnr_Type', 'Exter_Cond', 'Foundation', 'Bsmt_Cond', 'Bsmt_Exposure', 'BsmtFin_Type_1', 'BsmtFin_Type_2', 'Heating', 'Heating_QC', 'Central_Air', 'Electrical', 'Functional', 'Garage_Type', 'Garage_Finish', 'Garage_Cond', 'Paved_Drive', 'Pool_QC', 'Fence', 'Misc_Feature', 'Sale_Type', 'Sale_Condition'] passthroughpassthrough \n\nct.transform(ames)\n\n      onehot__Lot_Frontage  ...  remainder__Sale_Condition\n0                      141  ...                     Normal\n1                       80  ...                     Normal\n2                       81  ...                     Normal\n3                       93  ...                     Normal\n4                       74  ...                     Normal\n...                    ...  ...                        ...\n2925                    37  ...                     Normal\n2926                     0  ...                     Normal\n2927                    62  ...                     Normal\n2928                    77  ...                     Normal\n2929                    74  ...                     Normal\n\n[2930 rows x 70 columns]",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>67</span>  <span class='chapter-title'>Zero Variance Filter</span>"
    ]
  },
  {
    "objectID": "too-many-pca.html",
    "href": "too-many-pca.html",
    "title": "68  Principal Component Analysis",
    "section": "",
    "text": "68.1 Principal Component Analysis\nThe first dimensionality reduction method most people are introduced to is Principal Component Analysis (PCA). There is a good reason for this, it is a dependable method that has stood the test of time. This chapter will cover what PCA is in its main form, with its variants described in next chapter on PCA variants.\nThe reason why we want to apply PCA in the first place, is because we expect that the information in the predictors could be more efficiently stored. Or in other words, because there is redundancy in the data. This can also be seen in part using a correlation plot. Using the numeric variables of the ames data set, we can plot the correlation of it.\nWhat we see is that many of the variables are correlated with eachother. This is expected since larger houses is more likely to have more bed rooms, larger basements, more fireplaces, etc etc. But it begs the question: are we efficiently storing this information? It Many of the variables store some variant of “large house” with something else mixed in.\nWe will motivate what PCA does using the following simulated 2-dimensional data set.\nThere is a clear correlation between these two variables. One way of thinking about what PCA does is that it rotates the data set, in order to maximize the variance along one dimension. visually we can find that line as seen below.\nRotating the data gives us the following data set after applying PCA.\nIn this adjusted data set, we would be able to drop the second predictor as the first contains most of the variance in the data set.\nWe can extend this method to higher dimensional data sets as well. The first principle component was found by finding the hyperplane that maximized the variance along it, finding the second principle component is found by calculating the hyperplace that maximized the variance of any planes that are orthogonal to the first principle component. The third is found by finding one that is orthogonal to the first and second. This is repeated until you have a principle component for each variable in the original data set.\nIn math terms this amounts to finding \\(\\phi_{ij}\\) that satisfy\n\\[\n\\sum_{j=1}^p  \\phi_{ji}^2 = 1\n\\]\nWhere \\(\\phi_{ij}\\) indicates the value of the loading for the ith input predictor and jth outcome predictor. We can write out the full list of equations like so.\n\\[\n\\begin{align}\n&Z_1 = \\phi_{11} X_1 + \\phi_{21} X_2 + ... + \\phi_{p1}X_p \\\\\n&Z_2 = \\phi_{12} X_1 + \\phi_{22} X_2 + ... + \\phi_{p2}X_p \\\\\n&... \\\\\n&Z_p = \\phi_{1p} X_1 + \\phi_{2p} X_2 + ... + \\phi_{pp}X_p\n\\end{align}\n\\]\nLuckily this is this can be found using linear algebra operations, hence why this operation has the computational speed it has.\nThe reason why this method works as a feature engineering method, is that the loadings \\(\\phi_{ij}\\) that are estimated can be saved and applied to new data.\nThe astute reader will notice that the above description technically doesn’t reduce dimensions, it merely rotates the data in a way that maximizes variance for each variable, under the constraint that they are all orthogonal. It becomes a dimensionality reduction method by selecting only a certain number of principle components. We are hoping that we can place a cutoff between that selects enough useful components without removing too much.\nOne downside to PCA is that we are not able to completely stop the loss of signal, but it is used as we can often vastly decrease the number of columns in our data set. It is a trade-off between computational speed and performance. And can be hyperparameter tuned.\nIn the below chart we have calculated the cumulative percent variance for 4 different data sets being processed by PCA.\nThese charts show how different datasets react differently by having PCA applied. Remember that we only want to select the top N components, such that it acts like a dimensionality reduction method. Both of the left charts shows a fast rise in the cumulative variance, with a fairly early plateau. Making a cutoff at the plateau is likely a good choice. The lower left data set captures 99% of the variance with 27 components, which amounts 50% decrease in predictors. On the other hand we have the bottom left data set which achieves the same 99& variance with the first 31 components, which only amounts to 10% decrease in predictors. A 99% threshold is quite high, and should be tuned for performance and speed.\nIt is a common misconception that PCA is a computationally intensive for large data sets, that is only the case if you apply PCA to generate all the components. If you instead run your method such that is captures 90% of the variance, or only the first 10 components then your method should only calculate those and stop. Remember that this method is iterative over the components, so for optimal computational speed you just need to make sure the settings are correct before running.\nA benefit to using PCA is that the resulting predictors are uncorrelated by definition. For performance reasons this makes PCA a prime candidate for methods that have a hard time dealing with multicollinearity. Linear regression being one such model. Principal Component Regression (PCR) is a term for such a scenario. It is a model that applies PCA to the predictors before passing them to a linear regression model. However we don’t treat PCR as its own method, instead thinking of it as a workflow/pipeline with PCA as a preprocessor and linear regression as the model.\nAnother common misconception is that PCA requires your predictors to be normally distributed. PCA operates over the correlation matrix which can be done with normality. That doesn’t mean that normally is bad, just that it isn’t required here.\nWhat might have been the cause of this misconception is the fact that PCA is sensitive to scaling (normalization). This should make sense intuitively since we are optimizing over variances. If one predictor is age and another is yearly income, then the first PC would overwhelming be yearly income almost exclusively because of the scale involved. It is therefore highly recommended to apply any type of normalization. You should further verify whether the PCA implementation you are using have a bundled normalization preprocessor.\nThere are a number of limitations, with the first and foremost being that PCA is only able to extract effects that come from linear combinations of predictors, in other words, it won’t be able to detect non-linear relationships. The algorithms used will be determanistic, however they will only give identical results up to a sign flip for each variable. This should not be a big issue but it can be surpricing at first if you do not expect it to happen. PCA Makes for harder, but not impossible interprebility. The PCA variants try to deal with this issue. This is especially true since the base case combines EVERY predictor in the input, to produce EVERY predictor in the output. Another limitation is that PCA doesn’t have a native way of dealing with missing data. However that doesn’t mean you can’t use PCA with data with missing values, just that you have to be aware of what will happen if you try. Since PCA is calculated from a correlation matrix, missing values could simply be excluded during the construction of the correlation matrix. This will be imperfect but is sometimes done instead of dealing with the missing values first.\nBelow is an example of the principle components in actions. We took the MNIST database and performed PCA on the pixel values as predictors. First We apply it to the entire data set.\nWe clearly see some effects here. Remember that it isn’t important whether something is positive or negative, just that something is different than something else. the first PC more or less detectif something is in the middle of the image, and maybe a faint outline of an 8 or 3. The second PC more or less looks for a diagonal like as we see in 2s and 7s. The third PC becomes harder to decipher, but it looks a little like 4s and 9s. As we go up in PCs it gets harder and harder to find the signal, which is fine as PCA here isn’t trying to make a representation of the data, it is simply finding the combination of values (pixels in this example) that leads to the highest variance.\nIf we instead did what we did before but only included data for the number 2 we will see the following principle components.\nHere we can see that the PCs very much resemple what we would expect a 2 to look like. And you would honest get a pretty good performance if you just used the first PC as an input into a classifier.",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-pca.html#pros-and-cons",
    "href": "too-many-pca.html#pros-and-cons",
    "title": "68  Principal Component Analysis",
    "section": "68.2 Pros and Cons",
    "text": "68.2 Pros and Cons\n\n68.2.1 Pros\n\nFast and reliable performance\n\n\n\n68.2.2 Cons\n\nCan’t capture non-linear effects\nOnly works with numeric data",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-pca.html#r-examples",
    "href": "too-many-pca.html#r-examples",
    "title": "68  Principal Component Analysis",
    "section": "68.3 R Examples",
    "text": "68.3 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names_num &lt;- ames |&gt;\n  select(where(is.numeric))\n\n{recipes} provides step_pca() which is the standard way to perform PCA.\n\npca_rec &lt;- recipe(~ ., data = ames_num) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_pca(all_numeric_predictors(), threshold = 0.75)\n\npca_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 15\n$ PC01 &lt;dbl&gt; -1.29302633, 2.44938542, 0.38867105, -2.83884535, -0.95000586, -1…\n$ PC02 &lt;dbl&gt; -1.4349755, -0.8954603, -0.6564163, -0.8245922, 0.4031128, 0.5472…\n$ PC03 &lt;dbl&gt; -2.73394709, -0.46555724, -2.60354922, -2.21540751, 1.84242862, 1…\n$ PC04 &lt;dbl&gt; -1.24130360, 0.45302075, -1.89476335, -0.09576065, -0.82419328, -…\n$ PC05 &lt;dbl&gt; -0.234670439, -1.158034467, 0.539299859, 0.108399390, -0.00465055…\n$ PC06 &lt;dbl&gt; -0.53071493, -0.74448291, 2.01605256, -1.63612239, 0.09178837, 0.…\n$ PC07 &lt;dbl&gt; -1.6667080, -0.9919920, 1.7271016, -1.6760806, -1.4437569, -0.644…\n$ PC08 &lt;dbl&gt; -0.3914627, -0.7070779, -8.0957831, -1.2322444, -1.3064689, -1.14…\n$ PC09 &lt;dbl&gt; 0.31653849, 0.26548666, -13.85439341, 0.18173142, 0.57359254, 0.6…\n$ PC10 &lt;dbl&gt; -0.25039814, 1.25638151, -7.81439719, 0.02538957, 0.02419863, -0.…\n$ PC11 &lt;dbl&gt; 0.27811935, 0.45201399, -5.43053271, -0.09578846, -0.06697844, -0…\n$ PC12 &lt;dbl&gt; 0.54106553, -0.13714522, -3.34354971, -0.10743242, -0.28789251, -…\n$ PC13 &lt;dbl&gt; -1.4797391, -0.9946868, -6.7879045, 0.4175475, -0.3688771, -0.327…\n$ PC14 &lt;dbl&gt; 0.0671268, 1.0330778, 4.6157955, -0.6283405, 0.1680150, 0.1357667…\n$ PC15 &lt;dbl&gt; 1.5901974, 0.9449574, 2.3581422, 0.5016875, 0.8524806, 1.2719532,…\n\n\nYou can either use num_comp or threshold to determine how many components to generate.\nIf you have data with a lot of predictors in it, it can be time saving to only calculate the needed components, do to this we can use the step_pca_truncated() from the {embed} package.\n\nlibrary(embed)\npca_rec &lt;- recipe(~ ., data = ames_num) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  step_pca_truncated(all_numeric_predictors(), num_comp = 10)\n\npca_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 10\n$ PC01 &lt;dbl&gt; -1.29302633, 2.44938542, 0.38867105, -2.83884535, -0.95000586, -1…\n$ PC02 &lt;dbl&gt; 1.4349755, 0.8954603, 0.6564163, 0.8245922, -0.4031128, -0.547251…\n$ PC03 &lt;dbl&gt; 2.73394709, 0.46555724, 2.60354922, 2.21540751, -1.84242862, -1.8…\n$ PC04 &lt;dbl&gt; -1.24130360, 0.45302075, -1.89476335, -0.09576065, -0.82419328, -…\n$ PC05 &lt;dbl&gt; 0.234670439, 1.158034467, -0.539299859, -0.108399390, 0.004650556…\n$ PC06 &lt;dbl&gt; 0.53071562, 0.74448296, -2.01605044, 1.63612287, -0.09178812, -0.…\n$ PC07 &lt;dbl&gt; -1.6667076, -0.9919920, 1.7271030, -1.6760803, -1.4437567, -0.644…\n$ PC08 &lt;dbl&gt; 0.3914640, 0.7070780, 8.0957873, 1.2322452, 1.3064693, 1.1430527,…\n$ PC09 &lt;dbl&gt; -0.31660104, -0.26549832, 13.85418232, -0.18176164, -0.57361371, …\n$ PC10 &lt;dbl&gt; -0.25058066, 1.25634247, -7.81502802, 0.02531114, 0.02413794, -0.…",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-pca.html#python-examples",
    "href": "too-many-pca.html#python-examples",
    "title": "68  Principal Component Analysis",
    "section": "68.4 Python Examples",
    "text": "68.4 Python Examples\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>68</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-pca-variants.html",
    "href": "too-many-pca-variants.html",
    "title": "69  🏗️ Principal Component Analysis Variants",
    "section": "",
    "text": "69.1 Principal Component Analysis Variants\nWIP\nhttps://www.youtube.com/watch?v=9iol3Lk6kyU&t=3s",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>🏗️ Principal Component Analysis Variants</span>"
    ]
  },
  {
    "objectID": "too-many-pca-variants.html#pros-and-cons",
    "href": "too-many-pca-variants.html#pros-and-cons",
    "title": "69  🏗️ Principal Component Analysis Variants",
    "section": "69.2 Pros and Cons",
    "text": "69.2 Pros and Cons\n\n69.2.1 Pros\n\n\n69.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>🏗️ Principal Component Analysis Variants</span>"
    ]
  },
  {
    "objectID": "too-many-pca-variants.html#r-examples",
    "href": "too-many-pca-variants.html#r-examples",
    "title": "69  🏗️ Principal Component Analysis Variants",
    "section": "69.3 R Examples",
    "text": "69.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>🏗️ Principal Component Analysis Variants</span>"
    ]
  },
  {
    "objectID": "too-many-pca-variants.html#python-examples",
    "href": "too-many-pca-variants.html#python-examples",
    "title": "69  🏗️ Principal Component Analysis Variants",
    "section": "69.4 Python Examples",
    "text": "69.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>69</span>  <span class='chapter-title'>🏗️ Principal Component Analysis Variants</span>"
    ]
  },
  {
    "objectID": "too-many-ica.html",
    "href": "too-many-ica.html",
    "title": "70  🏗️ Independent Component Analysis",
    "section": "",
    "text": "70.1 Independent Component Analysis\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>🏗️ Independent Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-ica.html#pros-and-cons",
    "href": "too-many-ica.html#pros-and-cons",
    "title": "70  🏗️ Independent Component Analysis",
    "section": "70.2 Pros and Cons",
    "text": "70.2 Pros and Cons\n\n70.2.1 Pros\n\n\n70.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>🏗️ Independent Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-ica.html#r-examples",
    "href": "too-many-ica.html#r-examples",
    "title": "70  🏗️ Independent Component Analysis",
    "section": "70.3 R Examples",
    "text": "70.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>🏗️ Independent Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-ica.html#python-examples",
    "href": "too-many-ica.html#python-examples",
    "title": "70  🏗️ Independent Component Analysis",
    "section": "70.4 Python Examples",
    "text": "70.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>70</span>  <span class='chapter-title'>🏗️ Independent Component Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-nmf.html",
    "href": "too-many-nmf.html",
    "title": "71  🏗️ Non-Negative Matrix Factorization",
    "section": "",
    "text": "71.1 Non-Negative Matrix Factorization\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>🏗️ Non-Negative Matrix Factorization</span>"
    ]
  },
  {
    "objectID": "too-many-nmf.html#pros-and-cons",
    "href": "too-many-nmf.html#pros-and-cons",
    "title": "71  🏗️ Non-Negative Matrix Factorization",
    "section": "71.2 Pros and Cons",
    "text": "71.2 Pros and Cons\n\n71.2.1 Pros\n\n\n71.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>🏗️ Non-Negative Matrix Factorization</span>"
    ]
  },
  {
    "objectID": "too-many-nmf.html#r-examples",
    "href": "too-many-nmf.html#r-examples",
    "title": "71  🏗️ Non-Negative Matrix Factorization",
    "section": "71.3 R Examples",
    "text": "71.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>🏗️ Non-Negative Matrix Factorization</span>"
    ]
  },
  {
    "objectID": "too-many-nmf.html#python-examples",
    "href": "too-many-nmf.html#python-examples",
    "title": "71  🏗️ Non-Negative Matrix Factorization",
    "section": "71.4 Python Examples",
    "text": "71.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>71</span>  <span class='chapter-title'>🏗️ Non-Negative Matrix Factorization</span>"
    ]
  },
  {
    "objectID": "too-many-lda.html",
    "href": "too-many-lda.html",
    "title": "72  🏗️ Linear Discriminant Analysis",
    "section": "",
    "text": "72.1 Linear Discriminant Analysis\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>🏗️ Linear Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-lda.html#pros-and-cons",
    "href": "too-many-lda.html#pros-and-cons",
    "title": "72  🏗️ Linear Discriminant Analysis",
    "section": "72.2 Pros and Cons",
    "text": "72.2 Pros and Cons\n\n72.2.1 Pros\n\n\n72.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>🏗️ Linear Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-lda.html#r-examples",
    "href": "too-many-lda.html#r-examples",
    "title": "72  🏗️ Linear Discriminant Analysis",
    "section": "72.3 R Examples",
    "text": "72.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>🏗️ Linear Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-lda.html#python-examples",
    "href": "too-many-lda.html#python-examples",
    "title": "72  🏗️ Linear Discriminant Analysis",
    "section": "72.4 Python Examples",
    "text": "72.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>72</span>  <span class='chapter-title'>🏗️ Linear Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-gda.html",
    "href": "too-many-gda.html",
    "title": "73  🏗️ Generalized Discriminant Analysis",
    "section": "",
    "text": "73.1 Generalized Discriminant Analysis\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>🏗️ Generalized Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-gda.html#pros-and-cons",
    "href": "too-many-gda.html#pros-and-cons",
    "title": "73  🏗️ Generalized Discriminant Analysis",
    "section": "73.2 Pros and Cons",
    "text": "73.2 Pros and Cons\n\n73.2.1 Pros\n\n\n73.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>🏗️ Generalized Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-gda.html#r-examples",
    "href": "too-many-gda.html#r-examples",
    "title": "73  🏗️ Generalized Discriminant Analysis",
    "section": "73.3 R Examples",
    "text": "73.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>🏗️ Generalized Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-gda.html#python-examples",
    "href": "too-many-gda.html#python-examples",
    "title": "73  🏗️ Generalized Discriminant Analysis",
    "section": "73.4 Python Examples",
    "text": "73.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>73</span>  <span class='chapter-title'>🏗️ Generalized Discriminant Analysis</span>"
    ]
  },
  {
    "objectID": "too-many-autoencoder.html",
    "href": "too-many-autoencoder.html",
    "title": "74  🏗️ Autoencoders",
    "section": "",
    "text": "74.1 Autoencoders\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>🏗️ Autoencoders</span>"
    ]
  },
  {
    "objectID": "too-many-autoencoder.html#pros-and-cons",
    "href": "too-many-autoencoder.html#pros-and-cons",
    "title": "74  🏗️ Autoencoders",
    "section": "74.2 Pros and Cons",
    "text": "74.2 Pros and Cons\n\n74.2.1 Pros\n\n\n74.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>🏗️ Autoencoders</span>"
    ]
  },
  {
    "objectID": "too-many-autoencoder.html#r-examples",
    "href": "too-many-autoencoder.html#r-examples",
    "title": "74  🏗️ Autoencoders",
    "section": "74.3 R Examples",
    "text": "74.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>🏗️ Autoencoders</span>"
    ]
  },
  {
    "objectID": "too-many-autoencoder.html#python-examples",
    "href": "too-many-autoencoder.html#python-examples",
    "title": "74  🏗️ Autoencoders",
    "section": "74.4 Python Examples",
    "text": "74.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>74</span>  <span class='chapter-title'>🏗️ Autoencoders</span>"
    ]
  },
  {
    "objectID": "too-many-umap.html",
    "href": "too-many-umap.html",
    "title": "75  🏗️ Uniform Manifold Approximation and Projection",
    "section": "",
    "text": "75.1 UMAP\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>🏗️ Uniform Manifold Approximation and Projection</span>"
    ]
  },
  {
    "objectID": "too-many-umap.html#pros-and-cons",
    "href": "too-many-umap.html#pros-and-cons",
    "title": "75  🏗️ Uniform Manifold Approximation and Projection",
    "section": "75.2 Pros and Cons",
    "text": "75.2 Pros and Cons\n\n75.2.1 Pros\n\n\n75.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>🏗️ Uniform Manifold Approximation and Projection</span>"
    ]
  },
  {
    "objectID": "too-many-umap.html#r-examples",
    "href": "too-many-umap.html#r-examples",
    "title": "75  🏗️ Uniform Manifold Approximation and Projection",
    "section": "75.3 R Examples",
    "text": "75.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>🏗️ Uniform Manifold Approximation and Projection</span>"
    ]
  },
  {
    "objectID": "too-many-umap.html#python-examples",
    "href": "too-many-umap.html#python-examples",
    "title": "75  🏗️ Uniform Manifold Approximation and Projection",
    "section": "75.4 Python Examples",
    "text": "75.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>75</span>  <span class='chapter-title'>🏗️ Uniform Manifold Approximation and Projection</span>"
    ]
  },
  {
    "objectID": "too-many-isomap.html",
    "href": "too-many-isomap.html",
    "title": "76  🏗️ ISOMAP",
    "section": "",
    "text": "76.1 ISOMAP\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>🏗️ ISOMAP</span>"
    ]
  },
  {
    "objectID": "too-many-isomap.html#pros-and-cons",
    "href": "too-many-isomap.html#pros-and-cons",
    "title": "76  🏗️ ISOMAP",
    "section": "76.2 Pros and Cons",
    "text": "76.2 Pros and Cons\n\n76.2.1 Pros\n\n\n76.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>🏗️ ISOMAP</span>"
    ]
  },
  {
    "objectID": "too-many-isomap.html#r-examples",
    "href": "too-many-isomap.html#r-examples",
    "title": "76  🏗️ ISOMAP",
    "section": "76.3 R Examples",
    "text": "76.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>🏗️ ISOMAP</span>"
    ]
  },
  {
    "objectID": "too-many-isomap.html#python-examples",
    "href": "too-many-isomap.html#python-examples",
    "title": "76  🏗️ ISOMAP",
    "section": "76.4 Python Examples",
    "text": "76.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>76</span>  <span class='chapter-title'>🏗️ ISOMAP</span>"
    ]
  },
  {
    "objectID": "too-many-filter.html",
    "href": "too-many-filter.html",
    "title": "77  🏗️ Filter based feature selection",
    "section": "",
    "text": "77.1 Filter based feature selection\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>🏗️ Filter based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-filter.html#pros-and-cons",
    "href": "too-many-filter.html#pros-and-cons",
    "title": "77  🏗️ Filter based feature selection",
    "section": "77.2 Pros and Cons",
    "text": "77.2 Pros and Cons\n\n77.2.1 Pros\n\n\n77.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>🏗️ Filter based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-filter.html#r-examples",
    "href": "too-many-filter.html#r-examples",
    "title": "77  🏗️ Filter based feature selection",
    "section": "77.3 R Examples",
    "text": "77.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>🏗️ Filter based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-filter.html#python-examples",
    "href": "too-many-filter.html#python-examples",
    "title": "77  🏗️ Filter based feature selection",
    "section": "77.4 Python Examples",
    "text": "77.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>77</span>  <span class='chapter-title'>🏗️ Filter based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-wrapper.html",
    "href": "too-many-wrapper.html",
    "title": "78  🏗️ Wrapper based feature selection",
    "section": "",
    "text": "78.1 Wrapper Based feature selection\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>🏗️ Wrapper based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-wrapper.html#pros-and-cons",
    "href": "too-many-wrapper.html#pros-and-cons",
    "title": "78  🏗️ Wrapper based feature selection",
    "section": "78.2 Pros and Cons",
    "text": "78.2 Pros and Cons\n\n78.2.1 Pros\n\n\n78.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>🏗️ Wrapper based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-wrapper.html#r-examples",
    "href": "too-many-wrapper.html#r-examples",
    "title": "78  🏗️ Wrapper based feature selection",
    "section": "78.3 R Examples",
    "text": "78.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>🏗️ Wrapper based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-wrapper.html#python-examples",
    "href": "too-many-wrapper.html#python-examples",
    "title": "78  🏗️ Wrapper based feature selection",
    "section": "78.4 Python Examples",
    "text": "78.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>78</span>  <span class='chapter-title'>🏗️ Wrapper based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-embedded.html",
    "href": "too-many-embedded.html",
    "title": "79  🏗️ Embedded based feature selection",
    "section": "",
    "text": "79.1 Embedded based feature selection\nWIP",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>🏗️ Embedded based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-embedded.html#pros-and-cons",
    "href": "too-many-embedded.html#pros-and-cons",
    "title": "79  🏗️ Embedded based feature selection",
    "section": "79.2 Pros and Cons",
    "text": "79.2 Pros and Cons\n\n79.2.1 Pros\n\n\n79.2.2 Cons",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>🏗️ Embedded based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-embedded.html#r-examples",
    "href": "too-many-embedded.html#r-examples",
    "title": "79  🏗️ Embedded based feature selection",
    "section": "79.3 R Examples",
    "text": "79.3 R Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>🏗️ Embedded based feature selection</span>"
    ]
  },
  {
    "objectID": "too-many-embedded.html#python-examples",
    "href": "too-many-embedded.html#python-examples",
    "title": "79  🏗️ Embedded based feature selection",
    "section": "79.4 Python Examples",
    "text": "79.4 Python Examples",
    "crumbs": [
      "Too Many Variables",
      "<span class='chapter-number'>79</span>  <span class='chapter-title'>🏗️ Embedded based feature selection</span>"
    ]
  },
  {
    "objectID": "correlated.html",
    "href": "correlated.html",
    "title": "80  Correlated Overview",
    "section": "",
    "text": "80.1 Correlated Overview\nCorrelation happens when two or more variables contain similar information. We typically refer to correlation when we talk about predictors. This can be a problem for some machine learning models as they don’t perform well with correlated predictors. There are many different ways to calculate the degree of correlation. And those details aren’t going to matter much right now. The important thing is that it can happen. Below we see such examples\nThe reason why correlated features are bad for our models is that two correlated features have the potential to share information that is useful. Imagine we are working with strongly correlated variables. Furthermore, we propose that predictor_1 is highly predictive in our model, since predictor_1 and predictor_2 are correlated, we can conclude that predictor_2 would also be highly predictive. The problem then arises when one of these predictors is used, the other predictor will no longer be a predictor since they share their information. Another way to think about it is that we could replace these two predictors with just one predictor with minimal loss of information. This is one of the reasons why we sometimes want to do dimension reduction, as seen in Chapter 66.\nWe will see how we can use the correlation structure to figure out which variables we can eliminate, this is covered in Chapter 81. This is a more specialized version of the methods we cover in Chapter 77 as we are looking at correlation to determine which variables to remove rather than their relationship to the outcome.\nAnother set of methods that works well in anything PCA-related, which are split into two chapters, one explaining PCA and one going over PCA varients. The resulting data coming out of PCA will be uncorrelated.",
    "crumbs": [
      "Correlated Data",
      "<span class='chapter-number'>80</span>  <span class='chapter-title'>Correlated Overview</span>"
    ]
  },
  {
    "objectID": "correlated-filter.html",
    "href": "correlated-filter.html",
    "title": "81  High Correlation Filter",
    "section": "",
    "text": "81.1 High Correlation Filter\nWe are looking to remove correlated features. By correlated features, we typically talk about Pearson correlation. The methods in this chapter don’t require that Pearson correlation is used, just that any correlation method is used. Next, we will look at how we can remove the offending variables. We will show an iterative approach and a clustering approach. Both methods are learned methods as they identify variables to be kept.\nI always find it useful to look at a correlation matrix first\nlooking at the above chart we see some correlated features. One way to perform our filtering is to find all the correlated pairs over a certain threshold and remove one of them. Below is a chart of the 10 most correlated pairs\nOne way to do filtering is to pick a threshold and repeatably remove one of the variables of the most correlated pair until there are no pairs left with a correlation over the threshold. This method has a minimal computational footprint as it just needs to calculate the correlations once at the beginning. The threshold is likely to need to be tuned as we can’t say for sure what a good threshold is. With the removal of variables, there is always a chance that we are removing signal rather than noise, this is increasingly true as we remove more and more predictors.\nIf we look at the above table, we notice that some of the variables occur together. One such example is Garage_Cars, Garage_Area and Sale_Price. These 3 variables are highly co-correlated and it would be neat if we could deal with these variables at the same time.\nWhat we could do, is take the correlation matrix and apply a clustering model on it. Then we use the clustering model to lump together the groups of highly correlated predictors. Then within each cluster, one predictor is chosen to be kept. The clusters should ideally be chosen such that uncorrelated predictors are alone in their cluster. This method can work better with the global structure of the data but requires fitting and tuning another model.",
    "crumbs": [
      "Correlated Data",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>High Correlation Filter</span>"
    ]
  },
  {
    "objectID": "correlated-filter.html#pros-and-cons",
    "href": "correlated-filter.html#pros-and-cons",
    "title": "81  High Correlation Filter",
    "section": "81.2 Pros and Cons",
    "text": "81.2 Pros and Cons\n\n81.2.1 Pros\n\nComputationally simple and fast\nEasily explainable. “Predictors were removed”\nWill lead to a faster and simpler model\n\n\n\n81.2.2 Cons\n\nCan be hard to justify. “Why was this predictor kept instead of this one?”\nWill lead to loss of signal and performance, with the hope that this loss is kept minimal",
    "crumbs": [
      "Correlated Data",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>High Correlation Filter</span>"
    ]
  },
  {
    "objectID": "correlated-filter.html#r-examples",
    "href": "correlated-filter.html#r-examples",
    "title": "81  High Correlation Filter",
    "section": "81.3 R Examples",
    "text": "81.3 R Examples\nWe will use the ames data set from {modeldata} in this example. The {recipes} step step_corrr() performs the simple correlation filter described at the beginning of this chapter.\n\nlibrary(recipes)\nlibrary(modeldata)\n\ncorr_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_corr(all_numeric_predictors(), threshold = 0.75) |&gt;\n  prep()\n\nWe can see the variables that were removed with the tidy() method\n\ncorr_rec |&gt;\n  tidy(1)\n\n# A tibble: 3 × 2\n  terms        id        \n  &lt;chr&gt;        &lt;chr&gt;     \n1 First_Flr_SF corr_Bp5vK\n2 Gr_Liv_Area  corr_Bp5vK\n3 Garage_Cars  corr_Bp5vK\n\n\nWe can see that when we lower this threshold to the extreme, more predictors are removed.\n\nrecipe(Sale_Price ~ ., data = ames) |&gt;\n  step_corr(all_numeric_predictors(), threshold = 0.25) |&gt;\n  prep() |&gt;\n  tidy(1)\n\n# A tibble: 13 × 2\n   terms          id        \n   &lt;chr&gt;          &lt;chr&gt;     \n 1 Bsmt_Unf_SF    corr_RUieL\n 2 Total_Bsmt_SF  corr_RUieL\n 3 First_Flr_SF   corr_RUieL\n 4 Gr_Liv_Area    corr_RUieL\n 5 Bsmt_Full_Bath corr_RUieL\n 6 Full_Bath      corr_RUieL\n 7 TotRms_AbvGrd  corr_RUieL\n 8 Fireplaces     corr_RUieL\n 9 Garage_Cars    corr_RUieL\n10 Garage_Area    corr_RUieL\n11 Year_Built     corr_RUieL\n12 Second_Flr_SF  corr_RUieL\n13 Year_Remod_Add corr_RUieL",
    "crumbs": [
      "Correlated Data",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>High Correlation Filter</span>"
    ]
  },
  {
    "objectID": "correlated-filter.html#python-examples",
    "href": "correlated-filter.html#python-examples",
    "title": "81  High Correlation Filter",
    "section": "81.4 Python Examples",
    "text": "81.4 Python Examples\nI’m not aware of a good way to do this in a scikit-learn way. Please file an issue on github if you know of a good way.",
    "crumbs": [
      "Correlated Data",
      "<span class='chapter-number'>81</span>  <span class='chapter-title'>High Correlation Filter</span>"
    ]
  },
  {
    "objectID": "outliers.html",
    "href": "outliers.html",
    "title": "82  Outliers Overview",
    "section": "",
    "text": "82.1 Outliers Overview\nWhen we talk about outliers, we mean values that are different from the rest of the values. This is typically seen as extreme values. Let us talk about it with an example first. Below is the famous Ames housing data set. We have plotted the living area against the sale price.\nTwo groups of observations appear to be quite far away from the rest of the points. We are in luck as these points are discussed in the data directory. The relevant quote is shown below:\nThese types of points would typically be called outliers. They are different enough from the rest of the observations. “Different enough” is by itself hard to define. And we won’t try in this book. What we will show is some ways that people define it and let you decide what is best for your data about treatment.\nIn this care, we had enough domain knowledge to be able to determine the reason for these points to be outliers, and how to deal with them. We won’t always be this lucky. These points were outliers in 2 ways. 3 of the points didn’t include the full price of the house, so it could be classified as an error. If we think of this data set as “houses with know full prices” then we could exclude them as not fitting that criteria. The other two houses are outliers in a purely numerical sense. They take values that are much more different than the rest of the observations.\nWhen you systematically remove observations, regardless of whether you think they are outliers or not. Then you are limiting the domain where your model is expected to work. This may be fine or not, it will depend on the specific problem you working on. But be very careful not to remove actual observations from your data.\nWe have a handful of different ways to deal with outliers. the first choice is to not. Some model types don’t care about outliers that much. Anything that uses distances is very affected by outliers. Tree-based models don’t. Some other preprocessing method such as Robust Scaling also isn’t affected by outliers.\nIf you are planning on handling outliers you want to start by identifying them. In the Outlier removal chapter we look at how we can identify and remove outliers. In the outliers issues section we cover numerical transformations that lessen the effect that outliers have on the distribution. Instead of removing the specific observation that has outliers, or transforming the whole variable, we can choose to only modify the value of the outliers. We look at methods on how to do that in the Outliers Imputation chapter. In addition to all of these methods, we can also add additional indicator variables to denote whether an observation is an outlier or not, this is covered in the Outliers Indicate chapter.\nLastly, it might be appropriate to treat the outliers as a separate data set and fit a specific model to that part of the data.",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>82</span>  <span class='chapter-title'>Outliers Overview</span>"
    ]
  },
  {
    "objectID": "outliers-remove.html",
    "href": "outliers-remove.html",
    "title": "83  🏗️ Removal",
    "section": "",
    "text": "83.1 Removal\nWIP",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>🏗️ Removal</span>"
    ]
  },
  {
    "objectID": "outliers-remove.html#pros-and-cons",
    "href": "outliers-remove.html#pros-and-cons",
    "title": "83  🏗️ Removal",
    "section": "83.2 Pros and Cons",
    "text": "83.2 Pros and Cons\n\n83.2.1 Pros\n\n\n83.2.2 Cons",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>🏗️ Removal</span>"
    ]
  },
  {
    "objectID": "outliers-remove.html#r-examples",
    "href": "outliers-remove.html#r-examples",
    "title": "83  🏗️ Removal",
    "section": "83.3 R Examples",
    "text": "83.3 R Examples",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>🏗️ Removal</span>"
    ]
  },
  {
    "objectID": "outliers-remove.html#python-examples",
    "href": "outliers-remove.html#python-examples",
    "title": "83  🏗️ Removal",
    "section": "83.4 Python Examples",
    "text": "83.4 Python Examples",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>83</span>  <span class='chapter-title'>🏗️ Removal</span>"
    ]
  },
  {
    "objectID": "outliers-imputation.html",
    "href": "outliers-imputation.html",
    "title": "84  🏗️ Imputation",
    "section": "",
    "text": "84.1 Imputation\nWIP",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>84</span>  <span class='chapter-title'>🏗️ Imputation</span>"
    ]
  },
  {
    "objectID": "outliers-imputation.html#pros-and-cons",
    "href": "outliers-imputation.html#pros-and-cons",
    "title": "84  🏗️ Imputation",
    "section": "84.2 Pros and Cons",
    "text": "84.2 Pros and Cons\n\n84.2.1 Pros\n\n\n84.2.2 Cons",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>84</span>  <span class='chapter-title'>🏗️ Imputation</span>"
    ]
  },
  {
    "objectID": "outliers-imputation.html#r-examples",
    "href": "outliers-imputation.html#r-examples",
    "title": "84  🏗️ Imputation",
    "section": "84.3 R Examples",
    "text": "84.3 R Examples",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>84</span>  <span class='chapter-title'>🏗️ Imputation</span>"
    ]
  },
  {
    "objectID": "outliers-imputation.html#python-examples",
    "href": "outliers-imputation.html#python-examples",
    "title": "84  🏗️ Imputation",
    "section": "84.4 Python Examples",
    "text": "84.4 Python Examples",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>84</span>  <span class='chapter-title'>🏗️ Imputation</span>"
    ]
  },
  {
    "objectID": "outliers-indicate.html",
    "href": "outliers-indicate.html",
    "title": "85  🏗️ Indicate",
    "section": "",
    "text": "85.1 Indicate\nWIP",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>85</span>  <span class='chapter-title'>🏗️ Indicate</span>"
    ]
  },
  {
    "objectID": "outliers-indicate.html#pros-and-cons",
    "href": "outliers-indicate.html#pros-and-cons",
    "title": "85  🏗️ Indicate",
    "section": "85.2 Pros and Cons",
    "text": "85.2 Pros and Cons\n\n85.2.1 Pros\n\n\n85.2.2 Cons",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>85</span>  <span class='chapter-title'>🏗️ Indicate</span>"
    ]
  },
  {
    "objectID": "outliers-indicate.html#r-examples",
    "href": "outliers-indicate.html#r-examples",
    "title": "85  🏗️ Indicate",
    "section": "85.3 R Examples",
    "text": "85.3 R Examples",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>85</span>  <span class='chapter-title'>🏗️ Indicate</span>"
    ]
  },
  {
    "objectID": "outliers-indicate.html#python-examples",
    "href": "outliers-indicate.html#python-examples",
    "title": "85  🏗️ Indicate",
    "section": "85.4 Python Examples",
    "text": "85.4 Python Examples",
    "crumbs": [
      "Outliers",
      "<span class='chapter-number'>85</span>  <span class='chapter-title'>🏗️ Indicate</span>"
    ]
  },
  {
    "objectID": "imbalenced.html",
    "href": "imbalenced.html",
    "title": "86  Imbalanced Overview",
    "section": "",
    "text": "86.1 Imbalanced Overview\nWhether to keep this chapter in this book or not was considered, as the methods in this section are borderline feature engineering methods.\nThe potential problem with imbalanced data can most easily be seen in a classification setting. Propose we want to predict if an incoming email is spam or not. Furthermore, we assume that the spam rate is low and around 1% of the incoming emails. If you are not careful, you can easily end up with a model that predicts “not spam” all the time, since it will be correct 99% of the times. This is a common scenario, and it happens all the time. One culprit could be that there isn’t enough information in the minority class to be able to distinguish it from the majority class. And that is okay, not all modeling problems are easy. But your model will do its best anyway.\nThere are several different ways to handle imbalanced data, we will list all the ways in broad strokes, and then cover the methods that we could count as feature engineering.\nThe example above showcases why accuracy as a metric isn’t a good choice when the classes are not uniformly represented. So you can look at other metrics, such as precision, recall, ROC-AUC, or Brier score. You will need to know what metric works best for your project.\nAnother way we can handle this is by adding weights, either to the observations directly, or in the modeling framework as class weights. Giving your minority class high enough weight forces your model to consider them.\nRelated to the last point, some methods allow you the user to pass in custom objective functions, this can also be beneficial.\nEven if your model performs badly by default, your classification model might still have good separation, just not around the 50% cut-off point. Changing the threshold is another way you can overcome an imbalanced data set.\nLastly, and the ways that will be covered in this book is a sampling of the data. There are several different methods that we will cover in this book. These methods cluster somehow, so for some groups we only explain the general idea.\nWe can split these methods into two groups, under-sampling methods and over-sampling methods. In the under-sampling method, we are removing observations and in the over-sampling we are adding observations. Adding observations is usually done by basing the new observations on the existing observations, exactly or by interpolation.\nOver-sampling methods we will cover are:\nUnder-sampling methods we will cover are:\nSome methods do these methods together. We won’t consider those methods by themselves and instead let you know that you can do over-sampling followed by under-sampling if you choose.",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>86</span>  <span class='chapter-title'>Imbalanced Overview</span>"
    ]
  },
  {
    "objectID": "imbalenced-upsample.html",
    "href": "imbalenced-upsample.html",
    "title": "87  🏗️ Up-Sampling",
    "section": "",
    "text": "87.1 Up-Sampling\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>🏗️ Up-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-upsample.html#pros-and-cons",
    "href": "imbalenced-upsample.html#pros-and-cons",
    "title": "87  🏗️ Up-Sampling",
    "section": "87.2 Pros and Cons",
    "text": "87.2 Pros and Cons\n\n87.2.1 Pros\n\n\n87.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>🏗️ Up-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-upsample.html#r-examples",
    "href": "imbalenced-upsample.html#r-examples",
    "title": "87  🏗️ Up-Sampling",
    "section": "87.3 R Examples",
    "text": "87.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>🏗️ Up-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-upsample.html#python-examples",
    "href": "imbalenced-upsample.html#python-examples",
    "title": "87  🏗️ Up-Sampling",
    "section": "87.4 Python Examples",
    "text": "87.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>87</span>  <span class='chapter-title'>🏗️ Up-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-rose.html",
    "href": "imbalenced-rose.html",
    "title": "88  🏗️ ROSE",
    "section": "",
    "text": "88.1 ROSE\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>🏗️ ROSE</span>"
    ]
  },
  {
    "objectID": "imbalenced-rose.html#pros-and-cons",
    "href": "imbalenced-rose.html#pros-and-cons",
    "title": "88  🏗️ ROSE",
    "section": "88.2 Pros and Cons",
    "text": "88.2 Pros and Cons\n\n88.2.1 Pros\n\n\n88.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>🏗️ ROSE</span>"
    ]
  },
  {
    "objectID": "imbalenced-rose.html#r-examples",
    "href": "imbalenced-rose.html#r-examples",
    "title": "88  🏗️ ROSE",
    "section": "88.3 R Examples",
    "text": "88.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>🏗️ ROSE</span>"
    ]
  },
  {
    "objectID": "imbalenced-rose.html#python-examples",
    "href": "imbalenced-rose.html#python-examples",
    "title": "88  🏗️ ROSE",
    "section": "88.4 Python Examples",
    "text": "88.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>88</span>  <span class='chapter-title'>🏗️ ROSE</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote.html",
    "href": "imbalenced-smote.html",
    "title": "89  🏗️ SMOTE",
    "section": "",
    "text": "89.1 SMOTE\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>🏗️ SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote.html#pros-and-cons",
    "href": "imbalenced-smote.html#pros-and-cons",
    "title": "89  🏗️ SMOTE",
    "section": "89.2 Pros and Cons",
    "text": "89.2 Pros and Cons\n\n89.2.1 Pros\n\n\n89.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>🏗️ SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote.html#r-examples",
    "href": "imbalenced-smote.html#r-examples",
    "title": "89  🏗️ SMOTE",
    "section": "89.3 R Examples",
    "text": "89.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>🏗️ SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote.html#python-examples",
    "href": "imbalenced-smote.html#python-examples",
    "title": "89  🏗️ SMOTE",
    "section": "89.4 Python Examples",
    "text": "89.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>89</span>  <span class='chapter-title'>🏗️ SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote-variants.html",
    "href": "imbalenced-smote-variants.html",
    "title": "90  🏗️ SMOTE Variants",
    "section": "",
    "text": "90.1 SMOTE Variants\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>🏗️ SMOTE Variants</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote-variants.html#pros-and-cons",
    "href": "imbalenced-smote-variants.html#pros-and-cons",
    "title": "90  🏗️ SMOTE Variants",
    "section": "90.2 Pros and Cons",
    "text": "90.2 Pros and Cons\n\n90.2.1 Pros\n\n\n90.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>🏗️ SMOTE Variants</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote-variants.html#r-examples",
    "href": "imbalenced-smote-variants.html#r-examples",
    "title": "90  🏗️ SMOTE Variants",
    "section": "90.3 R Examples",
    "text": "90.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>🏗️ SMOTE Variants</span>"
    ]
  },
  {
    "objectID": "imbalenced-smote-variants.html#python-examples",
    "href": "imbalenced-smote-variants.html#python-examples",
    "title": "90  🏗️ SMOTE Variants",
    "section": "90.4 Python Examples",
    "text": "90.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>90</span>  <span class='chapter-title'>🏗️ SMOTE Variants</span>"
    ]
  },
  {
    "objectID": "imbalenced-borderline-smote.html",
    "href": "imbalenced-borderline-smote.html",
    "title": "91  🏗️ Borderline SMOTE",
    "section": "",
    "text": "91.1 Borderline SMOTE\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>🏗️ Borderline SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-borderline-smote.html#pros-and-cons",
    "href": "imbalenced-borderline-smote.html#pros-and-cons",
    "title": "91  🏗️ Borderline SMOTE",
    "section": "91.2 Pros and Cons",
    "text": "91.2 Pros and Cons\n\n91.2.1 Pros\n\n\n91.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>🏗️ Borderline SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-borderline-smote.html#r-examples",
    "href": "imbalenced-borderline-smote.html#r-examples",
    "title": "91  🏗️ Borderline SMOTE",
    "section": "91.3 R Examples",
    "text": "91.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>🏗️ Borderline SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-borderline-smote.html#python-examples",
    "href": "imbalenced-borderline-smote.html#python-examples",
    "title": "91  🏗️ Borderline SMOTE",
    "section": "91.4 Python Examples",
    "text": "91.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>91</span>  <span class='chapter-title'>🏗️ Borderline SMOTE</span>"
    ]
  },
  {
    "objectID": "imbalenced-adasyn.html",
    "href": "imbalenced-adasyn.html",
    "title": "92  🏗️ Adaptive Synthetic Algorithm",
    "section": "",
    "text": "92.1 Adaptive Synthetic Algorithm\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>🏗️ Adaptive Synthetic Algorithm</span>"
    ]
  },
  {
    "objectID": "imbalenced-adasyn.html#pros-and-cons",
    "href": "imbalenced-adasyn.html#pros-and-cons",
    "title": "92  🏗️ Adaptive Synthetic Algorithm",
    "section": "92.2 Pros and Cons",
    "text": "92.2 Pros and Cons\n\n92.2.1 Pros\n\n\n92.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>🏗️ Adaptive Synthetic Algorithm</span>"
    ]
  },
  {
    "objectID": "imbalenced-adasyn.html#r-examples",
    "href": "imbalenced-adasyn.html#r-examples",
    "title": "92  🏗️ Adaptive Synthetic Algorithm",
    "section": "92.3 R Examples",
    "text": "92.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>🏗️ Adaptive Synthetic Algorithm</span>"
    ]
  },
  {
    "objectID": "imbalenced-adasyn.html#python-examples",
    "href": "imbalenced-adasyn.html#python-examples",
    "title": "92  🏗️ Adaptive Synthetic Algorithm",
    "section": "92.4 Python Examples",
    "text": "92.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>92</span>  <span class='chapter-title'>🏗️ Adaptive Synthetic Algorithm</span>"
    ]
  },
  {
    "objectID": "imbalenced-downsample.html",
    "href": "imbalenced-downsample.html",
    "title": "93  🏗️ Down-Sampling",
    "section": "",
    "text": "93.1 Down-Sampling\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>🏗️ Down-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-downsample.html#pros-and-cons",
    "href": "imbalenced-downsample.html#pros-and-cons",
    "title": "93  🏗️ Down-Sampling",
    "section": "93.2 Pros and Cons",
    "text": "93.2 Pros and Cons\n\n93.2.1 Pros\n\n\n93.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>🏗️ Down-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-downsample.html#r-examples",
    "href": "imbalenced-downsample.html#r-examples",
    "title": "93  🏗️ Down-Sampling",
    "section": "93.3 R Examples",
    "text": "93.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>🏗️ Down-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-downsample.html#python-examples",
    "href": "imbalenced-downsample.html#python-examples",
    "title": "93  🏗️ Down-Sampling",
    "section": "93.4 Python Examples",
    "text": "93.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>93</span>  <span class='chapter-title'>🏗️ Down-Sampling</span>"
    ]
  },
  {
    "objectID": "imbalenced-nearmiss.html",
    "href": "imbalenced-nearmiss.html",
    "title": "94  🏗️ Near-Miss",
    "section": "",
    "text": "94.1 Near-Miss\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>🏗️ Near-Miss</span>"
    ]
  },
  {
    "objectID": "imbalenced-nearmiss.html#pros-and-cons",
    "href": "imbalenced-nearmiss.html#pros-and-cons",
    "title": "94  🏗️ Near-Miss",
    "section": "94.2 Pros and Cons",
    "text": "94.2 Pros and Cons\n\n94.2.1 Pros\n\n\n94.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>🏗️ Near-Miss</span>"
    ]
  },
  {
    "objectID": "imbalenced-nearmiss.html#r-examples",
    "href": "imbalenced-nearmiss.html#r-examples",
    "title": "94  🏗️ Near-Miss",
    "section": "94.3 R Examples",
    "text": "94.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>🏗️ Near-Miss</span>"
    ]
  },
  {
    "objectID": "imbalenced-nearmiss.html#python-examples",
    "href": "imbalenced-nearmiss.html#python-examples",
    "title": "94  🏗️ Near-Miss",
    "section": "94.4 Python Examples",
    "text": "94.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>94</span>  <span class='chapter-title'>🏗️ Near-Miss</span>"
    ]
  },
  {
    "objectID": "imbalenced-tomek.html",
    "href": "imbalenced-tomek.html",
    "title": "95  🏗️ Tomek Links",
    "section": "",
    "text": "95.1 Tomek Links\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>🏗️ Tomek Links</span>"
    ]
  },
  {
    "objectID": "imbalenced-tomek.html#pros-and-cons",
    "href": "imbalenced-tomek.html#pros-and-cons",
    "title": "95  🏗️ Tomek Links",
    "section": "95.2 Pros and Cons",
    "text": "95.2 Pros and Cons\n\n95.2.1 Pros\n\n\n95.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>🏗️ Tomek Links</span>"
    ]
  },
  {
    "objectID": "imbalenced-tomek.html#r-examples",
    "href": "imbalenced-tomek.html#r-examples",
    "title": "95  🏗️ Tomek Links",
    "section": "95.3 R Examples",
    "text": "95.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>🏗️ Tomek Links</span>"
    ]
  },
  {
    "objectID": "imbalenced-tomek.html#python-examples",
    "href": "imbalenced-tomek.html#python-examples",
    "title": "95  🏗️ Tomek Links",
    "section": "95.4 Python Examples",
    "text": "95.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>95</span>  <span class='chapter-title'>🏗️ Tomek Links</span>"
    ]
  },
  {
    "objectID": "imbalenced-cnn.html",
    "href": "imbalenced-cnn.html",
    "title": "96  🏗️ Condensed Nearest Neighbor",
    "section": "",
    "text": "96.1 Condensed Nearest Neighbor\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>🏗️ Condensed Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-cnn.html#pros-and-cons",
    "href": "imbalenced-cnn.html#pros-and-cons",
    "title": "96  🏗️ Condensed Nearest Neighbor",
    "section": "96.2 Pros and Cons",
    "text": "96.2 Pros and Cons\n\n96.2.1 Pros\n\n\n96.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>🏗️ Condensed Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-cnn.html#r-examples",
    "href": "imbalenced-cnn.html#r-examples",
    "title": "96  🏗️ Condensed Nearest Neighbor",
    "section": "96.3 R Examples",
    "text": "96.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>🏗️ Condensed Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-cnn.html#python-examples",
    "href": "imbalenced-cnn.html#python-examples",
    "title": "96  🏗️ Condensed Nearest Neighbor",
    "section": "96.4 Python Examples",
    "text": "96.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>96</span>  <span class='chapter-title'>🏗️ Condensed Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-enn.html",
    "href": "imbalenced-enn.html",
    "title": "97  🏗️ Edited Nearest Neighbor",
    "section": "",
    "text": "97.1 Edited Nearest Neighbor\nWIP\nThis includes Repeated Edited Nearest Neighbor\nAnd AllKNN\nAnd Neighborhood Cleaning Rule",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>🏗️ Edited Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-enn.html#pros-and-cons",
    "href": "imbalenced-enn.html#pros-and-cons",
    "title": "97  🏗️ Edited Nearest Neighbor",
    "section": "97.2 Pros and Cons",
    "text": "97.2 Pros and Cons\n\n97.2.1 Pros\n\n\n97.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>🏗️ Edited Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-enn.html#r-examples",
    "href": "imbalenced-enn.html#r-examples",
    "title": "97  🏗️ Edited Nearest Neighbor",
    "section": "97.3 R Examples",
    "text": "97.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>🏗️ Edited Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-enn.html#python-examples",
    "href": "imbalenced-enn.html#python-examples",
    "title": "97  🏗️ Edited Nearest Neighbor",
    "section": "97.4 Python Examples",
    "text": "97.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>97</span>  <span class='chapter-title'>🏗️ Edited Nearest Neighbor</span>"
    ]
  },
  {
    "objectID": "imbalenced-hardness-threshold.html",
    "href": "imbalenced-hardness-threshold.html",
    "title": "98  🏗️ Instance Hardness Threshold",
    "section": "",
    "text": "98.1 Instance Hardness Threshold\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>🏗️ Instance Hardness Threshold</span>"
    ]
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#pros-and-cons",
    "href": "imbalenced-hardness-threshold.html#pros-and-cons",
    "title": "98  🏗️ Instance Hardness Threshold",
    "section": "98.2 Pros and Cons",
    "text": "98.2 Pros and Cons\n\n98.2.1 Pros\n\n\n98.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>🏗️ Instance Hardness Threshold</span>"
    ]
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#r-examples",
    "href": "imbalenced-hardness-threshold.html#r-examples",
    "title": "98  🏗️ Instance Hardness Threshold",
    "section": "98.3 R Examples",
    "text": "98.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>🏗️ Instance Hardness Threshold</span>"
    ]
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#python-examples",
    "href": "imbalenced-hardness-threshold.html#python-examples",
    "title": "98  🏗️ Instance Hardness Threshold",
    "section": "98.4 Python Examples",
    "text": "98.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>98</span>  <span class='chapter-title'>🏗️ Instance Hardness Threshold</span>"
    ]
  },
  {
    "objectID": "imbalenced-one-sided.html",
    "href": "imbalenced-one-sided.html",
    "title": "99  🏗️ One Sided Selection",
    "section": "",
    "text": "99.1 One Sided Selection\nWIP",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>🏗️ One Sided Selection</span>"
    ]
  },
  {
    "objectID": "imbalenced-one-sided.html#pros-and-cons",
    "href": "imbalenced-one-sided.html#pros-and-cons",
    "title": "99  🏗️ One Sided Selection",
    "section": "99.2 Pros and Cons",
    "text": "99.2 Pros and Cons\n\n99.2.1 Pros\n\n\n99.2.2 Cons",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>🏗️ One Sided Selection</span>"
    ]
  },
  {
    "objectID": "imbalenced-one-sided.html#r-examples",
    "href": "imbalenced-one-sided.html#r-examples",
    "title": "99  🏗️ One Sided Selection",
    "section": "99.3 R Examples",
    "text": "99.3 R Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>🏗️ One Sided Selection</span>"
    ]
  },
  {
    "objectID": "imbalenced-one-sided.html#python-examples",
    "href": "imbalenced-one-sided.html#python-examples",
    "title": "99  🏗️ One Sided Selection",
    "section": "99.4 Python Examples",
    "text": "99.4 Python Examples",
    "crumbs": [
      "Imbalanced Data",
      "<span class='chapter-number'>99</span>  <span class='chapter-title'>🏗️ One Sided Selection</span>"
    ]
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "100  Miscellaneous Overview",
    "section": "",
    "text": "100.1 Miscellaneous Overview\nEven though we try to cover as many methods as we can in this book. There will always be more methods. This can be types of data that don’t fit in any of the other sections in this book, or because they are too domain-specific. Nevertheless, this chapter will cover a few methods and techniques we want to show, that don’t fit anywhere else. The purpose of these chapters is not solely to teach you about these methods directly, but to try to broaden where you try to find information",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>100</span>  <span class='chapter-title'>Miscellaneous Overview</span>"
    ]
  },
  {
    "objectID": "miscellaneous-id.html",
    "href": "miscellaneous-id.html",
    "title": "101  🏗️ IDs",
    "section": "",
    "text": "101.1 IDs\nWIP",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>🏗️ IDs</span>"
    ]
  },
  {
    "objectID": "miscellaneous-id.html#pros-and-cons",
    "href": "miscellaneous-id.html#pros-and-cons",
    "title": "101  🏗️ IDs",
    "section": "101.2 Pros and Cons",
    "text": "101.2 Pros and Cons\n\n101.2.1 Pros\n\n\n101.2.2 Cons",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>🏗️ IDs</span>"
    ]
  },
  {
    "objectID": "miscellaneous-id.html#r-examples",
    "href": "miscellaneous-id.html#r-examples",
    "title": "101  🏗️ IDs",
    "section": "101.3 R Examples",
    "text": "101.3 R Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>🏗️ IDs</span>"
    ]
  },
  {
    "objectID": "miscellaneous-id.html#python-examples",
    "href": "miscellaneous-id.html#python-examples",
    "title": "101  🏗️ IDs",
    "section": "101.4 Python Examples",
    "text": "101.4 Python Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>101</span>  <span class='chapter-title'>🏗️ IDs</span>"
    ]
  },
  {
    "objectID": "miscellaneous-color.html",
    "href": "miscellaneous-color.html",
    "title": "102  Colors",
    "section": "",
    "text": "102.1 Colors\nColor is an interesting attribute that on the surface appears simple, but can be analyzed in many different ways. One could treat them like categorical features, ignoring the inherent structure and connections that come with colors.\nColor names are strongly tied to language and culture, it is thus imperative that we know that when creating mappings between color names and a numerical representation.\nWe also have that there can be a lot of ambiguity in color names. You see this most prominently when buying paint, with each store or chain having its own, sometimes humourous, names for each shade they can produce. These names try to help customers distinguish between small differences in shades. Color names can also be quite broadly used. The color “green” could in context mean an exact hue, and in another context refers to all the colors seen in a forest. The latter being akin to categorical collapse. All of this is to say that we think about our data, to allow us to better extract the signal if it is there.\nAssuming we want to use precise mapping, then we can construct a table of color names and their corresponding precise representation. When working with computers, a commonly used way to present colors is using hex codes, which uses a six-digit hexadecimal number to represent a color. They are represented as #a8662b with the first 2 digits representing how red the color is, the second 2 digits representing how green it is, and the last 2 digits representing how blue it is. This gives us 16^6 = 16,777,216 unique colors, which isn’t enough to specify all possible colors but good enough for our use cases.\nThrough these hex codes, we already have some numeric representations that we can use for modeling. However, they may not be the most effective representation depending on what question we are trying to answer. This is where the idea of color spaces comes in. The one we have worked with is the RGB space, easy to use and understand but doesn’t translate well to notions that we typically care about like “How dark is this color”. Another color space that might be able to solve these problems better would be the HSL color space. This is a color space that uses 3 values to describe its color, by its hue (think rainbow) that takes values between 0 and 360, saturation which you can define as its colorfulness relative to its own brightness on a scale from 0 to 100, and lightness which tells you how bright it is compared to pure white on a scale from 0 to 100.\nViewing these colors in this color space allows us to create different features. We can now with relatively easy say if a color is close to blue, by looking at whether its hue is sufficiently close to 240. This could be expanded to any color on the hue wheel. We can likewise ask straightforward questions about saturation and lightness.\nImagine you wanted a feature to say “How close is this measured color to my reference color”, then you would need something called a perceptually uniform color space. These color spaces try to make Euclidian distances makes sense, examples include CIELAB and Oklab. The downside of these spaces is that each of the axes doesn’t contain any meaningful information.\nThese are by no means all we can do with colors as predictors, but it might spark some helpful creativity.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Colors</span>"
    ]
  },
  {
    "objectID": "miscellaneous-color.html#pros-and-cons",
    "href": "miscellaneous-color.html#pros-and-cons",
    "title": "102  Colors",
    "section": "102.2 Pros and Cons",
    "text": "102.2 Pros and Cons\n\n102.2.1 Pros\n\nUsing color spaces to write creative features can provide a significant impact\n\n\n\n102.2.2 Cons\n-Creating the mappings between color words and their numerical representation can be challenging",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Colors</span>"
    ]
  },
  {
    "objectID": "miscellaneous-color.html#r-examples",
    "href": "miscellaneous-color.html#r-examples",
    "title": "102  Colors",
    "section": "102.3 R Examples",
    "text": "102.3 R Examples\nWait for steps to do this.\n\nlibrary(tidymodels)\nlibrary(animalshelter)\nlongbeach |&gt;\n  mutate(primary_color = stringr::str_remove(primary_color, \" .*\")) |&gt;\n  count(primary_color, sort = TRUE) |&gt;\n  print(n = 20)",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Colors</span>"
    ]
  },
  {
    "objectID": "miscellaneous-color.html#python-examples",
    "href": "miscellaneous-color.html#python-examples",
    "title": "102  Colors",
    "section": "102.4 Python Examples",
    "text": "102.4 Python Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>102</span>  <span class='chapter-title'>Colors</span>"
    ]
  },
  {
    "objectID": "miscellaneous-zipcodes.html",
    "href": "miscellaneous-zipcodes.html",
    "title": "103  🏗️ Zip Codes",
    "section": "",
    "text": "103.1 Zip Codes\nWIP",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>🏗️ Zip Codes</span>"
    ]
  },
  {
    "objectID": "miscellaneous-zipcodes.html#pros-and-cons",
    "href": "miscellaneous-zipcodes.html#pros-and-cons",
    "title": "103  🏗️ Zip Codes",
    "section": "103.2 Pros and Cons",
    "text": "103.2 Pros and Cons\n\n103.2.1 Pros\n\n\n103.2.2 Cons",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>🏗️ Zip Codes</span>"
    ]
  },
  {
    "objectID": "miscellaneous-zipcodes.html#r-examples",
    "href": "miscellaneous-zipcodes.html#r-examples",
    "title": "103  🏗️ Zip Codes",
    "section": "103.3 R Examples",
    "text": "103.3 R Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>🏗️ Zip Codes</span>"
    ]
  },
  {
    "objectID": "miscellaneous-zipcodes.html#python-examples",
    "href": "miscellaneous-zipcodes.html#python-examples",
    "title": "103  🏗️ Zip Codes",
    "section": "103.4 Python Examples",
    "text": "103.4 Python Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>103</span>  <span class='chapter-title'>🏗️ Zip Codes</span>"
    ]
  },
  {
    "objectID": "miscellaneous-email.html",
    "href": "miscellaneous-email.html",
    "title": "104  🏗️ Emails",
    "section": "",
    "text": "104.1 Emails\nWIP\nCheck for validity\nCheck for parts - username - domain - plus addressing or subaddressing\ncharacter level statistics. Mostly counts - digits - dots - letters - casing\nCheck for matching information. requires matching information - birthdays - names",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>🏗️ Emails</span>"
    ]
  },
  {
    "objectID": "miscellaneous-email.html#pros-and-cons",
    "href": "miscellaneous-email.html#pros-and-cons",
    "title": "104  🏗️ Emails",
    "section": "104.2 Pros and Cons",
    "text": "104.2 Pros and Cons\n\n104.2.1 Pros\n\n\n104.2.2 Cons",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>🏗️ Emails</span>"
    ]
  },
  {
    "objectID": "miscellaneous-email.html#r-examples",
    "href": "miscellaneous-email.html#r-examples",
    "title": "104  🏗️ Emails",
    "section": "104.3 R Examples",
    "text": "104.3 R Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>🏗️ Emails</span>"
    ]
  },
  {
    "objectID": "miscellaneous-email.html#python-examples",
    "href": "miscellaneous-email.html#python-examples",
    "title": "104  🏗️ Emails",
    "section": "104.4 Python Examples",
    "text": "104.4 Python Examples",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>104</span>  <span class='chapter-title'>🏗️ Emails</span>"
    ]
  },
  {
    "objectID": "spatial.html",
    "href": "spatial.html",
    "title": "105  Spatial Overview",
    "section": "",
    "text": "105.1 Spatial Overview\nWhen we talk about spatial and geospatial feature engineering, we want to focus on the transformation and enrichment of the data set, based on spatial information. Typically this will be longitude and latitude-based, with the additional information being based on areas and regions such as cities or countries.\nWhat sets these methods apart from some of the other methods we see in this book, is that they almost always require a reference data set to be able to perform the calculations. If you want to find the closest city to an observation, you need a data set of all the cities and their location. For all the methods in this section, the reader is expected to know how to gather this reference material for their problem.\nWe will split this data up into two types of methods, depending on your spatial information. point based methods and shape based methods.\nIn point-based methods, you know the location of your observation, and you calculate where it is in relationship to something else. You could look at distances by finding the distance to a fixed point, or multiple points as covered in Chapter 106. You could find the nearest of something as covered in Chapter 107. These two methods are different sides of the same coin. Another thing we could do is count the number of occurrences within a given distance or region. This is covered in Chapter 108. By knowing the location of something we are also able to query certain types of information such as “height from sea”, “rainfall in inches in 2000” and so on. We cover these types of methods in Chapter 109. We can also expand some of these concepts and look at spatial embeddings, we will be covered in Chapter 110.\nIn shape-based methods, you don’t just have the positioning of your observation, but also its shape. This can be any shape; line, polygon, or circle. The methods seen in point-based methods can be applied to shape-based methods, we just need to be a little more careful when performing the calculations. Since we are given the shape of our observation, there are characteristics we can extract from those that might be useful. We look at how we can incorporate that information in Chapter 111.",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>105</span>  <span class='chapter-title'>Spatial Overview</span>"
    ]
  },
  {
    "objectID": "spatial-distance.html",
    "href": "spatial-distance.html",
    "title": "106  🏗️ Spatial Distance",
    "section": "",
    "text": "106.1 Spatial Distance\nWIP",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>🏗️ Spatial Distance</span>"
    ]
  },
  {
    "objectID": "spatial-distance.html#pros-and-cons",
    "href": "spatial-distance.html#pros-and-cons",
    "title": "106  🏗️ Spatial Distance",
    "section": "106.2 Pros and Cons",
    "text": "106.2 Pros and Cons\n\n106.2.1 Pros\n\n\n106.2.2 Cons",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>🏗️ Spatial Distance</span>"
    ]
  },
  {
    "objectID": "spatial-distance.html#r-examples",
    "href": "spatial-distance.html#r-examples",
    "title": "106  🏗️ Spatial Distance",
    "section": "106.3 R Examples",
    "text": "106.3 R Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>🏗️ Spatial Distance</span>"
    ]
  },
  {
    "objectID": "spatial-distance.html#python-examples",
    "href": "spatial-distance.html#python-examples",
    "title": "106  🏗️ Spatial Distance",
    "section": "106.4 Python Examples",
    "text": "106.4 Python Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>106</span>  <span class='chapter-title'>🏗️ Spatial Distance</span>"
    ]
  },
  {
    "objectID": "spatial-nearest.html",
    "href": "spatial-nearest.html",
    "title": "107  🏗️ Spatial Nearest",
    "section": "",
    "text": "107.1 Spatial Nearest\nWIP",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>🏗️ Spatial Nearest</span>"
    ]
  },
  {
    "objectID": "spatial-nearest.html#pros-and-cons",
    "href": "spatial-nearest.html#pros-and-cons",
    "title": "107  🏗️ Spatial Nearest",
    "section": "107.2 Pros and Cons",
    "text": "107.2 Pros and Cons\n\n107.2.1 Pros\n\n\n107.2.2 Cons",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>🏗️ Spatial Nearest</span>"
    ]
  },
  {
    "objectID": "spatial-nearest.html#r-examples",
    "href": "spatial-nearest.html#r-examples",
    "title": "107  🏗️ Spatial Nearest",
    "section": "107.3 R Examples",
    "text": "107.3 R Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>🏗️ Spatial Nearest</span>"
    ]
  },
  {
    "objectID": "spatial-nearest.html#python-examples",
    "href": "spatial-nearest.html#python-examples",
    "title": "107  🏗️ Spatial Nearest",
    "section": "107.4 Python Examples",
    "text": "107.4 Python Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>107</span>  <span class='chapter-title'>🏗️ Spatial Nearest</span>"
    ]
  },
  {
    "objectID": "spatial-count.html",
    "href": "spatial-count.html",
    "title": "108  🏗️ Spatial Count",
    "section": "",
    "text": "108.1 Spatial Count\nWIP",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>🏗️ Spatial Count</span>"
    ]
  },
  {
    "objectID": "spatial-count.html#pros-and-cons",
    "href": "spatial-count.html#pros-and-cons",
    "title": "108  🏗️ Spatial Count",
    "section": "108.2 Pros and Cons",
    "text": "108.2 Pros and Cons\n\n108.2.1 Pros\n\n\n108.2.2 Cons",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>🏗️ Spatial Count</span>"
    ]
  },
  {
    "objectID": "spatial-count.html#r-examples",
    "href": "spatial-count.html#r-examples",
    "title": "108  🏗️ Spatial Count",
    "section": "108.3 R Examples",
    "text": "108.3 R Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>🏗️ Spatial Count</span>"
    ]
  },
  {
    "objectID": "spatial-count.html#python-examples",
    "href": "spatial-count.html#python-examples",
    "title": "108  🏗️ Spatial Count",
    "section": "108.4 Python Examples",
    "text": "108.4 Python Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>108</span>  <span class='chapter-title'>🏗️ Spatial Count</span>"
    ]
  },
  {
    "objectID": "spatial-query.html",
    "href": "spatial-query.html",
    "title": "109  🏗️ Spatial Query",
    "section": "",
    "text": "109.1 Spatial Query\nWIP",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>🏗️ Spatial Query</span>"
    ]
  },
  {
    "objectID": "spatial-query.html#pros-and-cons",
    "href": "spatial-query.html#pros-and-cons",
    "title": "109  🏗️ Spatial Query",
    "section": "109.2 Pros and Cons",
    "text": "109.2 Pros and Cons\n\n109.2.1 Pros\n\n\n109.2.2 Cons",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>🏗️ Spatial Query</span>"
    ]
  },
  {
    "objectID": "spatial-query.html#r-examples",
    "href": "spatial-query.html#r-examples",
    "title": "109  🏗️ Spatial Query",
    "section": "109.3 R Examples",
    "text": "109.3 R Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>🏗️ Spatial Query</span>"
    ]
  },
  {
    "objectID": "spatial-query.html#python-examples",
    "href": "spatial-query.html#python-examples",
    "title": "109  🏗️ Spatial Query",
    "section": "109.4 Python Examples",
    "text": "109.4 Python Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>109</span>  <span class='chapter-title'>🏗️ Spatial Query</span>"
    ]
  },
  {
    "objectID": "spatial-embedding.html",
    "href": "spatial-embedding.html",
    "title": "110  🏗️ Spatial Embedding",
    "section": "",
    "text": "110.1 Spatial Embedding\nWIP",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>🏗️ Spatial Embedding</span>"
    ]
  },
  {
    "objectID": "spatial-embedding.html#pros-and-cons",
    "href": "spatial-embedding.html#pros-and-cons",
    "title": "110  🏗️ Spatial Embedding",
    "section": "110.2 Pros and Cons",
    "text": "110.2 Pros and Cons\n\n110.2.1 Pros\n\n\n110.2.2 Cons",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>🏗️ Spatial Embedding</span>"
    ]
  },
  {
    "objectID": "spatial-embedding.html#r-examples",
    "href": "spatial-embedding.html#r-examples",
    "title": "110  🏗️ Spatial Embedding",
    "section": "110.3 R Examples",
    "text": "110.3 R Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>🏗️ Spatial Embedding</span>"
    ]
  },
  {
    "objectID": "spatial-embedding.html#python-examples",
    "href": "spatial-embedding.html#python-examples",
    "title": "110  🏗️ Spatial Embedding",
    "section": "110.4 Python Examples",
    "text": "110.4 Python Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>110</span>  <span class='chapter-title'>🏗️ Spatial Embedding</span>"
    ]
  },
  {
    "objectID": "spatial-characteristics.html",
    "href": "spatial-characteristics.html",
    "title": "111  🏗️ Spatial Characteristics",
    "section": "",
    "text": "111.1 Spatial Characteristics\nWIP",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>🏗️ Spatial Characteristics</span>"
    ]
  },
  {
    "objectID": "spatial-characteristics.html#pros-and-cons",
    "href": "spatial-characteristics.html#pros-and-cons",
    "title": "111  🏗️ Spatial Characteristics",
    "section": "111.2 Pros and Cons",
    "text": "111.2 Pros and Cons\n\n111.2.1 Pros\n\n\n111.2.2 Cons",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>🏗️ Spatial Characteristics</span>"
    ]
  },
  {
    "objectID": "spatial-characteristics.html#r-examples",
    "href": "spatial-characteristics.html#r-examples",
    "title": "111  🏗️ Spatial Characteristics",
    "section": "111.3 R Examples",
    "text": "111.3 R Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>🏗️ Spatial Characteristics</span>"
    ]
  },
  {
    "objectID": "spatial-characteristics.html#python-examples",
    "href": "spatial-characteristics.html#python-examples",
    "title": "111  🏗️ Spatial Characteristics",
    "section": "111.4 Python Examples",
    "text": "111.4 Python Examples",
    "crumbs": [
      "Spatial",
      "<span class='chapter-number'>111</span>  <span class='chapter-title'>🏗️ Spatial Characteristics</span>"
    ]
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "112  Time-series Overview",
    "section": "",
    "text": "112.1 Time-series Overview\nA very common type of data is time-series data. This is data where the observations are taken across time with time stamps. This data will inherently be correlated with itself as the same measurement for the same unit likely isn’t going to change too much. Time series data is typically modeled using different methods than other predictive modeling for this reason.\nNevertheless, there are still some methods that will be useful for us. It is important to note that there is a time component to this data, which is what we are trying to predict. The observations can happen at regular intervals, once a day, or irregular intervals, each time the engine errors. Different types of data use different types of models and make the feature engineering a little different depending on what you are trying to do.\nThis chapter assumes the user knows how to use time series data and the precautions that are needed when working with data of this type.\nOne area of work is modifying the sequence itself. Some of these transformations can be taken from the Numeric section. However, some metrics are specific to time series data. Likewise, some metrics are similar to what we do in Missing section and Outliers section, but special care needs to be taken with time series data.\nAnother area of methods is where we extract or modify the sequence or its data to create new variables, this is done by considering the time component. This will result in breaking about the time component in a decomposition kind of way, or by getting information out of the values themselves. One such example of the latter is found in the datetime section. Particularly holiday extraction.",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>112</span>  <span class='chapter-title'>Time-series Overview</span>"
    ]
  },
  {
    "objectID": "time-series-smooth.html",
    "href": "time-series-smooth.html",
    "title": "113  🏗️ Smoothing",
    "section": "",
    "text": "113.1 Smoothing\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>🏗️ Smoothing</span>"
    ]
  },
  {
    "objectID": "time-series-smooth.html#pros-and-cons",
    "href": "time-series-smooth.html#pros-and-cons",
    "title": "113  🏗️ Smoothing",
    "section": "113.2 Pros and Cons",
    "text": "113.2 Pros and Cons\n\n113.2.1 Pros\n\n\n113.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>🏗️ Smoothing</span>"
    ]
  },
  {
    "objectID": "time-series-smooth.html#r-examples",
    "href": "time-series-smooth.html#r-examples",
    "title": "113  🏗️ Smoothing",
    "section": "113.3 R Examples",
    "text": "113.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>🏗️ Smoothing</span>"
    ]
  },
  {
    "objectID": "time-series-smooth.html#python-examples",
    "href": "time-series-smooth.html#python-examples",
    "title": "113  🏗️ Smoothing",
    "section": "113.4 Python Examples",
    "text": "113.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>113</span>  <span class='chapter-title'>🏗️ Smoothing</span>"
    ]
  },
  {
    "objectID": "time-series-sliding.html",
    "href": "time-series-sliding.html",
    "title": "114  🏗️ Sliding",
    "section": "",
    "text": "114.1 Sliding\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>🏗️ Sliding</span>"
    ]
  },
  {
    "objectID": "time-series-sliding.html#pros-and-cons",
    "href": "time-series-sliding.html#pros-and-cons",
    "title": "114  🏗️ Sliding",
    "section": "114.2 Pros and Cons",
    "text": "114.2 Pros and Cons\n\n114.2.1 Pros\n\n\n114.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>🏗️ Sliding</span>"
    ]
  },
  {
    "objectID": "time-series-sliding.html#r-examples",
    "href": "time-series-sliding.html#r-examples",
    "title": "114  🏗️ Sliding",
    "section": "114.3 R Examples",
    "text": "114.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>🏗️ Sliding</span>"
    ]
  },
  {
    "objectID": "time-series-sliding.html#python-examples",
    "href": "time-series-sliding.html#python-examples",
    "title": "114  🏗️ Sliding",
    "section": "114.4 Python Examples",
    "text": "114.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>114</span>  <span class='chapter-title'>🏗️ Sliding</span>"
    ]
  },
  {
    "objectID": "time-series-log-interval.html",
    "href": "time-series-log-interval.html",
    "title": "115  🏗️ Log Interval",
    "section": "",
    "text": "115.1 Log Interval\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>115</span>  <span class='chapter-title'>🏗️ Log Interval</span>"
    ]
  },
  {
    "objectID": "time-series-log-interval.html#pros-and-cons",
    "href": "time-series-log-interval.html#pros-and-cons",
    "title": "115  🏗️ Log Interval",
    "section": "115.2 Pros and Cons",
    "text": "115.2 Pros and Cons\n\n115.2.1 Pros\n\n\n115.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>115</span>  <span class='chapter-title'>🏗️ Log Interval</span>"
    ]
  },
  {
    "objectID": "time-series-log-interval.html#r-examples",
    "href": "time-series-log-interval.html#r-examples",
    "title": "115  🏗️ Log Interval",
    "section": "115.3 R Examples",
    "text": "115.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>115</span>  <span class='chapter-title'>🏗️ Log Interval</span>"
    ]
  },
  {
    "objectID": "time-series-log-interval.html#python-examples",
    "href": "time-series-log-interval.html#python-examples",
    "title": "115  🏗️ Log Interval",
    "section": "115.4 Python Examples",
    "text": "115.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>115</span>  <span class='chapter-title'>🏗️ Log Interval</span>"
    ]
  },
  {
    "objectID": "time-series-missing.html",
    "href": "time-series-missing.html",
    "title": "116  🏗️ Time series Missing values",
    "section": "",
    "text": "116.1 Time series Missing values\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>🏗️ Time series Missing values</span>"
    ]
  },
  {
    "objectID": "time-series-missing.html#pros-and-cons",
    "href": "time-series-missing.html#pros-and-cons",
    "title": "116  🏗️ Time series Missing values",
    "section": "116.2 Pros and Cons",
    "text": "116.2 Pros and Cons\n\n116.2.1 Pros\n\n\n116.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>🏗️ Time series Missing values</span>"
    ]
  },
  {
    "objectID": "time-series-missing.html#r-examples",
    "href": "time-series-missing.html#r-examples",
    "title": "116  🏗️ Time series Missing values",
    "section": "116.3 R Examples",
    "text": "116.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>🏗️ Time series Missing values</span>"
    ]
  },
  {
    "objectID": "time-series-missing.html#python-examples",
    "href": "time-series-missing.html#python-examples",
    "title": "116  🏗️ Time series Missing values",
    "section": "116.4 Python Examples",
    "text": "116.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>116</span>  <span class='chapter-title'>🏗️ Time series Missing values</span>"
    ]
  },
  {
    "objectID": "time-series-outliers.html",
    "href": "time-series-outliers.html",
    "title": "117  🏗️ Time Series outliers",
    "section": "",
    "text": "117.1 Time Series outliers\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>🏗️ Time Series outliers</span>"
    ]
  },
  {
    "objectID": "time-series-outliers.html#pros-and-cons",
    "href": "time-series-outliers.html#pros-and-cons",
    "title": "117  🏗️ Time Series outliers",
    "section": "117.2 Pros and Cons",
    "text": "117.2 Pros and Cons\n\n117.2.1 Pros\n\n\n117.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>🏗️ Time Series outliers</span>"
    ]
  },
  {
    "objectID": "time-series-outliers.html#r-examples",
    "href": "time-series-outliers.html#r-examples",
    "title": "117  🏗️ Time Series outliers",
    "section": "117.3 R Examples",
    "text": "117.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>🏗️ Time Series outliers</span>"
    ]
  },
  {
    "objectID": "time-series-outliers.html#python-examples",
    "href": "time-series-outliers.html#python-examples",
    "title": "117  🏗️ Time Series outliers",
    "section": "117.4 Python Examples",
    "text": "117.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>117</span>  <span class='chapter-title'>🏗️ Time Series outliers</span>"
    ]
  },
  {
    "objectID": "time-series-diff.html",
    "href": "time-series-diff.html",
    "title": "118  🏗️ Differences",
    "section": "",
    "text": "118.1 Differences\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>118</span>  <span class='chapter-title'>🏗️ Differences</span>"
    ]
  },
  {
    "objectID": "time-series-diff.html#pros-and-cons",
    "href": "time-series-diff.html#pros-and-cons",
    "title": "118  🏗️ Differences",
    "section": "118.2 Pros and Cons",
    "text": "118.2 Pros and Cons\n\n118.2.1 Pros\n\n\n118.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>118</span>  <span class='chapter-title'>🏗️ Differences</span>"
    ]
  },
  {
    "objectID": "time-series-diff.html#r-examples",
    "href": "time-series-diff.html#r-examples",
    "title": "118  🏗️ Differences",
    "section": "118.3 R Examples",
    "text": "118.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>118</span>  <span class='chapter-title'>🏗️ Differences</span>"
    ]
  },
  {
    "objectID": "time-series-diff.html#python-examples",
    "href": "time-series-diff.html#python-examples",
    "title": "118  🏗️ Differences",
    "section": "118.4 Python Examples",
    "text": "118.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>118</span>  <span class='chapter-title'>🏗️ Differences</span>"
    ]
  },
  {
    "objectID": "time-series-lag.html",
    "href": "time-series-lag.html",
    "title": "119  🏗️ Lagging Features",
    "section": "",
    "text": "119.1 Lagging Features\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>119</span>  <span class='chapter-title'>🏗️ Lagging Features</span>"
    ]
  },
  {
    "objectID": "time-series-lag.html#pros-and-cons",
    "href": "time-series-lag.html#pros-and-cons",
    "title": "119  🏗️ Lagging Features",
    "section": "119.2 Pros and Cons",
    "text": "119.2 Pros and Cons\n\n119.2.1 Pros\n\n\n119.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>119</span>  <span class='chapter-title'>🏗️ Lagging Features</span>"
    ]
  },
  {
    "objectID": "time-series-lag.html#r-examples",
    "href": "time-series-lag.html#r-examples",
    "title": "119  🏗️ Lagging Features",
    "section": "119.3 R Examples",
    "text": "119.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>119</span>  <span class='chapter-title'>🏗️ Lagging Features</span>"
    ]
  },
  {
    "objectID": "time-series-lag.html#python-examples",
    "href": "time-series-lag.html#python-examples",
    "title": "119  🏗️ Lagging Features",
    "section": "119.4 Python Examples",
    "text": "119.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>119</span>  <span class='chapter-title'>🏗️ Lagging Features</span>"
    ]
  },
  {
    "objectID": "time-series-rolling-window.html",
    "href": "time-series-rolling-window.html",
    "title": "120  🏗️ Rolling Window",
    "section": "",
    "text": "120.1 Rolling Window\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>120</span>  <span class='chapter-title'>🏗️ Rolling Window</span>"
    ]
  },
  {
    "objectID": "time-series-rolling-window.html#pros-and-cons",
    "href": "time-series-rolling-window.html#pros-and-cons",
    "title": "120  🏗️ Rolling Window",
    "section": "120.2 Pros and Cons",
    "text": "120.2 Pros and Cons\n\n120.2.1 Pros\n\n\n120.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>120</span>  <span class='chapter-title'>🏗️ Rolling Window</span>"
    ]
  },
  {
    "objectID": "time-series-rolling-window.html#r-examples",
    "href": "time-series-rolling-window.html#r-examples",
    "title": "120  🏗️ Rolling Window",
    "section": "120.3 R Examples",
    "text": "120.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>120</span>  <span class='chapter-title'>🏗️ Rolling Window</span>"
    ]
  },
  {
    "objectID": "time-series-rolling-window.html#python-examples",
    "href": "time-series-rolling-window.html#python-examples",
    "title": "120  🏗️ Rolling Window",
    "section": "120.4 Python Examples",
    "text": "120.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>120</span>  <span class='chapter-title'>🏗️ Rolling Window</span>"
    ]
  },
  {
    "objectID": "time-series-expanding-window.html",
    "href": "time-series-expanding-window.html",
    "title": "121  🏗️ Expanding Window",
    "section": "",
    "text": "121.1 Expanding Window\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>121</span>  <span class='chapter-title'>🏗️ Expanding Window</span>"
    ]
  },
  {
    "objectID": "time-series-expanding-window.html#pros-and-cons",
    "href": "time-series-expanding-window.html#pros-and-cons",
    "title": "121  🏗️ Expanding Window",
    "section": "121.2 Pros and Cons",
    "text": "121.2 Pros and Cons\n\n121.2.1 Pros\n\n\n121.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>121</span>  <span class='chapter-title'>🏗️ Expanding Window</span>"
    ]
  },
  {
    "objectID": "time-series-expanding-window.html#r-examples",
    "href": "time-series-expanding-window.html#r-examples",
    "title": "121  🏗️ Expanding Window",
    "section": "121.3 R Examples",
    "text": "121.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>121</span>  <span class='chapter-title'>🏗️ Expanding Window</span>"
    ]
  },
  {
    "objectID": "time-series-expanding-window.html#python-examples",
    "href": "time-series-expanding-window.html#python-examples",
    "title": "121  🏗️ Expanding Window",
    "section": "121.4 Python Examples",
    "text": "121.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>121</span>  <span class='chapter-title'>🏗️ Expanding Window</span>"
    ]
  },
  {
    "objectID": "time-series-fourier.html",
    "href": "time-series-fourier.html",
    "title": "122  🏗️ Fourier Features",
    "section": "",
    "text": "122.1 Fourier Features\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>122</span>  <span class='chapter-title'>🏗️ Fourier Features</span>"
    ]
  },
  {
    "objectID": "time-series-fourier.html#pros-and-cons",
    "href": "time-series-fourier.html#pros-and-cons",
    "title": "122  🏗️ Fourier Features",
    "section": "122.2 Pros and Cons",
    "text": "122.2 Pros and Cons\n\n122.2.1 Pros\n\n\n122.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>122</span>  <span class='chapter-title'>🏗️ Fourier Features</span>"
    ]
  },
  {
    "objectID": "time-series-fourier.html#r-examples",
    "href": "time-series-fourier.html#r-examples",
    "title": "122  🏗️ Fourier Features",
    "section": "122.3 R Examples",
    "text": "122.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>122</span>  <span class='chapter-title'>🏗️ Fourier Features</span>"
    ]
  },
  {
    "objectID": "time-series-fourier.html#python-examples",
    "href": "time-series-fourier.html#python-examples",
    "title": "122  🏗️ Fourier Features",
    "section": "122.4 Python Examples",
    "text": "122.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>122</span>  <span class='chapter-title'>🏗️ Fourier Features</span>"
    ]
  },
  {
    "objectID": "time-series-wavelet.html",
    "href": "time-series-wavelet.html",
    "title": "123  🏗️ Wavelet",
    "section": "",
    "text": "123.1 Wavelet\nWIP",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>123</span>  <span class='chapter-title'>🏗️ Wavelet</span>"
    ]
  },
  {
    "objectID": "time-series-wavelet.html#pros-and-cons",
    "href": "time-series-wavelet.html#pros-and-cons",
    "title": "123  🏗️ Wavelet",
    "section": "123.2 Pros and Cons",
    "text": "123.2 Pros and Cons\n\n123.2.1 Pros\n\n\n123.2.2 Cons",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>123</span>  <span class='chapter-title'>🏗️ Wavelet</span>"
    ]
  },
  {
    "objectID": "time-series-wavelet.html#r-examples",
    "href": "time-series-wavelet.html#r-examples",
    "title": "123  🏗️ Wavelet",
    "section": "123.3 R Examples",
    "text": "123.3 R Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>123</span>  <span class='chapter-title'>🏗️ Wavelet</span>"
    ]
  },
  {
    "objectID": "time-series-wavelet.html#python-examples",
    "href": "time-series-wavelet.html#python-examples",
    "title": "123  🏗️ Wavelet",
    "section": "123.4 Python Examples",
    "text": "123.4 Python Examples",
    "crumbs": [
      "Time-Series Data",
      "<span class='chapter-number'>123</span>  <span class='chapter-title'>🏗️ Wavelet</span>"
    ]
  },
  {
    "objectID": "image.html",
    "href": "image.html",
    "title": "124  Image Overview",
    "section": "",
    "text": "124.1 Image Overview\nImage data is quite dissimilar to anything we have seen so far in this book. And will need a whole different set of techniques to deal with them. For Most of this book, the unit of data was 1 value, such as 4, 7, or 0.3562 for numeric, and “cat” and “dog” for categorical. Text data as seen in the Text section, is being treated as a series of tokens.\nImage data on the other hand is represented as an array of numbers, typically integers with varying dimensions. Each image is comprised of pixels, these pixels are laid out in a rectangular grid. For each pixel value, you typically have between 1 and 4 values. These are called channels. 1 channel is used for gray-scale images. 3 channels are used for color images as they have a red channel, a green channel and a blue channel. Lastly, sometimes there is also a fourth channel for opacity.\nThis means that for a 500-pixel by 1000-pixel color image, we have 500 * 1000 * 3 = 1500000 values. This is quite a lot of data, and hopefully, we will be able to squeeze some out of it.\nThe preprocessing techniques for images can be split into a couple of categories. All of which will be covered.",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>124</span>  <span class='chapter-title'>Image Overview</span>"
    ]
  },
  {
    "objectID": "image.html#feature-extraction",
    "href": "image.html#feature-extraction",
    "title": "124  Image Overview",
    "section": "124.2 Feature Extraction",
    "text": "124.2 Feature Extraction\nIn the extraction setting, we take the images and try to extract smaller, hopefully smaller vectors of information. These could be simple statistics or larger and more complicated methods. One does not need to do this right away, and sometimes it is beneficial to apply some of the image modification methods below before doing the extraction.\n\nEdge detection and corner detection\nTexture Analysis",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>124</span>  <span class='chapter-title'>Image Overview</span>"
    ]
  },
  {
    "objectID": "image.html#image-modification",
    "href": "image.html#image-modification",
    "title": "124  Image Overview",
    "section": "124.3 Image Modification",
    "text": "124.3 Image Modification\nSometimes the images you get will not be in the best shape for your task at hand. This could be for various reasons. Applying color changes of different kinds can help highlight the important parts of the image, such that later preprocessing steps or models have an easier time picking up on it. Likewise, you might need to scale the data to help the model and well as reduce noise. Lastly, you will most likely need to resize your images as many deep-learning image modes work on fixed input sizes.\n\nGrayscale Conversion\nColor Modifications\nNoise Reduction\nValue Normalization\nResizing",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>124</span>  <span class='chapter-title'>Image Overview</span>"
    ]
  },
  {
    "objectID": "image.html#augmentation",
    "href": "image.html#augmentation",
    "title": "124  Image Overview",
    "section": "124.4 Augmentation",
    "text": "124.4 Augmentation\nA common trick when working with image data is to do augmentation. What we mean by that, is that we do different kinds of transformations to generate new images that contain the same information but in different ways. It creates a larger data set. With the hopes of increasing the performance and generalization. Being able to detect cat pictures regardless if they are centered in the image or not.\n\nChanging brightness\nShifting, Flipping, Rotation\nCropping and Scaling",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>124</span>  <span class='chapter-title'>Image Overview</span>"
    ]
  },
  {
    "objectID": "image.html#embeddings",
    "href": "image.html#embeddings",
    "title": "124  Image Overview",
    "section": "124.5 Embeddings",
    "text": "124.5 Embeddings\nWe can also take advantage of transfer learning. People have fit image deep learning models on many images before us. And some of these trained models can be reused for us. We will look at that in Chapter 135.",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>124</span>  <span class='chapter-title'>Image Overview</span>"
    ]
  },
  {
    "objectID": "image-edge-corner.html",
    "href": "image-edge-corner.html",
    "title": "125  🏗️ Edge and corner detection",
    "section": "",
    "text": "125.1 Edge and corner detection\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>125</span>  <span class='chapter-title'>🏗️ Edge and corner detection</span>"
    ]
  },
  {
    "objectID": "image-edge-corner.html#pros-and-cons",
    "href": "image-edge-corner.html#pros-and-cons",
    "title": "125  🏗️ Edge and corner detection",
    "section": "125.2 Pros and Cons",
    "text": "125.2 Pros and Cons\n\n125.2.1 Pros\n\n\n125.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>125</span>  <span class='chapter-title'>🏗️ Edge and corner detection</span>"
    ]
  },
  {
    "objectID": "image-edge-corner.html#r-examples",
    "href": "image-edge-corner.html#r-examples",
    "title": "125  🏗️ Edge and corner detection",
    "section": "125.3 R Examples",
    "text": "125.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>125</span>  <span class='chapter-title'>🏗️ Edge and corner detection</span>"
    ]
  },
  {
    "objectID": "image-edge-corner.html#python-examples",
    "href": "image-edge-corner.html#python-examples",
    "title": "125  🏗️ Edge and corner detection",
    "section": "125.4 Python Examples",
    "text": "125.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>125</span>  <span class='chapter-title'>🏗️ Edge and corner detection</span>"
    ]
  },
  {
    "objectID": "image-texture.html",
    "href": "image-texture.html",
    "title": "126  🏗️ Texture Analysis",
    "section": "",
    "text": "126.1 Texture Analysis\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>126</span>  <span class='chapter-title'>🏗️ Texture Analysis</span>"
    ]
  },
  {
    "objectID": "image-texture.html#pros-and-cons",
    "href": "image-texture.html#pros-and-cons",
    "title": "126  🏗️ Texture Analysis",
    "section": "126.2 Pros and Cons",
    "text": "126.2 Pros and Cons\n\n126.2.1 Pros\n\n\n126.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>126</span>  <span class='chapter-title'>🏗️ Texture Analysis</span>"
    ]
  },
  {
    "objectID": "image-texture.html#r-examples",
    "href": "image-texture.html#r-examples",
    "title": "126  🏗️ Texture Analysis",
    "section": "126.3 R Examples",
    "text": "126.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>126</span>  <span class='chapter-title'>🏗️ Texture Analysis</span>"
    ]
  },
  {
    "objectID": "image-texture.html#python-examples",
    "href": "image-texture.html#python-examples",
    "title": "126  🏗️ Texture Analysis",
    "section": "126.4 Python Examples",
    "text": "126.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>126</span>  <span class='chapter-title'>🏗️ Texture Analysis</span>"
    ]
  },
  {
    "objectID": "image-grayscale.html",
    "href": "image-grayscale.html",
    "title": "127  🏗️ Greyscale conversion",
    "section": "",
    "text": "127.1 Greyscale conversion\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>127</span>  <span class='chapter-title'>🏗️ Greyscale conversion</span>"
    ]
  },
  {
    "objectID": "image-grayscale.html#pros-and-cons",
    "href": "image-grayscale.html#pros-and-cons",
    "title": "127  🏗️ Greyscale conversion",
    "section": "127.2 Pros and Cons",
    "text": "127.2 Pros and Cons\n\n127.2.1 Pros\n\n\n127.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>127</span>  <span class='chapter-title'>🏗️ Greyscale conversion</span>"
    ]
  },
  {
    "objectID": "image-grayscale.html#r-examples",
    "href": "image-grayscale.html#r-examples",
    "title": "127  🏗️ Greyscale conversion",
    "section": "127.3 R Examples",
    "text": "127.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>127</span>  <span class='chapter-title'>🏗️ Greyscale conversion</span>"
    ]
  },
  {
    "objectID": "image-grayscale.html#python-examples",
    "href": "image-grayscale.html#python-examples",
    "title": "127  🏗️ Greyscale conversion",
    "section": "127.4 Python Examples",
    "text": "127.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>127</span>  <span class='chapter-title'>🏗️ Greyscale conversion</span>"
    ]
  },
  {
    "objectID": "image-colors.html",
    "href": "image-colors.html",
    "title": "128  🏗️ Color Modifications",
    "section": "",
    "text": "128.1 Color Modifications\n(includes brightness and contrast)\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>128</span>  <span class='chapter-title'>🏗️ Color Modifications</span>"
    ]
  },
  {
    "objectID": "image-colors.html#pros-and-cons",
    "href": "image-colors.html#pros-and-cons",
    "title": "128  🏗️ Color Modifications",
    "section": "128.2 Pros and Cons",
    "text": "128.2 Pros and Cons\n\n128.2.1 Pros\n\n\n128.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>128</span>  <span class='chapter-title'>🏗️ Color Modifications</span>"
    ]
  },
  {
    "objectID": "image-colors.html#r-examples",
    "href": "image-colors.html#r-examples",
    "title": "128  🏗️ Color Modifications",
    "section": "128.3 R Examples",
    "text": "128.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>128</span>  <span class='chapter-title'>🏗️ Color Modifications</span>"
    ]
  },
  {
    "objectID": "image-colors.html#python-examples",
    "href": "image-colors.html#python-examples",
    "title": "128  🏗️ Color Modifications",
    "section": "128.4 Python Examples",
    "text": "128.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>128</span>  <span class='chapter-title'>🏗️ Color Modifications</span>"
    ]
  },
  {
    "objectID": "image-noise.html",
    "href": "image-noise.html",
    "title": "129  🏗️ Noise Reduction",
    "section": "",
    "text": "129.1 Noise Reduction\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>129</span>  <span class='chapter-title'>🏗️ Noise Reduction</span>"
    ]
  },
  {
    "objectID": "image-noise.html#pros-and-cons",
    "href": "image-noise.html#pros-and-cons",
    "title": "129  🏗️ Noise Reduction",
    "section": "129.2 Pros and Cons",
    "text": "129.2 Pros and Cons\n\n129.2.1 Pros\n\n\n129.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>129</span>  <span class='chapter-title'>🏗️ Noise Reduction</span>"
    ]
  },
  {
    "objectID": "image-noise.html#r-examples",
    "href": "image-noise.html#r-examples",
    "title": "129  🏗️ Noise Reduction",
    "section": "129.3 R Examples",
    "text": "129.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>129</span>  <span class='chapter-title'>🏗️ Noise Reduction</span>"
    ]
  },
  {
    "objectID": "image-noise.html#python-examples",
    "href": "image-noise.html#python-examples",
    "title": "129  🏗️ Noise Reduction",
    "section": "129.4 Python Examples",
    "text": "129.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>129</span>  <span class='chapter-title'>🏗️ Noise Reduction</span>"
    ]
  },
  {
    "objectID": "image-normalization.html",
    "href": "image-normalization.html",
    "title": "130  🏗️ Value Normalization",
    "section": "",
    "text": "130.1 Value Normalization\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>130</span>  <span class='chapter-title'>🏗️ Value Normalization</span>"
    ]
  },
  {
    "objectID": "image-normalization.html#pros-and-cons",
    "href": "image-normalization.html#pros-and-cons",
    "title": "130  🏗️ Value Normalization",
    "section": "130.2 Pros and Cons",
    "text": "130.2 Pros and Cons\n\n130.2.1 Pros\n\n\n130.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>130</span>  <span class='chapter-title'>🏗️ Value Normalization</span>"
    ]
  },
  {
    "objectID": "image-normalization.html#r-examples",
    "href": "image-normalization.html#r-examples",
    "title": "130  🏗️ Value Normalization",
    "section": "130.3 R Examples",
    "text": "130.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>130</span>  <span class='chapter-title'>🏗️ Value Normalization</span>"
    ]
  },
  {
    "objectID": "image-normalization.html#python-examples",
    "href": "image-normalization.html#python-examples",
    "title": "130  🏗️ Value Normalization",
    "section": "130.4 Python Examples",
    "text": "130.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>130</span>  <span class='chapter-title'>🏗️ Value Normalization</span>"
    ]
  },
  {
    "objectID": "image-resize.html",
    "href": "image-resize.html",
    "title": "131  🏗️ Resizing",
    "section": "",
    "text": "131.1 Resizing\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>131</span>  <span class='chapter-title'>🏗️ Resizing</span>"
    ]
  },
  {
    "objectID": "image-resize.html#pros-and-cons",
    "href": "image-resize.html#pros-and-cons",
    "title": "131  🏗️ Resizing",
    "section": "131.2 Pros and Cons",
    "text": "131.2 Pros and Cons\n\n131.2.1 Pros\n\n\n131.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>131</span>  <span class='chapter-title'>🏗️ Resizing</span>"
    ]
  },
  {
    "objectID": "image-resize.html#r-examples",
    "href": "image-resize.html#r-examples",
    "title": "131  🏗️ Resizing",
    "section": "131.3 R Examples",
    "text": "131.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>131</span>  <span class='chapter-title'>🏗️ Resizing</span>"
    ]
  },
  {
    "objectID": "image-resize.html#python-examples",
    "href": "image-resize.html#python-examples",
    "title": "131  🏗️ Resizing",
    "section": "131.4 Python Examples",
    "text": "131.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>131</span>  <span class='chapter-title'>🏗️ Resizing</span>"
    ]
  },
  {
    "objectID": "image-brightness.html",
    "href": "image-brightness.html",
    "title": "132  🏗️ Changing Brightness",
    "section": "",
    "text": "132.1 Changing Brightness\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>132</span>  <span class='chapter-title'>🏗️ Changing Brightness</span>"
    ]
  },
  {
    "objectID": "image-brightness.html#pros-and-cons",
    "href": "image-brightness.html#pros-and-cons",
    "title": "132  🏗️ Changing Brightness",
    "section": "132.2 Pros and Cons",
    "text": "132.2 Pros and Cons\n\n132.2.1 Pros\n\n\n132.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>132</span>  <span class='chapter-title'>🏗️ Changing Brightness</span>"
    ]
  },
  {
    "objectID": "image-brightness.html#r-examples",
    "href": "image-brightness.html#r-examples",
    "title": "132  🏗️ Changing Brightness",
    "section": "132.3 R Examples",
    "text": "132.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>132</span>  <span class='chapter-title'>🏗️ Changing Brightness</span>"
    ]
  },
  {
    "objectID": "image-brightness.html#python-examples",
    "href": "image-brightness.html#python-examples",
    "title": "132  🏗️ Changing Brightness",
    "section": "132.4 Python Examples",
    "text": "132.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>132</span>  <span class='chapter-title'>🏗️ Changing Brightness</span>"
    ]
  },
  {
    "objectID": "image-shift-flip-rotate.html",
    "href": "image-shift-flip-rotate.html",
    "title": "133  🏗️ Shifting, Flipping, and Rotation",
    "section": "",
    "text": "133.1 Shifting, Flipping, and Rotation\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>133</span>  <span class='chapter-title'>🏗️ Shifting, Flipping, and Rotation</span>"
    ]
  },
  {
    "objectID": "image-shift-flip-rotate.html#pros-and-cons",
    "href": "image-shift-flip-rotate.html#pros-and-cons",
    "title": "133  🏗️ Shifting, Flipping, and Rotation",
    "section": "133.2 Pros and Cons",
    "text": "133.2 Pros and Cons\n\n133.2.1 Pros\n\n\n133.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>133</span>  <span class='chapter-title'>🏗️ Shifting, Flipping, and Rotation</span>"
    ]
  },
  {
    "objectID": "image-shift-flip-rotate.html#r-examples",
    "href": "image-shift-flip-rotate.html#r-examples",
    "title": "133  🏗️ Shifting, Flipping, and Rotation",
    "section": "133.3 R Examples",
    "text": "133.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>133</span>  <span class='chapter-title'>🏗️ Shifting, Flipping, and Rotation</span>"
    ]
  },
  {
    "objectID": "image-shift-flip-rotate.html#python-examples",
    "href": "image-shift-flip-rotate.html#python-examples",
    "title": "133  🏗️ Shifting, Flipping, and Rotation",
    "section": "133.4 Python Examples",
    "text": "133.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>133</span>  <span class='chapter-title'>🏗️ Shifting, Flipping, and Rotation</span>"
    ]
  },
  {
    "objectID": "image-crop-scale.html",
    "href": "image-crop-scale.html",
    "title": "134  🏗️ Cropping and Scaling",
    "section": "",
    "text": "134.1 Cropping and Scaling\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>134</span>  <span class='chapter-title'>🏗️ Cropping and Scaling</span>"
    ]
  },
  {
    "objectID": "image-crop-scale.html#pros-and-cons",
    "href": "image-crop-scale.html#pros-and-cons",
    "title": "134  🏗️ Cropping and Scaling",
    "section": "134.2 Pros and Cons",
    "text": "134.2 Pros and Cons\n\n134.2.1 Pros\n\n\n134.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>134</span>  <span class='chapter-title'>🏗️ Cropping and Scaling</span>"
    ]
  },
  {
    "objectID": "image-crop-scale.html#r-examples",
    "href": "image-crop-scale.html#r-examples",
    "title": "134  🏗️ Cropping and Scaling",
    "section": "134.3 R Examples",
    "text": "134.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>134</span>  <span class='chapter-title'>🏗️ Cropping and Scaling</span>"
    ]
  },
  {
    "objectID": "image-crop-scale.html#python-examples",
    "href": "image-crop-scale.html#python-examples",
    "title": "134  🏗️ Cropping and Scaling",
    "section": "134.4 Python Examples",
    "text": "134.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>134</span>  <span class='chapter-title'>🏗️ Cropping and Scaling</span>"
    ]
  },
  {
    "objectID": "image-embeddings.html",
    "href": "image-embeddings.html",
    "title": "135  🏗️ Image embeddings",
    "section": "",
    "text": "135.1 Image embeddings\nWIP",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>135</span>  <span class='chapter-title'>🏗️ Image embeddings</span>"
    ]
  },
  {
    "objectID": "image-embeddings.html#pros-and-cons",
    "href": "image-embeddings.html#pros-and-cons",
    "title": "135  🏗️ Image embeddings",
    "section": "135.2 Pros and Cons",
    "text": "135.2 Pros and Cons\n\n135.2.1 Pros\n\n\n135.2.2 Cons",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>135</span>  <span class='chapter-title'>🏗️ Image embeddings</span>"
    ]
  },
  {
    "objectID": "image-embeddings.html#r-examples",
    "href": "image-embeddings.html#r-examples",
    "title": "135  🏗️ Image embeddings",
    "section": "135.3 R Examples",
    "text": "135.3 R Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>135</span>  <span class='chapter-title'>🏗️ Image embeddings</span>"
    ]
  },
  {
    "objectID": "image-embeddings.html#python-examples",
    "href": "image-embeddings.html#python-examples",
    "title": "135  🏗️ Image embeddings",
    "section": "135.4 Python Examples",
    "text": "135.4 Python Examples",
    "crumbs": [
      "Image Data",
      "<span class='chapter-number'>135</span>  <span class='chapter-title'>🏗️ Image embeddings</span>"
    ]
  },
  {
    "objectID": "relational.html",
    "href": "relational.html",
    "title": "136  Relational Overview",
    "section": "",
    "text": "136.1 Relational Overview\nSo far in this book, we have almost exclusively talked about tabular data. Except for image data and to some degree time series data. The latter doesn’t quite count as is it typically stored in a tabular way. This section will talk about the scenario where you are working with more than 1 table. A situation that is quite common.\nWhen you have data, one way to store is in a database. You have several smaller tables with information. Once you want to do some kind of modeling, you go to your database and query out the data so you get it in a tabular format that our modeling tools will accept.\nIt is at this stage we can use some feature engineering tricks. Propose that we are looking at daily sales targets for a number of different stores. There will be tables of the individual store performances, who works there, the items they carry and so on. As well as their past behavior. Using the knowledge of these cross tables can be very valuable. We will look at two ways to handle this. Manually and automatically.",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>136</span>  <span class='chapter-title'>Relational Overview</span>"
    ]
  },
  {
    "objectID": "relational-manual.html",
    "href": "relational-manual.html",
    "title": "137  🏗️ Manual",
    "section": "",
    "text": "137.1 Manual\nWIP",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>137</span>  <span class='chapter-title'>🏗️ Manual</span>"
    ]
  },
  {
    "objectID": "relational-manual.html#pros-and-cons",
    "href": "relational-manual.html#pros-and-cons",
    "title": "137  🏗️ Manual",
    "section": "137.2 Pros and Cons",
    "text": "137.2 Pros and Cons\n\n137.2.1 Pros\n\n\n137.2.2 Cons",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>137</span>  <span class='chapter-title'>🏗️ Manual</span>"
    ]
  },
  {
    "objectID": "relational-manual.html#r-examples",
    "href": "relational-manual.html#r-examples",
    "title": "137  🏗️ Manual",
    "section": "137.3 R Examples",
    "text": "137.3 R Examples",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>137</span>  <span class='chapter-title'>🏗️ Manual</span>"
    ]
  },
  {
    "objectID": "relational-manual.html#python-examples",
    "href": "relational-manual.html#python-examples",
    "title": "137  🏗️ Manual",
    "section": "137.4 Python Examples",
    "text": "137.4 Python Examples",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>137</span>  <span class='chapter-title'>🏗️ Manual</span>"
    ]
  },
  {
    "objectID": "relational-auto.html",
    "href": "relational-auto.html",
    "title": "138  🏗️ Automatic",
    "section": "",
    "text": "138.1 Automatic\nWIP",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>138</span>  <span class='chapter-title'>🏗️ Automatic</span>"
    ]
  },
  {
    "objectID": "relational-auto.html#pros-and-cons",
    "href": "relational-auto.html#pros-and-cons",
    "title": "138  🏗️ Automatic",
    "section": "138.2 Pros and Cons",
    "text": "138.2 Pros and Cons\n\n138.2.1 Pros\n\n\n138.2.2 Cons",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>138</span>  <span class='chapter-title'>🏗️ Automatic</span>"
    ]
  },
  {
    "objectID": "relational-auto.html#r-examples",
    "href": "relational-auto.html#r-examples",
    "title": "138  🏗️ Automatic",
    "section": "138.3 R Examples",
    "text": "138.3 R Examples",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>138</span>  <span class='chapter-title'>🏗️ Automatic</span>"
    ]
  },
  {
    "objectID": "relational-auto.html#python-examples",
    "href": "relational-auto.html#python-examples",
    "title": "138  🏗️ Automatic",
    "section": "138.4 Python Examples",
    "text": "138.4 Python Examples",
    "crumbs": [
      "Ralational Data",
      "<span class='chapter-number'>138</span>  <span class='chapter-title'>🏗️ Automatic</span>"
    ]
  },
  {
    "objectID": "video.html",
    "href": "video.html",
    "title": "139  Video Overview",
    "section": "",
    "text": "139.1 Video Overview\nThis chapter is still tentative, if you have any methods you think would fit in this chapter, please post an issue in the Github repository.\nWIP",
    "crumbs": [
      "Video Data",
      "<span class='chapter-number'>139</span>  <span class='chapter-title'>Video Overview</span>"
    ]
  },
  {
    "objectID": "video-tmp.html",
    "href": "video-tmp.html",
    "title": "140  🏗️ Temporary",
    "section": "",
    "text": "140.1 Temprary\nWIP",
    "crumbs": [
      "Video Data",
      "<span class='chapter-number'>140</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "video-tmp.html#pros-and-cons",
    "href": "video-tmp.html#pros-and-cons",
    "title": "140  🏗️ Temporary",
    "section": "140.2 Pros and Cons",
    "text": "140.2 Pros and Cons\n\n140.2.1 Pros\n\n\n140.2.2 Cons",
    "crumbs": [
      "Video Data",
      "<span class='chapter-number'>140</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "video-tmp.html#r-examples",
    "href": "video-tmp.html#r-examples",
    "title": "140  🏗️ Temporary",
    "section": "140.3 R Examples",
    "text": "140.3 R Examples",
    "crumbs": [
      "Video Data",
      "<span class='chapter-number'>140</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "video-tmp.html#python-examples",
    "href": "video-tmp.html#python-examples",
    "title": "140  🏗️ Temporary",
    "section": "140.4 Python Examples",
    "text": "140.4 Python Examples",
    "crumbs": [
      "Video Data",
      "<span class='chapter-number'>140</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "sound.html",
    "href": "sound.html",
    "title": "141  Sound Overview",
    "section": "",
    "text": "141.1 Sound Overview\nThis chapter is still tentative, if you have any methods you think would fit in this chapter, please post an issue in the Github repository.\nWIP",
    "crumbs": [
      "Sound Data",
      "<span class='chapter-number'>141</span>  <span class='chapter-title'>Sound Overview</span>"
    ]
  },
  {
    "objectID": "sound-tmp.html",
    "href": "sound-tmp.html",
    "title": "142  🏗️ Temporary",
    "section": "",
    "text": "142.1 Temporary\nWIP",
    "crumbs": [
      "Sound Data",
      "<span class='chapter-number'>142</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "sound-tmp.html#pros-and-cons",
    "href": "sound-tmp.html#pros-and-cons",
    "title": "142  🏗️ Temporary",
    "section": "142.2 Pros and Cons",
    "text": "142.2 Pros and Cons\n\n142.2.1 Pros\n\n\n142.2.2 Cons",
    "crumbs": [
      "Sound Data",
      "<span class='chapter-number'>142</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "sound-tmp.html#r-examples",
    "href": "sound-tmp.html#r-examples",
    "title": "142  🏗️ Temporary",
    "section": "142.3 R Examples",
    "text": "142.3 R Examples",
    "crumbs": [
      "Sound Data",
      "<span class='chapter-number'>142</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "sound-tmp.html#python-examples",
    "href": "sound-tmp.html#python-examples",
    "title": "142  🏗️ Temporary",
    "section": "142.4 Python Examples",
    "text": "142.4 Python Examples",
    "crumbs": [
      "Sound Data",
      "<span class='chapter-number'>142</span>  <span class='chapter-title'>🏗️ Temporary</span>"
    ]
  },
  {
    "objectID": "order.html",
    "href": "order.html",
    "title": "143  🏗️ Order of transformations",
    "section": "",
    "text": "143.1 Order of transformations\nWIP",
    "crumbs": [
      "<span class='chapter-number'>143</span>  <span class='chapter-title'>🏗️ Order of transformations</span>"
    ]
  },
  {
    "objectID": "sparse.html",
    "href": "sparse.html",
    "title": "144  🏗️ What should you do if you have sparse data?",
    "section": "",
    "text": "144.1 What should you do if you have sparse data?\nWIP",
    "crumbs": [
      "<span class='chapter-number'>144</span>  <span class='chapter-title'>🏗️ What should you do if you have sparse data?</span>"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "145  🏗️ How Different Models Deal With Input",
    "section": "",
    "text": "145.1 How different Models Deal With Input\nWIP",
    "crumbs": [
      "<span class='chapter-number'>145</span>  <span class='chapter-title'>🏗️ How Different Models Deal With Input</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "146  🏗️ Summary",
    "section": "",
    "text": "146.1 Summary\nWIP",
    "crumbs": [
      "<span class='chapter-number'>146</span>  <span class='chapter-title'>🏗️ Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Angelov, Dimo. 2020. “Top2Vec: Distributed Representations of\nTopics.” https://arxiv.org/abs/2008.09470.\n\n\nAraci, Dogu. 2019. “FinBERT: Financial Sentiment Analysis with\nPre-Trained Language Models.” https://arxiv.org/abs/1908.10063.\n\n\nAsgari, Mohammad R. K., Ehsaneddin AND Mofrad. 2015. “Continuous\nDistributed Representation of Biological Sequences for Deep Proteomics\nand Genomics.” PLOS ONE 10 (11): 1–15. https://doi.org/10.1371/journal.pone.0141287.\n\n\nBeltagy, Iz, Kyle Lo, and Arman Cohan. 2019. “SciBERT: A\nPretrained Language Model for Scientific Text.” https://arxiv.org/abs/1903.10676.\n\n\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. 2003. “Latent\nDirichlet Allocation.” J. Mach. Learn. Res. 3 (null):\n993–1022.\n\n\nBuuren, S. van. 2012. Flexible Imputation of Missing Data.\nChapman & Hall/CRC Interdisciplinary Statistics. CRC Press. https://books.google.com/books?id=elDNBQAAQBAJ.\n\n\nCañete, José, Gabriel Chaperon, Rodrigo Fuentes, Jou-Hui Ho, Hojin Kang,\nand Jorge Pérez. 2023. “Spanish Pre-Trained BERT Model and\nEvaluation Data.” https://arxiv.org/abs/2308.02976.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\n“BERT: Pre-Training of Deep Bidirectional\nTransformers for Language Understanding.” In Proceedings of\nthe 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), edited by Jill Burstein, Christy\nDoran, and Thamar Solorio, 4171–86. Minneapolis, Minnesota: Association\nfor Computational Linguistics. https://doi.org/10.18653/v1/N19-1423.\n\n\nGalli, S. 2020. Python Feature Engineering Cookbook: Over 70 Recipes\nfor Creating, Engineering, and Transforming Features to Build Machine\nLearning Models. Packt Publishing. https://books.google.com/books?id=2c_LDwAAQBAJ.\n\n\nGéron, Aurélien. 2017. Hands-on Machine Learning with Scikit-Learn\nand TensorFlow : Concepts, Tools, and Techniques to Build Intelligent\nSystems. Sebastopol, CA: O’Reilly Media.\n\n\nHonnibal, Matthew, Ines Montani, Sofie Van Landeghem, and Adriane Boyd.\n2020. “spaCy: Industrial-strength Natural\nLanguage Processing in Python.” https://doi.org/10.5281/zenodo.1212303.\n\n\nHuang, Kexin, Jaan Altosaar, and Rajesh Ranganath. 2020.\n“ClinicalBERT: Modeling Clinical Notes and Predicting Hospital\nReadmission.” https://arxiv.org/abs/1904.05342.\n\n\nKuhn, M., and K. Johnson. 2013. Applied Predictive Modeling.\nSpringerLink : Bücher. Springer New York. https://books.google.com/books?id=xYRDAAAAQBAJ.\n\n\n———. 2019. Feature Engineering and Selection: A Practical Approach\nfor Predictive Models. Chapman & Hall/CRC Data Science Series.\nCRC Press. https://books.google.com/books?id=q5alDwAAQBAJ.\n\n\nKuhn, M., and J. Silge. 2022. Tidy Modeling with r. O’Reilly\nMedia. https://books.google.com/books?id=98J6EAAAQBAJ.\n\n\nLan, Zhenzhong, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2020. “ALBERT: A Lite BERT for\nSelf-Supervised Learning of Language Representations.” https://arxiv.org/abs/1909.11942.\n\n\nLe, Quoc V., and Tomas Mikolov. 2014. “Distributed Representations\nof Sentences and Documents.” https://arxiv.org/abs/1405.4053.\n\n\nLee, Jieh-Sheng, and Jieh Hsiang. 2019. “PatentBERT: Patent\nClassification with Fine-Tuning a Pre-Trained BERT Model.” https://arxiv.org/abs/1906.02124.\n\n\nLee, Jinhyuk, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan\nHo So, and Jaewoo Kang. 2019. “BioBERT: A Pre-Trained Biomedical\nLanguage Representation Model for Biomedical Text Mining.” Edited\nby Jonathan Wren. Bioinformatics 36 (4): 1234–40. https://doi.org/10.1093/bioinformatics/btz682.\n\n\nLewis, David D., Yiming Yang, Tony G. Rose, and Fan Li. 2004.\n“RCV1: A New Benchmark Collection for Text\nCategorization Research.” Journal of Machine Learning\nResearch 5: 361–97. https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf.\n\n\nLiu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\n2019. “RoBERTa: A Robustly Optimized BERT Pretraining\nApproach.” https://arxiv.org/abs/1907.11692.\n\n\nLuhn, H. P. 1960. “Key Word-in-Context Index for Technical\nLiterature (Kwic Index).” American Documentation 11 (4):\n288–95. https://doi.org/https://doi.org/10.1002/asi.5090110403.\n\n\nMicci-Barreca, Daniele. 2001. “A Preprocessing Scheme for\nHigh-Cardinality Categorical Attributes in Classification and Prediction\nProblems.” SIGKDD Explor. Newsl. 3 (1): 27–32. https://doi.org/10.1145/507533.507538.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.\n“Efficient Estimation of Word Representations in Vector\nSpace.” https://arxiv.org/abs/1301.3781.\n\n\nMougan, Carlos, David Masip, Jordi Nin, and Oriol Pujol. 2021.\n“Quantile Encoder: Tackling High Cardinality Categorical Features\nin Regression Problems.” In Modeling Decisions for Artificial\nIntelligence, edited by Vicenç Torra and Yasuo Narukawa, 168–80.\nCham: Springer International Publishing.\n\n\nNg, Patrick. 2017. “Dna2vec: Consistent Vector Representations of\nVariable-Length k-Mers.” https://arxiv.org/abs/1701.06279.\n\n\nNothman, Joel, Hanmin Qin, and Roman Yurchak. 2018. “Stop Word\nLists in Free Open-Source Software Packages.” In Proceedings\nof Workshop for NLP Open Source Software\n(NLP-OSS), edited by Eunjeong L. Park,\nMasato Hagiwara, Dmitrijs Milajevs, and Liling Tan, 7–12. Melbourne,\nAustralia: Association for Computational Linguistics. https://doi.org/10.18653/v1/W18-2502.\n\n\nOzdemir, S. 2022. Feature Engineering Bookcamp. Manning. https://books.google.com/books?id=3n6HEAAAQBAJ.\n\n\nPargent, Florian, Florian Pfisterer, Janek Thomas, and Bernd Bischl.\n2022. “Regularized Target Encoding Outperforms Traditional Methods\nin Supervised Machine Learning with High Cardinality Features.”\nComputational Statistics 37 (5): 2671–92. https://doi.org/10.1007/s00180-022-01207-6.\n\n\nPorter, Martin F. 1980. “An Algorithm for Suffix\nStripping.” Program 14 (3): 130–37. https://doi.org/10.1108/eb046814.\n\n\n———. 2001. “Snowball: A Language for Stemming Algorithms.”\nhttps://snowballstem.org.\n\n\nProkhorenkova, Liudmila, Gleb Gusev, Aleksandr Vorobev, Anna Veronika\nDorogush, and Andrey Gulin. 2019. “CatBoost: Unbiased Boosting\nwith Categorical Features.” https://arxiv.org/abs/1706.09516.\n\n\nRobertson, Stephen. 2004. “Understanding Inverse Document\nFrequency: On Theoretical Arguments for IDF.” Journal of\nDocumentation 60 (5): 503–20.\n\n\nRUBIN, DONALD B. 1976. “Inference and missing\ndata.” Biometrika 63 (3): 581–92. https://doi.org/10.1093/biomet/63.3.581.\n\n\nSanh, Victor, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020.\n“DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper\nand Lighter.” https://arxiv.org/abs/1910.01108.\n\n\nSPARCK JONES, K. 1972. “A STATISTICAL INTERPRETATION OF TERM\nSPECIFICITY AND ITS APPLICATION IN RETRIEVAL.” Journal of\nDocumentation 28 (1): 11–21. https://doi.org/https://doi.org/10.1108/eb026526.\n\n\nThakur, A. 2020. Approaching (Almost) Any Machine Learning\nProblem. Amazon Digital Services LLC - Kdp. https://books.google.com/books?id=ZbgAEAAAQBAJ.",
    "crumbs": [
      "References"
    ]
  }
]
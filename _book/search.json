[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feature Engineering A-Z",
    "section": "",
    "text": "Preface\nWelcome to ‚ÄúFeature Engineering A-Z‚Äù! This book is written to be used as a reference guide to the different techniques. This is reflected in the chapter structure. Any question a practitioner is having should be answered by looking at the index and finding the right chapter.\nEach section tries to be as comprehensive as possible with the number of different methods/solutions that are presented. A section on dimensionality reduction should list all the practical methods that could be used, as well as a comparison between the methods to help the reader decide what would be most appropriate. This does not mean that all methods are recommended to use. A number of these methods have little and narrow use cases.\nWhenever possible each method will be accompanied by simple mathematical formulas and visualizations to illustrate the mechanics of the method.\nLastly, each section will include code snippets showcasing how to implement the methods. Preferable in R and Python, Keras/PyTorch. This book is a methods book first, and a coding book second.\n\n\n\n\n\n\nEmpty chapters\n\n\n\nA chapter is prefixed with the emoji üèóÔ∏è to indicate that it hasn‚Äôt been fully written yet."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "It is commonly said that feature engineering, much like machine learning, is an art rather than a science. This wants to reemphasize this point. But just because it is an art doesn‚Äôt mean we can‚Äôt thoroughly explain the tools and techniques. This is the main goal of this book. Giving you the knowledge of the different techniques you are likely to see and work with, and enough of a base, that any future methods won‚Äôt be too intimidating."
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "Where does feature engineering fit into the modeling workflow?",
    "section": "",
    "text": "When we talk about the modeling workflow, it starts at the data source and ends with a fitted model. The fitted model in this instance should be created such that it can be used for the downstream task, be it inference or prediction. We want to make sure that the feature engineering methods we are applying are done correctly to avoid problems with the modeling. Things we especially want to avoid are data leakage, overfitting, and high computational cost.\n\n\n\n\n\n\nTODO\n\n\n\nAdd diagram of modeling workflow from data source to model\n\n\nWhen applying feature engineering methods, we need to think about trained and untrained methods. Trained methods will perform a calculation doing the training of the method, and then using the extracted values to perform the transformation again. We see this in Chapter¬†7, where we do centering. To do centering we subtract the mean value of the variable, calculated based on the training data set. Since this value needs to be calculated, it becomes a trained method. Examples of untrained methods are logarithmic transformation as seen in Chapter¬†2 and datetime value extraction as seen in Chapter¬†38. These methods are static in the sense the way they are performed doesn‚Äôt need any parameters.\nIn practice, this means that untrained methods can be applied before the data-splitting procedure, as it would give the same results regardless of when it was done. Trained methods have to be performed after the data-splitting to ensure you don‚Äôt have data leakage. The wrinkle to this is that untrained methods applied to variables that have already been transformed by a trained method will have to also be done after the data-splitting.\n\n\n\n\n\n\nTODO\n\n\n\nadd a diagram for untrained/trained rule\n\n\nSome untrained methods have a high computational cost, such as BERT from Chapter¬†60. A general advice that errs on the side of safety is to do as much as you can after the data-splitting if you are unsure.\n\nWhy do we use thresholds?\nOftentimes, when we use a method that selects something with a quantity, we end up doing it with a threshold instead of counting directly. The answer to this is purely practical, as it leaves less ambiguity. When selecting these features to keep in a feature selection routine Chapter¬†65 is a good example. It is easier to write the code that selects every feature that has more than X amount of variability. On the other hand, if we said ‚ÄúGive me the 25 most useful features‚Äù, we might have 4 variables tied for 25th place. Now we have another problem. Does it keep all of them in, leaving 27 variables? If we do that then we violated our request of 25 variables. What if we select the first? then we arbitrarily give a bias towards variables early in the data set. What if we randomly select among the ties? then we introduce randomness into the method.\nIt is for the above reasons that many methods in feature engineering and machine learning use thresholds instead of precise numbers."
  },
  {
    "objectID": "how-to-deal-with.html#sec-terminology",
    "href": "how-to-deal-with.html#sec-terminology",
    "title": "How to Deal With ‚Ä¶",
    "section": "Terminology",
    "text": "Terminology\nBelow are some terms we use throughout the book, that we want to make sure are clear. Some of these might differ from other books, and that is fine. This is why we have this section. Some of the methods described in this book are known under multiple names. When that is the case, it will be listed at the beginning of the chapter. The index will likewise point you to the right chapter regardless of which name you use.\n\nObservations\nThis book will mostly be working with rectangular data. In this context, each observation is defined as a row, with the columns holding the specific characteristics for each observation.\nThe observational unit can change depending on the data. If we were looking at a data set of restaurant health code inspections, you are likely to see the data with one row per inspection. However, depending on your problem statement or hypothesis, you might want to think of each restaurant as an observation. If you are thinking from a planning perspective you could think of each day/week as an observation.\nReading this book will not tell you how to think of your data as we don‚Äôt know what you are trying to do. Once you have your data in the right format and order, we can show you what is possible.\n\n\nLearned\nSome methods require information to be transformed, that we are not able to supply beforehand. In the case of centering of numeric variables described in Chapter¬†7. To be able to do this transformation, you need to know the mean value of the training data set. This means is the sufficient information needed to perform the calculations and is the reason why the method is a learned method.\nOn the other hand, taking the square root of a variable as described in Chapter¬†3 isn‚Äôt a learned method as there isn‚Äôt any sufficient information needed. The method can be applied right away.\n\n\nSupervised / Unsupervised\nSome methods use the outcome to guide the calculations. If the outcome is used, the method is said to be supervised. Most methods are unsupervised.\n\n\nLevels\nVariables that contain non-numeric information are typically called qualitative or categorical variables. This can be things such as eye color, street names, names, grades, car models and subscription types. Where there is a finite known set of values a categorical variable can take, we call these values the levels of that variable. So the levels of the variables containing weekdays are ‚ÄúMonday‚Äù, ‚ÄúTuesday‚Äù, ‚ÄúWednesday‚Äù, ‚ÄúThursday‚Äù, ‚ÄúFriday‚Äù, ‚ÄúSaturday‚Äù, and ‚ÄúSunday‚Äù. But the names of our subscribers don‚Äôt have levels as we don‚Äôt know all of them.\nWe will sometimes bend this definition, as it is sometimes useful to pretend that a variable has a finite known set of values, even if it doesn‚Äôt.\n\n\nLinear models\nWe talk about linear models as models that are specified as a linear combination of features. These models tend to be simple, and fast to use, but having the limitation of ‚Äúlinear combination of features‚Äù means that struggle if the data has non-linear effects.\n\n\nEmbedding\nThe word embedding is thrown around a lot in machine learning and artificial intelligence. but in essence, it is a simple concept. An embedding is what happens when each observation is transformed into a numerical representation. We see this often in text embeddings, where a free-from-text field is turned into a fixed-length numerical vector.\nSomething being an embedding doesn‚Äôt mean that it is useful. But with care, and signal, useful representations of the data can be created. The reason why we have embeddings in the first place is that most machine learning models require numerical features for the models to work."
  },
  {
    "objectID": "numeric.html#distributional-problems",
    "href": "numeric.html#distributional-problems",
    "title": "1¬† Overview",
    "section": "1.1 Distributional problems",
    "text": "1.1 Distributional problems\n\nlogarithm\nsqrt\nBoxCox\nYeo-Johnson\nPercentile"
  },
  {
    "objectID": "numeric.html#sec-numeric-scaling-issues",
    "href": "numeric.html#sec-numeric-scaling-issues",
    "title": "1¬† Overview",
    "section": "1.2 Scaling issues",
    "text": "1.2 Scaling issues\nThe topic of feature scaling is important and used widely in all of machine learning. This chapter will go over what feature scaling is and why we want to use it. The following chapters will each go over a different method of feature scaling.\n\n\n\n\n\n\nNote\n\n\n\nThere is some disagreement about the naming of these topics. These types of methods are called feature scaling and scaling in different fields. This book will call this general class of methods feature scaling and will make notes for each specific method and what other names they go by.\n\n\nIn this book, we will define feature scaling as an operation that modifies variables using multiplication and addition. While broadly defined, the methods typically reduce to the following form:\n\\[\nX_{scaled} = \\dfrac{X - a}{b}\n\\tag{1.1}\\]\nThe main difference between the methods is how \\(a\\) and \\(b\\) are calculated. These methods are learned transformation. So we use the training data to derive the right values of \\(a\\) and \\(b\\), and then these values are used to perform the transformations when applied to new data. The different methods might differ on what property is desired for the transformed variables, same range or same spread, but they never change the distribution itself. The power transformations we saw in Chapter¬†4 and Chapter¬†5, distort the transformations, where these feature scalings essentially perform a ‚Äúzooming‚Äù effect.\n\n\nTable¬†1.1: All feature scaling methods\n\n\n\n\n\n\nMethod\nDefinition\n\n\n\n\nCentering\n\\(X_{scaled} = X - \\text{mean}(X)\\)\n\n\nScaling\n\\(X_{scaled} = \\dfrac{X}{\\text{sd}(X)}\\)\n\n\nMax-Abs\n\\(X_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\\)\n\n\nNormalization\n\\(X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\)\n\n\nMin-Max\n\\(X_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\)\n\n\nRobust\n\\(X_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)}\\)\n\n\n\n\nWe see here that all the methods in Table¬†1.1 follow the equation Equation¬†1.1. Sometimes \\(a\\) and \\(b\\) take a value of 0, which is perfectly fine. Centering and scaling when used together is equal to normalization. They are kept separate in the table since they are sometimes used independently. Centering, scaling, and normalization will all be discussed in Chapter¬†7.\nThere are 2 main reasons why we want to perform feature scaling. Firstly, many different types of models take the magnitude of the variables into account when fitting the models, so having variables on different scales can be disadvantageous because some variables have high priorities. In turn, we get that the other variables have low priority. Models that work using Euclidean distances like KNN models are affected by this change. Regularized models such as lasso and ridge regression also need to be scaled since the regularization depends on the magnitude of the estimates. Secondly, some algorithms simply converge much faster when all the variables are on the same scale. These types of models produce the same fit, just at a slower pace than if you don‚Äôt scale the variables. Any algorithms using Gradient Descent fit into this category.\n\n\n\n\n\n\nTODO\n\n\n\nHave a KNN diagram show why this is important\n\n\nList which types of models need feature scaling. Should be a 2 column list. Left=name, right=comment %in% c(no effect, different fit, slow down)"
  },
  {
    "objectID": "numeric.html#non-linear-effect",
    "href": "numeric.html#non-linear-effect",
    "title": "1¬† Overview",
    "section": "1.3 Non-linear effect",
    "text": "1.3 Non-linear effect\n\nbinning\nsplines\npolynomial\n\n\n\n\n\n\n\nTODO\n\n\n\nShow different distributions, and how well the different methods do at dealing with them"
  },
  {
    "objectID": "numeric.html#sec-numeric-outliers-issues",
    "href": "numeric.html#sec-numeric-outliers-issues",
    "title": "1¬† Overview",
    "section": "1.4 Outliers",
    "text": "1.4 Outliers"
  },
  {
    "objectID": "numeric.html#other",
    "href": "numeric.html#other",
    "title": "1¬† Overview",
    "section": "1.5 Other",
    "text": "1.5 Other\nThere are any number of transformations we can apply to numeric data, other functions include:\n\nhyperbolic\nRelu\ninverse\ninverse logit\nlogit"
  },
  {
    "objectID": "numeric-logarithms.html#pros-and-cons",
    "href": "numeric-logarithms.html#pros-and-cons",
    "title": "2¬† Logarithms",
    "section": "2.1 Pros and Cons",
    "text": "2.1 Pros and Cons\n\n2.1.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data sets alike\n\n\n\n2.1.2 Cons\n\nNeeds offset to deal with negative data\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn‚Äôt skewed. See the effect below on 10,000 uniformly distributed values"
  },
  {
    "objectID": "numeric-logarithms.html#r-examples",
    "href": "numeric-logarithms.html#r-examples",
    "title": "2¬† Logarithms",
    "section": "2.2 R Examples",
    "text": "2.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 √ó 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nlog_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_log(Lot_Area)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    10.4      215000\n 2     9.36     105000\n 3     9.57     172000\n 4     9.32     244000\n 5     9.53     189900\n 6     9.21     195500\n 7     8.50     213500\n 8     8.52     191500\n 9     8.59     236500\n10     8.92     189000\n# ‚Ñπ 2,920 more rows\n\n\nThe base can be changed by setting the base argument.\n\nlog_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_log(Lot_Area, base = 2)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     15.0     215000\n 2     13.5     105000\n 3     13.8     172000\n 4     13.4     244000\n 5     13.8     189900\n 6     13.3     195500\n 7     12.3     213500\n 8     12.3     191500\n 9     12.4     236500\n10     12.9     189000\n# ‚Ñπ 2,920 more rows\n\n\nIf we have non-positive values, which we do in the Wood_Deck_SF variable because it has quite a lot of zeroes, we get -Inf which isn‚Äôt going to work.\n\nlog_rec &lt;- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |&gt;\n  step_log(Wood_Deck_SF)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Wood_Deck_SF Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1         5.35     215000\n 2         4.94     105000\n 3         5.97     172000\n 4      -Inf        244000\n 5         5.36     189900\n 6         5.89     195500\n 7      -Inf        213500\n 8      -Inf        191500\n 9         5.47     236500\n10         4.94     189000\n# ‚Ñπ 2,920 more rows\n\n\nSetting the offset argument helps us to deal with that problem.\n\nlog_rec &lt;- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |&gt;\n  step_log(Wood_Deck_SF, offset = 0.5)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Wood_Deck_SF Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1        5.35      215000\n 2        4.95      105000\n 3        5.98      172000\n 4       -0.693     244000\n 5        5.36      189900\n 6        5.89      195500\n 7       -0.693     213500\n 8       -0.693     191500\n 9        5.47      236500\n10        4.95      189000\n# ‚Ñπ 2,920 more rows"
  },
  {
    "objectID": "numeric-logarithms.html#python-examples",
    "href": "numeric-logarithms.html#python-examples",
    "title": "2¬† Logarithms",
    "section": "2.3 Python Examples",
    "text": "2.3 Python Examples"
  },
  {
    "objectID": "numeric-sqrt.html#pros-and-cons",
    "href": "numeric-sqrt.html#pros-and-cons",
    "title": "3¬† Square Root",
    "section": "3.1 Pros and Cons",
    "text": "3.1 Pros and Cons\n\n3.1.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data sets alike\nCan be applied to all numbers, not just non-negative values\n\n\n\n3.1.2 Cons\n\nIt will leave regression coefficients virtually uninterpretable\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn‚Äôt skewed"
  },
  {
    "objectID": "numeric-sqrt.html#r-examples",
    "href": "numeric-sqrt.html#r-examples",
    "title": "3¬† Square Root",
    "section": "3.2 R Examples",
    "text": "3.2 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\nlibrary(recipes)\n\nhotel_bookings |&gt;\n  select(lead_time, adr)\n\n# A tibble: 119,390 √ó 2\n   lead_time   adr\n       &lt;dbl&gt; &lt;dbl&gt;\n 1       342    0 \n 2       737    0 \n 3         7   75 \n 4        13   75 \n 5        14   98 \n 6        14   98 \n 7         0  107 \n 8         9  103 \n 9        85   82 \n10        75  106.\n# ‚Ñπ 119,380 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\n\n\n\n\n\nTODO\n\n\n\nuse signed sqrt\n\n\n\nsqrt_rec &lt;- recipe(lead_time ~ adr, data = hotel_bookings) |&gt;\n  step_sqrt(adr)\n\nsqrt_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL)\n\nWarning in sqrt(new_data[[col_name]]): NaNs produced\n\n\n# A tibble: 119,390 √ó 2\n     adr lead_time\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1  0          342\n 2  0          737\n 3  8.66         7\n 4  8.66        13\n 5  9.90        14\n 6  9.90        14\n 7 10.3          0\n 8 10.1          9\n 9  9.06        85\n10 10.3         75\n# ‚Ñπ 119,380 more rows"
  },
  {
    "objectID": "numeric-sqrt.html#python-examples",
    "href": "numeric-sqrt.html#python-examples",
    "title": "3¬† Square Root",
    "section": "3.3 Python Examples",
    "text": "3.3 Python Examples"
  },
  {
    "objectID": "numeric-boxcox.html#pros-and-cons",
    "href": "numeric-boxcox.html#pros-and-cons",
    "title": "4¬† Box-Cox",
    "section": "4.1 Pros and Cons",
    "text": "4.1 Pros and Cons\n\n4.1.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\n\n\n\n4.1.2 Cons\n\nDoesn‚Äôt work with negative values\nIsn‚Äôt a universal fix"
  },
  {
    "objectID": "numeric-boxcox.html#r-examples",
    "href": "numeric-boxcox.html#r-examples",
    "title": "4¬† Box-Cox",
    "section": "4.2 R Examples",
    "text": "4.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 √ó 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} provides a step to perform Box-Cox transformations.\n\nboxcox_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_BoxCox(Lot_Area) |&gt;\n  prep()\n\nboxcox_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# ‚Ñπ 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nboxcox_rec |&gt;\n  tidy(1)\n\n# A tibble: 1 √ó 3\n  terms    value id          \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 Lot_Area 0.129 BoxCox_3gJXR"
  },
  {
    "objectID": "numeric-boxcox.html#python-examples",
    "href": "numeric-boxcox.html#python-examples",
    "title": "4¬† Box-Cox",
    "section": "4.3 Python Examples",
    "text": "4.3 Python Examples"
  },
  {
    "objectID": "numeric-yeojohnson.html#pros-and-cons",
    "href": "numeric-yeojohnson.html#pros-and-cons",
    "title": "5¬† Yeo-Johnson",
    "section": "5.1 Pros and Cons",
    "text": "5.1 Pros and Cons\n\n5.1.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\nCan handle negative values\n\n\n\n5.1.2 Cons\n\nIsn‚Äôt a universal fix"
  },
  {
    "objectID": "numeric-yeojohnson.html#r-examples",
    "href": "numeric-yeojohnson.html#r-examples",
    "title": "5¬† Yeo-Johnson",
    "section": "5.2 R Examples",
    "text": "5.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 √ó 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} provides a step to perform Yeo-Johnson transformations, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nyeojohnson_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_YeoJohnson(Lot_Area) |&gt;\n  prep()\n\nyeojohnson_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# ‚Ñπ 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nyeojohnson_rec |&gt;\n  tidy(1)\n\n# A tibble: 1 √ó 3\n  terms    value id              \n  &lt;chr&gt;    &lt;dbl&gt; &lt;chr&gt;           \n1 Lot_Area 0.129 YeoJohnson_3gJXR"
  },
  {
    "objectID": "numeric-yeojohnson.html#python-examples",
    "href": "numeric-yeojohnson.html#python-examples",
    "title": "5¬† Yeo-Johnson",
    "section": "5.3 Python Examples",
    "text": "5.3 Python Examples"
  },
  {
    "objectID": "numeric-percentile.html#pros-and-cons",
    "href": "numeric-percentile.html#pros-and-cons",
    "title": "6¬† Percentile",
    "section": "6.1 Pros and Cons",
    "text": "6.1 Pros and Cons\n\n6.1.1 Pros\n\nTransformation isn‚Äôt affected much by outliers\n\n\n\n6.1.2 Cons\n\nDoesn‚Äôt allow to exact reverse transformation\nIsn‚Äôt ideal if training data doesn‚Äôt have that many unique values"
  },
  {
    "objectID": "numeric-percentile.html#r-examples",
    "href": "numeric-percentile.html#r-examples",
    "title": "6¬† Percentile",
    "section": "6.2 R Examples",
    "text": "6.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 √ó 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# ‚Ñπ 2,920 more rows\n\n\nThe {recipes} step to do this transformation is step_percentile(). It defaults to the calculation of 100 percentiles and uses those to transform the data\n\npercentile_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_percentile(Lot_Area) |&gt;\n  prep()\n\npercentile_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    0.989     215000\n 2    0.756     105000\n 3    0.898     172000\n 4    0.717     244000\n 5    0.883     189900\n 6    0.580     195500\n 7    0.104     213500\n 8    0.106     191500\n 9    0.120     236500\n10    0.259     189000\n# ‚Ñπ 2,920 more rows\n\n\nWe can use the tidy() method to pull out what the specific values are for each percentile\n\npercentile_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 √ó 4\n   terms    value percentile id              \n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n 1 Lot_Area 1300           0 percentile_Bp5vK\n 2 Lot_Area 1680           1 percentile_Bp5vK\n 3 Lot_Area 2040.          2 percentile_Bp5vK\n 4 Lot_Area 2362.          3 percentile_Bp5vK\n 5 Lot_Area 2779.          4 percentile_Bp5vK\n 6 Lot_Area 3188.          5 percentile_Bp5vK\n 7 Lot_Area 3674.          6 percentile_Bp5vK\n 8 Lot_Area 3901.          7 percentile_Bp5vK\n 9 Lot_Area 4122.          8 percentile_Bp5vK\n10 Lot_Area 4435           9 percentile_Bp5vK\n# ‚Ñπ 89 more rows\n\n\nYou can change the granularity by using the options argument. In this example, we are calculating 500 points evenly spaced between 0 and 1, both inclusive.\n\npercentile500_rec &lt;- recipe(Sale_Price ~ Lot_Area, data = ames) |&gt;\n  step_percentile(Lot_Area, options = list(probs = (0:500)/500)) |&gt;\n  prep()\n\npercentile500_rec |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Sale_Price\n      &lt;dbl&gt;      &lt;int&gt;\n 1    0.989     215000\n 2    0.755     105000\n 3    0.899     172000\n 4    0.717     244000\n 5    0.884     189900\n 6    0.580     195500\n 7    0.103     213500\n 8    0.106     191500\n 9    0.118     236500\n10    0.254     189000\n# ‚Ñπ 2,920 more rows\n\n\nAnd we can see the more precise numbers.\n\npercentile500_rec |&gt;\n  tidy(1)\n\n# A tibble: 457 √ó 4\n   terms    value percentile id              \n   &lt;chr&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;           \n 1 Lot_Area 1300         0   percentile_RUieL\n 2 Lot_Area 1487.        0.2 percentile_RUieL\n 3 Lot_Area 1531.        0.4 percentile_RUieL\n 4 Lot_Area 1605.        0.6 percentile_RUieL\n 5 Lot_Area 1680         0.8 percentile_RUieL\n 6 Lot_Area 1879.        1.4 percentile_RUieL\n 7 Lot_Area 1890         1.6 percentile_RUieL\n 8 Lot_Area 1946.        1.8 percentile_RUieL\n 9 Lot_Area 2040.        2   percentile_RUieL\n10 Lot_Area 2136.        2.2 percentile_RUieL\n# ‚Ñπ 447 more rows\n\n\nNotice how there are only 457 values in this output. This is happening because some percentile has been collapsed to save space since if the value for the 10.4 and 10.6 percentile is the same, we just store the 10.6 value."
  },
  {
    "objectID": "numeric-percentile.html#python-examples",
    "href": "numeric-percentile.html#python-examples",
    "title": "6¬† Percentile",
    "section": "6.3 Python Examples",
    "text": "6.3 Python Examples"
  },
  {
    "objectID": "numeric-normalization.html#pros-and-cons",
    "href": "numeric-normalization.html#pros-and-cons",
    "title": "7¬† Normalization",
    "section": "7.1 Pros and Cons",
    "text": "7.1 Pros and Cons\n\n7.1.1 Pros\n\nIf you don‚Äôt have any severe outliers then you will rarely see any downsides to applying normalization\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n7.1.2 Cons\n\nNot all software solutions will not be helpful when applying this transformation to a constant variable. A division by 0 error is likely what you will see\nCannot be used with sparse data as it isn‚Äôt preserved because of the centering that is happening. If you only scale the data you don‚Äôt have a problem\nThis transformation is highly affected by outliers, as they affect the mean and standard deviation quite a lot\n\nBelow is the figure Figure¬†7.2 is an illustration of the effect of having a single high value. In this case, a single observation with the value 10000 moved the transformed distribution much tighter around zero. And all but removed the variance of the non-outliers.\n\n\n\n\n\nFigure¬†7.2: Outliers can have a big effect on the resulting distribution when applying normalization."
  },
  {
    "objectID": "numeric-normalization.html#r-examples",
    "href": "numeric-normalization.html#r-examples",
    "title": "7¬† Normalization",
    "section": "7.2 R Examples",
    "text": "7.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} provides a step to perform scaling, centering, and normalization. They are called step_scale(), step_center() and step_normalize() respectively.\nBelow is an example using step_scale()\n\nscale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  prep()\n\nscale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    4.03          1.66        0.627\n 2     105000    1.47          1.11        0    \n 3     172000    1.81          3.11        0.605\n 4     244000    1.42          0           0    \n 5     189900    1.76          1.68        0    \n 6     195500    1.27          2.85        0.112\n 7     213500    0.624         0           0    \n 8     191500    0.635         0           0    \n 9     236500    0.684         1.88        0    \n10     189000    0.952         1.11        0    \n# ‚Ñπ 2,920 more rows\n\n\nWe can also pull out the value of the standard deviation for each variable that was affected using tidy()\n\nscale_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 √ó 3\n   terms            value id         \n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage     33.5  scale_FGmgk\n 2 Lot_Area       7880.   scale_FGmgk\n 3 Year_Built       30.2  scale_FGmgk\n 4 Year_Remod_Add   20.9  scale_FGmgk\n 5 Mas_Vnr_Area    179.   scale_FGmgk\n 6 BsmtFin_SF_1      2.23 scale_FGmgk\n 7 BsmtFin_SF_2    169.   scale_FGmgk\n 8 Bsmt_Unf_SF     440.   scale_FGmgk\n 9 Total_Bsmt_SF   441.   scale_FGmgk\n10 First_Flr_SF    392.   scale_FGmgk\n# ‚Ñπ 23 more rows\n\n\nWe could also have used step_center() and step_scale() together in one recipe\n\ncenter_scale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_scale(all_numeric_predictors()) |&gt;\n  prep()\n\ncenter_scale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# ‚Ñπ 2,920 more rows\n\n\nUsing tidy() we can see information about each step\n\ncenter_scale_rec |&gt;\n  tidy()\n\n# A tibble: 2 √ó 6\n  number operation type   trained skip  id          \n   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;       \n1      1 step      center TRUE    FALSE center_tSRk5\n2      2 step      scale  TRUE    FALSE scale_kjP2v \n\n\nAnd we can pull out the means using tidy(1)\n\ncenter_scale_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 √ó 3\n   terms             value id          \n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage      57.6  center_tSRk5\n 2 Lot_Area       10148.   center_tSRk5\n 3 Year_Built      1971.   center_tSRk5\n 4 Year_Remod_Add  1984.   center_tSRk5\n 5 Mas_Vnr_Area     101.   center_tSRk5\n 6 BsmtFin_SF_1       4.18 center_tSRk5\n 7 BsmtFin_SF_2      49.7  center_tSRk5\n 8 Bsmt_Unf_SF      559.   center_tSRk5\n 9 Total_Bsmt_SF   1051.   center_tSRk5\n10 First_Flr_SF    1160.   center_tSRk5\n# ‚Ñπ 23 more rows\n\n\nand the standard deviation using tidy(2)\n\ncenter_scale_rec |&gt;\n  tidy(2)\n\n# A tibble: 33 √ó 3\n   terms            value id         \n   &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage     33.5  scale_kjP2v\n 2 Lot_Area       7880.   scale_kjP2v\n 3 Year_Built       30.2  scale_kjP2v\n 4 Year_Remod_Add   20.9  scale_kjP2v\n 5 Mas_Vnr_Area    179.   scale_kjP2v\n 6 BsmtFin_SF_1      2.23 scale_kjP2v\n 7 BsmtFin_SF_2    169.   scale_kjP2v\n 8 Bsmt_Unf_SF     440.   scale_kjP2v\n 9 Total_Bsmt_SF   441.   scale_kjP2v\n10 First_Flr_SF    392.   scale_kjP2v\n# ‚Ñπ 23 more rows\n\n\nSince these steps often follow each other, we often use the step_normalize() as a shortcut to do both operations in one step\n\nscale_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_normalize(all_numeric_predictors()) |&gt;\n  prep()\n\nscale_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# ‚Ñπ 2,920 more rows\n\n\nAnd we can still pull out the means and standard deviations using tidy()\n\nscale_rec |&gt;\n  tidy(1) |&gt;\n  filter(terms %in% c(\"Lot_Area\", \"Wood_Deck_SF\", \"Mas_Vnr_Area\"))\n\n# A tibble: 6 √ó 4\n  terms        statistic   value id             \n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;          \n1 Lot_Area     mean      10148.  normalize_ucdPw\n2 Mas_Vnr_Area mean        101.  normalize_ucdPw\n3 Wood_Deck_SF mean         93.8 normalize_ucdPw\n4 Lot_Area     sd         7880.  normalize_ucdPw\n5 Mas_Vnr_Area sd          179.  normalize_ucdPw\n6 Wood_Deck_SF sd          126.  normalize_ucdPw"
  },
  {
    "objectID": "numeric-normalization.html#python-examples",
    "href": "numeric-normalization.html#python-examples",
    "title": "7¬† Normalization",
    "section": "7.3 Python Examples",
    "text": "7.3 Python Examples"
  },
  {
    "objectID": "numeric-range.html#pros-and-cons",
    "href": "numeric-range.html#pros-and-cons",
    "title": "8¬† Range",
    "section": "8.1 Pros and Cons",
    "text": "8.1 Pros and Cons\n\n8.1.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale, provided that clipping wasn‚Äôt turned on\n\n\n\n8.1.2 Cons\n\nTurning on clipping diminishes the effect of outliers by rounding them up/down\nDoesn‚Äôt work with zero variance data as max(x) - min(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn‚Äôt preserved"
  },
  {
    "objectID": "numeric-range.html#r-examples",
    "href": "numeric-range.html#r-examples",
    "title": "8¬† Range",
    "section": "8.2 R Examples",
    "text": "8.2 R Examples\nstep_range() clips. Does allow the user to specify range step_minmax() doesn‚Äôt clip. Doesn‚Äôt allow users to specify a range. A PR is planned to allow users to turn off clipping in step_range()\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ‚Ñπ 2,920 more rows\n\n\nWe will be using the step_range() step for this\n\nrange_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_range(all_numeric_predictors()) |&gt;\n  prep()\n\nrange_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   0.142        0.147        0.07  \n 2     105000   0.0482       0.0983       0     \n 3     172000   0.0606       0.276        0.0675\n 4     244000   0.0461       0            0     \n 5     189900   0.0586       0.149        0     \n 6     195500   0.0406       0.253        0.0125\n 7     213500   0.0169       0            0     \n 8     191500   0.0173       0            0     \n 9     236500   0.0191       0.166        0     \n10     189000   0.0290       0.0983       0     \n# ‚Ñπ 2,920 more rows\n\n\nWe can also pull out what the min and max values were for each variable using tidy()\n\nrange_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 √ó 4\n   terms            min    max id         \n   &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n 1 Lot_Frontage       0    313 range_FGmgk\n 2 Lot_Area        1300 215245 range_FGmgk\n 3 Year_Built      1872   2010 range_FGmgk\n 4 Year_Remod_Add  1950   2010 range_FGmgk\n 5 Mas_Vnr_Area       0   1600 range_FGmgk\n 6 BsmtFin_SF_1       0      7 range_FGmgk\n 7 BsmtFin_SF_2       0   1526 range_FGmgk\n 8 Bsmt_Unf_SF        0   2336 range_FGmgk\n 9 Total_Bsmt_SF      0   6110 range_FGmgk\n10 First_Flr_SF     334   5095 range_FGmgk\n# ‚Ñπ 23 more rows\n\n\nusing the min and max arguments we can set different ranges\n\nrange_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_range(all_numeric_predictors(), min = -2, max = 2) |&gt;\n  prep()\n\nrange_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    -1.43       -1.41         -1.72\n 2     105000    -1.81       -1.61         -2   \n 3     172000    -1.76       -0.896        -1.73\n 4     244000    -1.82       -2            -2   \n 5     189900    -1.77       -1.40         -2   \n 6     195500    -1.84       -0.989        -1.95\n 7     213500    -1.93       -2            -2   \n 8     191500    -1.93       -2            -2   \n 9     236500    -1.92       -1.33         -2   \n10     189000    -1.88       -1.61         -2   \n# ‚Ñπ 2,920 more rows"
  },
  {
    "objectID": "numeric-range.html#python-examples",
    "href": "numeric-range.html#python-examples",
    "title": "8¬† Range",
    "section": "8.3 Python Examples",
    "text": "8.3 Python Examples\nMinMaxScaler() doesn‚Äôt clip by default. Allows the user to specify a range."
  },
  {
    "objectID": "numeric-maxabs.html#pros-and-cons",
    "href": "numeric-maxabs.html#pros-and-cons",
    "title": "9¬† Max Abs",
    "section": "9.1 Pros and Cons",
    "text": "9.1 Pros and Cons\n\n9.1.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\nDoesn‚Äôt affect sparsity\nCan be used on a zero variance variable. Doesn‚Äôt matter much since you likely should get rid of it\n\n\n\n9.1.2 Cons\n\nIs highly affected by outliers"
  },
  {
    "objectID": "numeric-maxabs.html#r-examples",
    "href": "numeric-maxabs.html#r-examples",
    "title": "9¬† Max Abs",
    "section": "9.2 R Examples",
    "text": "9.2 R Examples\nWe will be using the ames data set for these examples.\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ‚Ñπ 2,920 more rows\n\n\nWe will be using the step_maxabs() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_maxabs(all_numeric_predictors()) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   0.148        0.147        0.07  \n 2     105000   0.0540       0.0983       0     \n 3     172000   0.0663       0.276        0.0675\n 4     244000   0.0518       0            0     \n 5     189900   0.0643       0.149        0     \n 6     195500   0.0464       0.253        0.0125\n 7     213500   0.0229       0            0     \n 8     191500   0.0233       0            0     \n 9     236500   0.0250       0.166        0     \n10     189000   0.0348       0.0983       0     \n# ‚Ñπ 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 33 √ó 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   max          313 maxabs_Bp5vK\n 2 Lot_Area       max       215245 maxabs_Bp5vK\n 3 Year_Built     max         2010 maxabs_Bp5vK\n 4 Year_Remod_Add max         2010 maxabs_Bp5vK\n 5 Mas_Vnr_Area   max         1600 maxabs_Bp5vK\n 6 BsmtFin_SF_1   max            7 maxabs_Bp5vK\n 7 BsmtFin_SF_2   max         1526 maxabs_Bp5vK\n 8 Bsmt_Unf_SF    max         2336 maxabs_Bp5vK\n 9 Total_Bsmt_SF  max         6110 maxabs_Bp5vK\n10 First_Flr_SF   max         5095 maxabs_Bp5vK\n# ‚Ñπ 23 more rows"
  },
  {
    "objectID": "numeric-maxabs.html#python-examples",
    "href": "numeric-maxabs.html#python-examples",
    "title": "9¬† Max Abs",
    "section": "9.3 Python Examples",
    "text": "9.3 Python Examples"
  },
  {
    "objectID": "numeric-robust.html#pros-and-cons",
    "href": "numeric-robust.html#pros-and-cons",
    "title": "10¬† Robust Scaling",
    "section": "10.1 Pros and Cons",
    "text": "10.1 Pros and Cons\n\n10.1.1 Pros\n\nIsn‚Äôt affected by outliers\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n10.1.2 Cons\n\nCompletely ignores part of the data outside the quantile ranges\nDoesn‚Äôt work with near zero variance data as Q1(x) - Q3(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn‚Äôt preserved"
  },
  {
    "objectID": "numeric-robust.html#r-examples",
    "href": "numeric-robust.html#r-examples",
    "title": "10¬† Robust Scaling",
    "section": "10.2 R Examples",
    "text": "10.2 R Examples\nWe will be using the ames data set for these examples.\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;int&gt;        &lt;int&gt;        &lt;dbl&gt;\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# ‚Ñπ 2,920 more rows\n\n\nWe will be using the step_robust() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_robust(all_numeric_predictors()) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000    5.43         1.25         0.688\n 2     105000    0.531        0.833        0    \n 3     172000    1.17         2.34         0.664\n 4     244000    0.419        0            0    \n 5     189900    1.07         1.26         0    \n 6     195500    0.132        2.14         0.123\n 7     213500   -1.10         0            0    \n 8     191500   -1.08         0            0    \n 9     236500   -0.984        1.41         0    \n10     189000   -0.471        0.833        0    \n# ‚Ñπ 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 √ó 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   lower        43  robust_Bp5vK\n 2 Lot_Frontage   median       63  robust_Bp5vK\n 3 Lot_Frontage   higher       78  robust_Bp5vK\n 4 Lot_Area       lower      7440. robust_Bp5vK\n 5 Lot_Area       median     9436. robust_Bp5vK\n 6 Lot_Area       higher    11555. robust_Bp5vK\n 7 Year_Built     lower      1954  robust_Bp5vK\n 8 Year_Built     median     1973  robust_Bp5vK\n 9 Year_Built     higher     2001  robust_Bp5vK\n10 Year_Remod_Add lower      1965  robust_Bp5vK\n# ‚Ñπ 89 more rows\n\n\nWe can also can\n\nmaxabs_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_robust(all_numeric_predictors(), range = c(0.1, 0.9)) |&gt;\n  prep()\n\nmaxabs_rec |&gt;\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 √ó 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        &lt;int&gt;    &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n 1     215000   2.35          0.820       0.350 \n 2     105000   0.230         0.547       0     \n 3     172000   0.509         1.53        0.337 \n 4     244000   0.181         0           0     \n 5     189900   0.463         0.828       0     \n 6     195500   0.0570        1.41        0.0625\n 7     213500  -0.475         0           0     \n 8     191500  -0.467         0           0     \n 9     236500  -0.426         0.925       0     \n10     189000  -0.204         0.547       0     \n# ‚Ñπ 2,920 more rows\n\n\nwhen we pull out the ranges, we see that they are wider\n\nmaxabs_rec |&gt;\n  tidy(1)\n\n# A tibble: 99 √ó 4\n   terms          statistic  value id          \n   &lt;chr&gt;          &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       \n 1 Lot_Frontage   lower         0  robust_RUieL\n 2 Lot_Frontage   median       63  robust_RUieL\n 3 Lot_Frontage   higher       91  robust_RUieL\n 4 Lot_Area       lower      4800  robust_RUieL\n 5 Lot_Area       median     9436. robust_RUieL\n 6 Lot_Area       higher    14299. robust_RUieL\n 7 Year_Built     lower      1925. robust_RUieL\n 8 Year_Built     median     1973  robust_RUieL\n 9 Year_Built     higher     2006  robust_RUieL\n10 Year_Remod_Add lower      1950  robust_RUieL\n# ‚Ñπ 89 more rows"
  },
  {
    "objectID": "numeric-robust.html#python-examples",
    "href": "numeric-robust.html#python-examples",
    "title": "10¬† Robust Scaling",
    "section": "10.3 Python Examples",
    "text": "10.3 Python Examples"
  },
  {
    "objectID": "numeric-binning.html#pros-and-cons",
    "href": "numeric-binning.html#pros-and-cons",
    "title": "11¬† Binning",
    "section": "11.1 Pros and Cons",
    "text": "11.1 Pros and Cons\n\n11.1.1 Pros\n\nWorks fast computationally\nBehaves predictably outside the range of the predictors\nIf cuts are placed well, it can handle sudden changes in distributions\nInterpretable\ndoesn‚Äôt create correlated features\n\n\n\n11.1.2 Cons\n\nThe inherent rounding that happens, can lead to loss of performance and interpretations\narguably less interpretable than binning\ncan produce a lot of variables"
  },
  {
    "objectID": "numeric-binning.html#r-examples",
    "href": "numeric-binning.html#r-examples",
    "title": "11¬† Binning",
    "section": "11.2 R Examples",
    "text": "11.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} has the function step_discretize() for just this occasion.\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 2\n$ Lot_Area   &lt;fct&gt; bin4, bin4, bin4, bin3, bin4, bin3, bin1, bin1, bin1, bin2,‚Ä¶\n$ Year_Built &lt;fct&gt; bin2, bin2, bin2, bin2, bin3, bin3, bin3, bin3, bin3, bin3,‚Ä¶\n\n\nIf you don‚Äôt like the default number of breaks created, you can use the num_breaks = 6 argument to change it.\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built, num_breaks = 6)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 2\n$ Lot_Area   &lt;fct&gt; bin6, bin5, bin6, bin5, bin6, bin4, bin1, bin1, bin1, bin2,‚Ä¶\n$ Year_Built &lt;fct&gt; bin2, bin3, bin2, bin3, bin5, bin5, bin5, bin4, bin4, bin5,‚Ä¶\n\n\nThis step technically creates a factor variable, but we can turn it into a series of indicator functions with step_dummy()\n\nbin_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_discretize(Lot_Area, Year_Built, num_breaks = 6) |&gt;\n  step_dummy(Lot_Area, Year_Built, one_hot = TRUE)\n\nbin_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 12\n$ Lot_Area_bin1   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ Lot_Area_bin2   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, ‚Ä¶\n$ Lot_Area_bin3   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ Lot_Area_bin4   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ Lot_Area_bin5   &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ‚Ä¶\n$ Lot_Area_bin6   &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ‚Ä¶\n$ Year_Built_bin1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ Year_Built_bin2 &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ Year_Built_bin3 &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ Year_Built_bin4 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, ‚Ä¶\n$ Year_Built_bin5 &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ‚Ä¶\n$ Year_Built_bin6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ‚Ä¶"
  },
  {
    "objectID": "numeric-binning.html#python-examples",
    "href": "numeric-binning.html#python-examples",
    "title": "11¬† Binning",
    "section": "11.3 Python Examples",
    "text": "11.3 Python Examples"
  },
  {
    "objectID": "numeric-splines.html#pros-and-cons",
    "href": "numeric-splines.html#pros-and-cons",
    "title": "12¬† Splines",
    "section": "12.1 Pros and Cons",
    "text": "12.1 Pros and Cons\n\n12.1.1 Pros\n\nWorks fast computationally\nGood performance compared to binning\nis good at handling continuous changes in predictors\n\n\n\n12.1.2 Cons\n\narguably less interpretable than binning\ncreates correlated features\ncan produce a lot of variables\nhave a hard time modeling sudden changes in distributions"
  },
  {
    "objectID": "numeric-splines.html#r-examples",
    "href": "numeric-splines.html#r-examples",
    "title": "12¬† Splines",
    "section": "12.2 R Examples",
    "text": "12.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} provides a number of steps to perform spline operations, each of them starting with step_spline_. Let us use a B-spline and a M-spline as examples here:\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area) |&gt;\n  step_spline_monotone(Year_Built)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 20\n$ Lot_Area_01   &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0.000000000, 0.00‚Ä¶\n$ Lot_Area_02   &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, ‚Ä¶\n$ Lot_Area_03   &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0‚Ä¶\n$ Lot_Area_04   &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, ‚Ä¶\n$ Lot_Area_05   &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0000000000, 0.0078526499, ‚Ä¶\n$ Lot_Area_06   &lt;dbl&gt; 0.000000000, 0.283603274, 0.000000000, 0.507327055, 0.00‚Ä¶\n$ Lot_Area_07   &lt;dbl&gt; 0.73399934, 0.71382012, 0.96474057, 0.48414256, 0.971047‚Ä¶\n$ Lot_Area_08   &lt;dbl&gt; 2.408258e-01, 2.576602e-03, 3.503161e-02, 6.777374e-04, ‚Ä¶\n$ Lot_Area_09   &lt;dbl&gt; 2.444735e-02, 3.441535e-09, 2.277849e-04, 0.000000e+00, ‚Ä¶\n$ Lot_Area_10   &lt;dbl&gt; 7.274651e-04, 0.000000e+00, 3.035128e-08, 0.000000e+00, ‚Ä¶\n$ Year_Built_01 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,‚Ä¶\n$ Year_Built_02 &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1‚Ä¶\n$ Year_Built_03 &lt;dbl&gt; 0.9991483, 0.9995281, 0.9977527, 1.0000000, 1.0000000, 1‚Ä¶\n$ Year_Built_04 &lt;dbl&gt; 0.9041892, 0.9210803, 0.8649607, 0.9884577, 1.0000000, 1‚Ä¶\n$ Year_Built_05 &lt;dbl&gt; 0.20672563, 0.24041792, 0.14882796, 0.54043388, 0.999999‚Ä¶\n$ Year_Built_06 &lt;dbl&gt; 4.792231e-04, 1.169978e-03, 2.995144e-05, 3.881537e-02, ‚Ä¶\n$ Year_Built_07 &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 4.491099e-07, ‚Ä¶\n$ Year_Built_08 &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.221125‚Ä¶\n$ Year_Built_09 &lt;dbl&gt; 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, ‚Ä¶\n$ Year_Built_10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,‚Ä¶\n\n\nWe can set the deg_free argument to specify how many spline features we want for each of the splines.\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area, deg_free = 3) |&gt;\n  step_spline_monotone(Year_Built, deg_free = 4)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 7\n$ Lot_Area_1   &lt;dbl&gt; 0.31422525, 0.13110895, 0.16045431, 0.12580964, 0.1557218‚Ä¶\n$ Lot_Area_2   &lt;dbl&gt; 0.0521839123, 0.0066461383, 0.0103524317, 0.0060782666, 0‚Ä¶\n$ Lot_Area_3   &lt;dbl&gt; 2.888757e-03, 1.123014e-04, 2.226446e-04, 9.788684e-05, 2‚Ä¶\n$ Year_Built_1 &lt;dbl&gt; 0.9827669, 0.9841047, 0.9798397, 0.9914201, 0.9999212, 0.‚Ä¶\n$ Year_Built_2 &lt;dbl&gt; 0.8614458, 0.8686207, 0.8464715, 0.9129756, 0.9968924, 0.‚Ä¶\n$ Year_Built_3 &lt;dbl&gt; 0.5411581, 0.5539857, 0.5156159, 0.6440229, 0.9532064, 0.‚Ä¶\n$ Year_Built_4 &lt;dbl&gt; 0.1653539, 0.1729990, 0.1508264, 0.2341901, 0.6731684, 0.‚Ä¶\n\n\nThese steps have more arguments, so we can change other things. The B-splines created by step_spline_b() default to cubic splines, but we can change that by specifying which polynomial degree with want with the degree argument.\n\nlog_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_spline_b(Lot_Area, deg_free = 3, degree = 1) |&gt;\n  step_spline_monotone(Year_Built, deg_free = 4)\n\nlog_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 7\n$ Lot_Area_1   &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.0000000‚Ä¶\n$ Lot_Area_2   &lt;dbl&gt; 0.89690903, 0.99540159, 0.98247163, 0.99766006, 0.9846078‚Ä¶\n$ Lot_Area_3   &lt;dbl&gt; 0.103090969, 0.004598405, 0.017528365, 0.002339940, 0.015‚Ä¶\n$ Year_Built_1 &lt;dbl&gt; 0.9827669, 0.9841047, 0.9798397, 0.9914201, 0.9999212, 0.‚Ä¶\n$ Year_Built_2 &lt;dbl&gt; 0.8614458, 0.8686207, 0.8464715, 0.9129756, 0.9968924, 0.‚Ä¶\n$ Year_Built_3 &lt;dbl&gt; 0.5411581, 0.5539857, 0.5156159, 0.6440229, 0.9532064, 0.‚Ä¶\n$ Year_Built_4 &lt;dbl&gt; 0.1653539, 0.1729990, 0.1508264, 0.2341901, 0.6731684, 0.‚Ä¶"
  },
  {
    "objectID": "numeric-splines.html#python-examples",
    "href": "numeric-splines.html#python-examples",
    "title": "12¬† Splines",
    "section": "12.3 Python Examples",
    "text": "12.3 Python Examples"
  },
  {
    "objectID": "numeric-polynomial.html#pros-and-cons",
    "href": "numeric-polynomial.html#pros-and-cons",
    "title": "13¬† Polynomial",
    "section": "13.1 Pros and Cons",
    "text": "13.1 Pros and Cons\n\n13.1.1 Pros\n\nWorks fast computationally\nGood performance compared to binning\nDoesn‚Äôt create correlated features\nis good at handling continuous changes in predictors\n\n\n\n13.1.2 Cons\n\narguably less interpretable than binning and splines\ncan produce a lot of variables\nhave a hard time modeling sudden changes in distributions"
  },
  {
    "objectID": "numeric-polynomial.html#r-examples",
    "href": "numeric-polynomial.html#r-examples",
    "title": "13¬† Polynomial",
    "section": "13.2 R Examples",
    "text": "13.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\n\names |&gt;\n  select(Lot_Area, Year_Built)\n\n# A tibble: 2,930 √ó 2\n   Lot_Area Year_Built\n      &lt;int&gt;      &lt;int&gt;\n 1    31770       1960\n 2    11622       1961\n 3    14267       1958\n 4    11160       1968\n 5    13830       1997\n 6     9978       1998\n 7     4920       2001\n 8     5005       1992\n 9     5389       1995\n10     7500       1999\n# ‚Ñπ 2,920 more rows\n\n\n{recipes} has the function step_poly() for just this occasion.\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built)\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 4\n$ Lot_Area_poly_1   &lt;dbl&gt; 5.070030e-02, 3.456477e-03, 9.658577e-03, 2.373161e-‚Ä¶\n$ Lot_Area_poly_2   &lt;dbl&gt; -0.052288355, -0.006139895, -0.013560043, -0.0048015‚Ä¶\n$ Year_Built_poly_1 &lt;dbl&gt; -0.0069377547, -0.0063268386, -0.0081595868, -0.0020‚Ä¶\n$ Year_Built_poly_2 &lt;dbl&gt; -0.0188536923, -0.0189190631, -0.0186090288, -0.0183‚Ä¶\n\n\nIf you don‚Äôt like the default number of features created, you can use the degree argument to change it.\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built, degree = 5)\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 10\n$ Lot_Area_poly_1   &lt;dbl&gt; 5.070030e-02, 3.456477e-03, 9.658577e-03, 2.373161e-‚Ä¶\n$ Lot_Area_poly_2   &lt;dbl&gt; -0.052288355, -0.006139895, -0.013560043, -0.0048015‚Ä¶\n$ Lot_Area_poly_3   &lt;dbl&gt; 0.0024951091, 0.0067956902, 0.0110336270, 0.00588901‚Ä¶\n$ Lot_Area_poly_4   &lt;dbl&gt; 0.0390305341, -0.0078110499, -0.0092519823, -0.00723‚Ä¶\n$ Lot_Area_poly_5   &lt;dbl&gt; -0.0649379780, 0.0051370320, 0.0004088393, 0.0055404‚Ä¶\n$ Year_Built_poly_1 &lt;dbl&gt; -0.0069377547, -0.0063268386, -0.0081595868, -0.0020‚Ä¶\n$ Year_Built_poly_2 &lt;dbl&gt; -0.0188536923, -0.0189190631, -0.0186090288, -0.0183‚Ä¶\n$ Year_Built_poly_3 &lt;dbl&gt; -0.0031709327, -0.0044248985, -0.0006208212, -0.0124‚Ä¶\n$ Year_Built_poly_4 &lt;dbl&gt; 1.420211e-02, 1.311711e-02, 1.609112e-02, 3.358699e-‚Ä¶\n$ Year_Built_poly_5 &lt;dbl&gt; 0.009938840, 0.011096007, 0.007277173, 0.015173692, ‚Ä¶\n\n\nwhile you properly shouldn‚Äôt, you can turn off the orthogonal polynomials by setting options = list(raw = TRUE).\n\npoly_rec &lt;- recipe(~ Lot_Area + Year_Built, data = ames) |&gt;\n  step_poly(Lot_Area, Year_Built, options = list(raw = TRUE))\n\npoly_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 4\n$ Lot_Area_poly_1   &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005,‚Ä¶\n$ Lot_Area_poly_2   &lt;dbl&gt; 1009332900, 135070884, 203547289, 124545600, 1912689‚Ä¶\n$ Year_Built_poly_1 &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 1995‚Ä¶\n$ Year_Built_poly_2 &lt;dbl&gt; 3841600, 3845521, 3833764, 3873024, 3988009, 3992004‚Ä¶"
  },
  {
    "objectID": "numeric-polynomial.html#python-examples",
    "href": "numeric-polynomial.html#python-examples",
    "title": "13¬† Polynomial",
    "section": "13.3 Python Examples",
    "text": "13.3 Python Examples"
  },
  {
    "objectID": "categorical.html#categorical-to-categorical",
    "href": "categorical.html#categorical-to-categorical",
    "title": "14¬† Overview",
    "section": "14.1 Categorical to Categorical",
    "text": "14.1 Categorical to Categorical\nThese methods take a categorical variable and improve them. Whether it means cleaning levels, collapsing levels, or making sure it handles new levels correctly. These Tasks as not always needed depending on the method you are using but they are generally helpful to apply. One method that would have been located here if it wasn‚Äôt for the fact that it has a whole section by itself is dealing with missing values as seen in Chapter¬†41."
  },
  {
    "objectID": "categorical.html#categorical-to-numerical",
    "href": "categorical.html#categorical-to-numerical",
    "title": "14¬† Overview",
    "section": "14.2 Categorical to Numerical",
    "text": "14.2 Categorical to Numerical\nThe vast majority of the chapters in these chapters concern methods that take a categorical variable and produce one or more numerical variables suitable for modeling. There are quite a lot of different methods, all have upsides and downsides and they will all be explored in the remaining chapters."
  },
  {
    "objectID": "categorical-cleaning.html#r-examples",
    "href": "categorical-cleaning.html#r-examples",
    "title": "15¬† Cleaning",
    "section": "15.1 R examples",
    "text": "15.1 R examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a good data set for these examples\n\n\nUse janitor\ntextrecipes::step_clean_levels()"
  },
  {
    "objectID": "categorical-cleaning.html#python-examples",
    "href": "categorical-cleaning.html#python-examples",
    "title": "15¬† Cleaning",
    "section": "15.2 Python Examples",
    "text": "15.2 Python Examples"
  },
  {
    "objectID": "categorical-unseen.html#r-examples",
    "href": "categorical-unseen.html#r-examples",
    "title": "16¬† Unseen Levels",
    "section": "16.1 R Examples",
    "text": "16.1 R Examples\nWe will be using the nycflights13 data set. We are downsampling just a bit to only work on the first day and doing a test-train split.\n\nlibrary(recipes)\nlibrary(rsample)\nlibrary(nycflights13)\n\nflights &lt;- flights |&gt;\n  filter(year == 2013, month == 1, day == 1)\n\nset.seed(13630)\nflights_split &lt;- initial_split(flights)\nflights_train &lt;- training(flights_split)\nflights_test &lt;- testing(flights_split)\n\nNow we are doing the cardinal sin by looking at the testing data. But in this case, it is okay because we are doing it for educational purposes.\n\nflights_train |&gt; pull(carrier) |&gt; unique() |&gt; sort()\n\n [1] \"9E\" \"AA\" \"B6\" \"DL\" \"EV\" \"F9\" \"FL\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\nflights_test |&gt; pull(carrier) |&gt; unique() |&gt; sort()\n\n [1] \"9E\" \"AA\" \"AS\" \"B6\" \"DL\" \"EV\" \"FL\" \"HA\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\n\nNotice that the testing data includes the carrier \"AS\" and \"HA\" but the training data doesn‚Äôt know that. Let us see what would happen if we were to calculate dummy variables without doing any adjusting.\n\ndummy_spec &lt;- recipe(arr_delay ~ carrier, data = flights_train) |&gt;\n  step_dummy(carrier)\n\ndummy_spec_prepped &lt;- prep(dummy_spec)\n\nbake(dummy_spec_prepped, new_data = flights_test)\n\nWarning: ! There are new levels in a factor: `AS` and `HA`.\n\n\n# A tibble: 211 √ó 12\n   arr_delay carrier_AA carrier_B6 carrier_DL carrier_EV carrier_F9 carrier_FL\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        12          0          0          0          0          0          0\n 2         8          1          0          0          0          0          0\n 3       -14          0          0          0          0          0          0\n 4        -6          0          1          0          0          0          0\n 5        -3          1          0          0          0          0          0\n 6       -33          0          0          1          0          0          0\n 7        -7          1          0          0          0          0          0\n 8         5          0          1          0          0          0          0\n 9        31          1          0          0          0          0          0\n10       -10         NA         NA         NA         NA         NA         NA\n# ‚Ñπ 201 more rows\n# ‚Ñπ 5 more variables: carrier_MQ &lt;dbl&gt;, carrier_UA &lt;dbl&gt;, carrier_US &lt;dbl&gt;,\n#   carrier_VX &lt;dbl&gt;, carrier_WN &lt;dbl&gt;\n\n\nWe get a warning, and if you look at the rows that were affected we see that it produces NAs. Let us now use the function step_novel() that implements the above-described method.\n\nnovel_spec &lt;- recipe(arr_delay ~ carrier, data = flights_train) |&gt;\n  step_novel(carrier) |&gt;\n  step_dummy(carrier) \n\nnovel_spec_prepped &lt;- prep(novel_spec)\n\nbake(novel_spec_prepped, new_data = flights_test)\n\n# A tibble: 211 √ó 13\n   arr_delay carrier_AA carrier_B6 carrier_DL carrier_EV carrier_F9 carrier_FL\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1        12          0          0          0          0          0          0\n 2         8          1          0          0          0          0          0\n 3       -14          0          0          0          0          0          0\n 4        -6          0          1          0          0          0          0\n 5        -3          1          0          0          0          0          0\n 6       -33          0          0          1          0          0          0\n 7        -7          1          0          0          0          0          0\n 8         5          0          1          0          0          0          0\n 9        31          1          0          0          0          0          0\n10       -10          0          0          0          0          0          0\n# ‚Ñπ 201 more rows\n# ‚Ñπ 6 more variables: carrier_MQ &lt;dbl&gt;, carrier_UA &lt;dbl&gt;, carrier_US &lt;dbl&gt;,\n#   carrier_VX &lt;dbl&gt;, carrier_WN &lt;dbl&gt;, carrier_new &lt;dbl&gt;\n\n\nAnd we see that we get no error or anything."
  },
  {
    "objectID": "categorical-unseen.html#python-examples",
    "href": "categorical-unseen.html#python-examples",
    "title": "16¬† Unseen Levels",
    "section": "16.2 Python Examples",
    "text": "16.2 Python Examples"
  },
  {
    "objectID": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "href": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "title": "17¬† Dummy Encoding",
    "section": "17.1 Dummy or one-hot encoding",
    "text": "17.1 Dummy or one-hot encoding\n\n\n\n\n\n\nTODO\n\n\n\nadd diagram\n\n\nThe terms dummy encoding and one-hot encoding get thrown around interchangeably, but they do have different and distinct meanings. One-hot encoding is when you return k variables when you have k different levels. Like we have shown above\n\n\n     cat dog horse\n[1,]   0   1     0\n[2,]   1   0     0\n[3,]   0   0     1\n[4,]   0   1     0\n[5,]   1   0     0\n\n\nDummy encoding on the other hand returns k-1 variables, where the excluded one typically is the first one.\n\n\n     dog horse\n[1,]   1     0\n[2,]   0     0\n[3,]   0     1\n[4,]   1     0\n[5,]   0     0\n\n\nThese two encodings store the same information, even though the dummy encoding has 1 less column. Because we can deduce which observations are cat by finding the rows with all zeros. The main reason why one would use dummy variables is because of what some people call the dummy variable trap. When you use one-hot encoding, you are increasing the likelihood that you run into a collinearity problem. With the above example, if you included an intercept in your model you have that intercept = cat + dog + horse which gives perfect collinearity and would cause some models to error as they aren‚Äôt able to handle that.\n\n\n\n\n\n\nNote\n\n\n\nAn intercept is a variable that takes the value 1 for all entries.\n\n\nEven if you don‚Äôt include an intercept you could still run into collinearity. Imagine that in addition to the animal variable also creates a one-hot encoding of the home variable taking the two values \"house\" and \"apartment\", you would get the following indicator variables\n\n\n     cat dog horse house apartment\n[1,]   0   1     0     0         1\n[2,]   1   0     0     1         0\n[3,]   0   0     1     0         1\n[4,]   0   1     0     0         1\n[5,]   1   0     0     1         0\n\n\nAnd in this case, we have that house = cat + dog + horse - apartment which again is an example of perfect collinearity. Unless you have a reason to do otherwise I would suggest that you use dummy encoding in your models. Additionally, this leads to slightly smaller models as each categorical variable produces 1 less variable. It is worth noting that the choice between dummy encoding and one-hot encoding does matter for some models such as decision trees. Depending on what types of rules they can use. Being able to write animal == \"cat\" is easier then saying animal != \"dog\" & animal != \"horse\". This is unlikely to be an issue as many tree-based models can work with categorical variables directly without the need for encoding."
  },
  {
    "objectID": "categorical-dummy.html#ordered-factors",
    "href": "categorical-dummy.html#ordered-factors",
    "title": "17¬† Dummy Encoding",
    "section": "17.2 Ordered factors",
    "text": "17.2 Ordered factors\n\n\n\n\n\n\nTODO\n\n\n\nfinish section"
  },
  {
    "objectID": "categorical-dummy.html#contrasts",
    "href": "categorical-dummy.html#contrasts",
    "title": "17¬† Dummy Encoding",
    "section": "17.3 Contrasts",
    "text": "17.3 Contrasts\n\n\n\n\n\n\nTODO\n\n\n\nfinish section"
  },
  {
    "objectID": "categorical-dummy.html#pros-and-cons",
    "href": "categorical-dummy.html#pros-and-cons",
    "title": "17¬† Dummy Encoding",
    "section": "17.4 Pros and Cons",
    "text": "17.4 Pros and Cons\n\n17.4.1 Pros\n\nVersatile and commonly used\nEasy interpretation\nWill rarely lead to a decrease in performance\n\n\n\n17.4.2 Cons\n\nDoes require fairly clean categorical levels\nCan be quite memory intensive if you have many levels in your categorical variables and you are unable to use sparse representation\nProvides a complete, but not necessarily compact set of variables"
  },
  {
    "objectID": "categorical-dummy.html#r-examples",
    "href": "categorical-dummy.html#r-examples",
    "title": "17¬† Dummy Encoding",
    "section": "17.5 R Examples",
    "text": "17.5 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 √ó 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ‚Ñπ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 √ó 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nAnd since MS_SubClass is a factor, we can verify that they match and that all the levels are observed\n\names |&gt; pull(MS_SubClass) |&gt; levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_dummy() step for this, which defaults to creating dummy variables\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 21\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1‚Ä¶\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; 0, 1, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 1, 0, 1, 1, 1, 1‚Ä¶\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_C_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_I_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n\n\nWe can pull the factor levels for each variable by using tidy(). If a character vector was present in the data set, it would record the observed variables.\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 243 √ó 3\n   terms       columns                                id         \n   &lt;chr&gt;       &lt;chr&gt;                                  &lt;chr&gt;      \n 1 MS_SubClass One_Story_1945_and_Older               dummy_Bp5vK\n 2 MS_SubClass One_Story_with_Finished_Attic_All_Ages dummy_Bp5vK\n 3 MS_SubClass One_and_Half_Story_Unfinished_All_Ages dummy_Bp5vK\n 4 MS_SubClass One_and_Half_Story_Finished_All_Ages   dummy_Bp5vK\n 5 MS_SubClass Two_Story_1946_and_Newer               dummy_Bp5vK\n 6 MS_SubClass Two_Story_1945_and_Older               dummy_Bp5vK\n 7 MS_SubClass Two_and_Half_Story_All_Ages            dummy_Bp5vK\n 8 MS_SubClass Split_or_Multilevel                    dummy_Bp5vK\n 9 MS_SubClass Split_Foyer                            dummy_Bp5vK\n10 MS_SubClass Duplex_All_Styles_and_Ages             dummy_Bp5vK\n# ‚Ñπ 233 more rows\n\n\nsetting one_hot = TRUE gives us the complete one-hot encoding results.\n\nonehot_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt;\n  prep()\n\nonehot_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 23\n$ MS_SubClass_One_Story_1946_and_Newer_All_Styles       &lt;dbl&gt; 1, 1, 1, 1, 0, 0‚Ä¶\n$ MS_SubClass_One_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_Story_1946_and_Newer                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1‚Ä¶\n$ MS_SubClass_Two_Story_1945_and_Older                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_and_Half_Story_All_Ages               &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Split_or_Multilevel                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Split_Foyer                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Duplex_All_Styles_and_Ages                &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_Floating_Village_Residential                &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_Residential_High_Density                    &lt;dbl&gt; 0, 1, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_Residential_Low_Density                     &lt;dbl&gt; 1, 0, 1, 1, 1, 1‚Ä¶\n$ MS_Zoning_Residential_Medium_Density                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_A_agr                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_C_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_I_all                                       &lt;dbl&gt; 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "categorical-dummy.html#python-examples",
    "href": "categorical-dummy.html#python-examples",
    "title": "17¬† Dummy Encoding",
    "section": "17.6 Python Examples",
    "text": "17.6 Python Examples"
  },
  {
    "objectID": "categorical-label.html#pros-and-cons",
    "href": "categorical-label.html#pros-and-cons",
    "title": "18¬† Label Encoding",
    "section": "18.1 Pros and Cons",
    "text": "18.1 Pros and Cons\n\n18.1.1 Pros\n\nOnly produces a single numeric variable for each categorical variable\nHas a way to handle unseen levels, although poorly\n\n\n\n18.1.2 Cons\n\nOrdering of the levels matters a lot!\nWill very often give inferior performance compared to other methods."
  },
  {
    "objectID": "categorical-label.html#r-examples",
    "href": "categorical-label.html#r-examples",
    "title": "18¬† Label Encoding",
    "section": "18.2 R Examples",
    "text": "18.2 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 √ó 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ‚Ñπ 2,920 more rows\n\n\nLooking at the levels of MS_SubClass we see that levels are set in a specific way. It isn‚Äôt alphabetical, but there isn‚Äôt one clear order. No clarification of the ordering can be done in the data documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt.\n\names |&gt; pull(MS_SubClass) |&gt; levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_integer() step for this, which defaults to 1-based indexing\n\nlabel_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_integer(all_nominal_predictors()) |&gt;\n  prep()\n\nlabel_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\"))\n\n# A tibble: 2,930 √ó 2\n   MS_SubClass MS_Zoning\n         &lt;int&gt;     &lt;int&gt;\n 1           1         3\n 2           1         2\n 3           1         3\n 4           1         3\n 5           6         3\n 6           6         3\n 7          12         3\n 8          12         3\n 9          12         3\n10           6         3\n# ‚Ñπ 2,920 more rows"
  },
  {
    "objectID": "categorical-label.html#python-examples",
    "href": "categorical-label.html#python-examples",
    "title": "18¬† Label Encoding",
    "section": "18.3 Python Examples",
    "text": "18.3 Python Examples"
  },
  {
    "objectID": "categorical-ordinal.html#pros-and-cons",
    "href": "categorical-ordinal.html#pros-and-cons",
    "title": "19¬† Ordinal Encoding",
    "section": "19.1 Pros and Cons",
    "text": "19.1 Pros and Cons\n\n19.1.1 Pros\n\nOnly produces a single numeric variable for each categorical variable\nPreserves the natural ordering of ordered values\n\n\n\n19.1.2 Cons\n\nWill very often give inferior performance compared to other methods\nUnseen levels need to be manually specified"
  },
  {
    "objectID": "categorical-ordinal.html#r-examples",
    "href": "categorical-ordinal.html#r-examples",
    "title": "19¬† Ordinal Encoding",
    "section": "19.2 R Examples",
    "text": "19.2 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allows us to perform dummy encoding and one-hot encoding.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Lot_Shape, Land_Slope)\n\n# A tibble: 2,930 √ó 2\n   Lot_Shape          Land_Slope\n   &lt;fct&gt;              &lt;fct&gt;     \n 1 Slightly_Irregular Gtl       \n 2 Regular            Gtl       \n 3 Slightly_Irregular Gtl       \n 4 Regular            Gtl       \n 5 Slightly_Irregular Gtl       \n 6 Slightly_Irregular Gtl       \n 7 Regular            Gtl       \n 8 Slightly_Irregular Gtl       \n 9 Slightly_Irregular Gtl       \n10 Regular            Gtl       \n# ‚Ñπ 2,920 more rows\n\n\nLooking at the levels of Lot_Shape and Land_Slope we see that they match the levels in the documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt. Furthermore, these variables are listed as ordinal, they just aren‚Äôt denoted like this in this data set.\n\names |&gt; pull(Lot_Shape) |&gt; levels()\n\n[1] \"Regular\"              \"Slightly_Irregular\"   \"Moderately_Irregular\"\n[4] \"Irregular\"           \n\names |&gt; pull(Land_Slope) |&gt; levels()\n\n[1] \"Gtl\" \"Mod\" \"Sev\"\n\n\nWe will fix that by turning them into ordered factors.\n\names &lt;- ames |&gt;\n  mutate(across(.cols = c(Lot_Shape, Land_Slope), .fns = as.ordered))\n\nto perform ordinal encoding we will use the step_ordinalscore() step. This defaults to giving each level values between 1 and n, much like step_integer().\n\nordinal_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_ordinalscore(Lot_Shape, Land_Slope) |&gt;\n  prep()\n\nordinal_rec |&gt;\n  bake(new_data = NULL, starts_with(\"Lot_Shape\"), starts_with(\"Land_Slope\"))\n\n# A tibble: 2,930 √ó 2\n   Lot_Shape Land_Slope\n       &lt;int&gt;      &lt;int&gt;\n 1         2          1\n 2         1          1\n 3         2          1\n 4         1          1\n 5         2          1\n 6         2          1\n 7         1          1\n 8         2          1\n 9         2          1\n10         1          1\n# ‚Ñπ 2,920 more rows\n\n\nWhat we can do is define a special transformation function for each of the steps. One way is to use the case_when() function\n\nLot_Shape_transformer &lt;- function(x) {\n  case_when(\n    x == \"Regular\" ~ 0, \n    x == \"Slightly_Irregular\" ~ -1,\n    x == \"Moderately_Irregular\" ~ -5,\n    x == \"Irregular\" ~ -10\n  )\n}\n\nIf you have the values for each of the levels as a vector or data, you can write the function to use that information as well.\n\nLand_Slope_values &lt;- c(Gtl = 0, Mod = 1, Sev = 5)\n\nLand_Slope_transformer &lt;- function(x) {\n  Land_Slope_values[x]\n}\n\nWith these functions, we can now apply them to the respective columns by using the convert argument.\n\nordinal_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_ordinalscore(Lot_Shape, convert = Lot_Shape_transformer) |&gt;\n  step_ordinalscore(Land_Slope, convert = Land_Slope_transformer) |&gt;\n  prep()\n\nordinal_rec |&gt;\n  bake(new_data = NULL, starts_with(\"Lot_Shape\"), starts_with(\"Land_Slope\")) |&gt;\n  distinct()\n\n# A tibble: 11 √ó 2\n   Lot_Shape Land_Slope\n       &lt;int&gt;      &lt;int&gt;\n 1        -1          0\n 2         0          0\n 3        -5          1\n 4        -1          1\n 5         0          1\n 6        -5          0\n 7         0          5\n 8        -1          5\n 9       -10          0\n10        -5          5\n11       -10          5"
  },
  {
    "objectID": "categorical-ordinal.html#python-examples",
    "href": "categorical-ordinal.html#python-examples",
    "title": "19¬† Ordinal Encoding",
    "section": "19.3 Python Examples",
    "text": "19.3 Python Examples"
  },
  {
    "objectID": "categorical-binary.html#pros-and-cons",
    "href": "categorical-binary.html#pros-and-cons",
    "title": "20¬† Binary Encoding",
    "section": "20.1 Pros and Cons",
    "text": "20.1 Pros and Cons\n\n20.1.1 Pros\n\nuses fewer variables to store the same information as dummy encoding\n\n\n\n20.1.2 Cons\n\nLess interpretability compared to dummy variables"
  },
  {
    "objectID": "categorical-binary.html#r-examples",
    "href": "categorical-binary.html#r-examples",
    "title": "20¬† Binary Encoding",
    "section": "20.2 R Examples",
    "text": "20.2 R Examples\nWe will be using the ames data set for these examples. The step_encoding_binary() function from the extrasteps package allows us to perform binary encoding.\n\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 √ó 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ‚Ñπ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 √ó 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nWe can then apply binary encoding using step_encoding_binary(). Notice how we only get 1 numeric variable for each categorical variable\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_encoding_binary(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 9\n$ MS_SubClass_1  &lt;int&gt; 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1‚Ä¶\n$ MS_SubClass_2  &lt;int&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0‚Ä¶\n$ MS_SubClass_4  &lt;int&gt; 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0‚Ä¶\n$ MS_SubClass_8  &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0‚Ä¶\n$ MS_SubClass_16 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_1    &lt;int&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ MS_Zoning_2    &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1‚Ä¶\n$ MS_Zoning_4    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ MS_Zoning_8    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n\n\nWe can pull the number of distinct levels of each variable by using tidy().\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 40 √ó 3\n   terms        value id                   \n   &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;                \n 1 MS_SubClass     16 encoding_binary_Bp5vK\n 2 MS_Zoning        7 encoding_binary_Bp5vK\n 3 Street           2 encoding_binary_Bp5vK\n 4 Alley            3 encoding_binary_Bp5vK\n 5 Lot_Shape        4 encoding_binary_Bp5vK\n 6 Land_Contour     4 encoding_binary_Bp5vK\n 7 Utilities        3 encoding_binary_Bp5vK\n 8 Lot_Config       5 encoding_binary_Bp5vK\n 9 Land_Slope       3 encoding_binary_Bp5vK\n10 Neighborhood    29 encoding_binary_Bp5vK\n# ‚Ñπ 30 more rows"
  },
  {
    "objectID": "categorical-binary.html#python-examples",
    "href": "categorical-binary.html#python-examples",
    "title": "20¬† Binary Encoding",
    "section": "20.3 Python Examples",
    "text": "20.3 Python Examples\nhttp://contrib.scikit-learn.org/category_encoders/binary.html"
  },
  {
    "objectID": "categorical-frequency.html#pros-and-cons",
    "href": "categorical-frequency.html#pros-and-cons",
    "title": "21¬† Frequency Encoding",
    "section": "21.1 Pros and Cons",
    "text": "21.1 Pros and Cons\n\n21.1.1 Pros\n\nPowerful and simple when used correctly\nHigh interpretability\n\n\n\n21.1.2 Cons\n\nIs not able to distinguish between two levels that have the same frequency\nMay not add predictive power"
  },
  {
    "objectID": "categorical-frequency.html#r-examples",
    "href": "categorical-frequency.html#r-examples",
    "title": "21¬† Frequency Encoding",
    "section": "21.2 R Examples",
    "text": "21.2 R Examples\nWe will be using the ames data set for these examples. The step_encoding_frequency() function from the extrasteps package allows us to perform frequency encoding.\n\nlibrary(recipes)\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 √ó 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        &lt;int&gt; &lt;fct&gt;                               &lt;fct&gt;                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# ‚Ñπ 2,920 more rows\n\n\nWe can take a quick look at the possible values MS_SubClass takes\n\names |&gt;\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 √ó 2\n   MS_SubClass                                   n\n   &lt;fct&gt;                                     &lt;int&gt;\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nWe can then apply frequency encoding using step_encoding_frequency(). Notice how we only get 1 numeric variable for each categorical variable\n\ndummy_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_encoding_frequency(all_nominal_predictors()) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 2\n$ MS_SubClass &lt;dbl&gt; 0.36825939, 0.36825939, 0.36825939, 0.36825939, 0.19624573‚Ä¶\n$ MS_Zoning   &lt;dbl&gt; 0.775767918, 0.009215017, 0.775767918, 0.775767918, 0.7757‚Ä¶\n\n\nWe can pull the frequencies for each level of each variable by using tidy().\n\ndummy_rec |&gt;\n  tidy(1)\n\n# A tibble: 283 √ó 4\n   terms       level                                  frequency id              \n   &lt;chr&gt;       &lt;chr&gt;                                      &lt;dbl&gt; &lt;chr&gt;           \n 1 MS_SubClass One_Story_1946_and_Newer_All_Styles      0.368   encoding_freque‚Ä¶\n 2 MS_SubClass One_Story_1945_and_Older                 0.0474  encoding_freque‚Ä¶\n 3 MS_SubClass One_Story_with_Finished_Attic_All_Ages   0.00205 encoding_freque‚Ä¶\n 4 MS_SubClass One_and_Half_Story_Unfinished_All_Ages   0.00614 encoding_freque‚Ä¶\n 5 MS_SubClass One_and_Half_Story_Finished_All_Ages     0.0980  encoding_freque‚Ä¶\n 6 MS_SubClass Two_Story_1946_and_Newer                 0.196   encoding_freque‚Ä¶\n 7 MS_SubClass Two_Story_1945_and_Older                 0.0437  encoding_freque‚Ä¶\n 8 MS_SubClass Two_and_Half_Story_All_Ages              0.00785 encoding_freque‚Ä¶\n 9 MS_SubClass Split_or_Multilevel                      0.0403  encoding_freque‚Ä¶\n10 MS_SubClass Split_Foyer                              0.0164  encoding_freque‚Ä¶\n# ‚Ñπ 273 more rows"
  },
  {
    "objectID": "categorical-frequency.html#python-examples",
    "href": "categorical-frequency.html#python-examples",
    "title": "21¬† Frequency Encoding",
    "section": "21.3 Python Examples",
    "text": "21.3 Python Examples"
  },
  {
    "objectID": "categorical-target.html#pros-and-cons",
    "href": "categorical-target.html#pros-and-cons",
    "title": "22¬† Target Encoding",
    "section": "22.1 Pros and Cons",
    "text": "22.1 Pros and Cons\n\n22.1.1 Pros\n\nCan deal with categorical variables with many levels\nCan deal with unseen levels in a sensible way\n\n\n\n22.1.2 Cons\n\nCan be prone to overfitting"
  },
  {
    "objectID": "categorical-target.html#r-examples",
    "href": "categorical-target.html#r-examples",
    "title": "22¬† Target Encoding",
    "section": "22.2 R Examples",
    "text": "22.2 R Examples\nThe embed package comes with a couple of functions to do target encoding. step_lencode_glm(), step_lencode_bayes() and step_lencode_mixed(). These functions are named such because they likelihood encode variables, and because the encodings can be calculated using no intercept models.\nstep_lencode_glm() implements the no-smoothing method, so we will look at that one first using the ames data set.\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\n\nlibrary(recipes)\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrec_target &lt;- recipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_lencode_glm(Neighborhood, outcome = vars(Sale_Price)) |&gt;\n  prep()\n\nrec_target |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Neighborhood Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1      145097.     215000\n 2      145097.     105000\n 3      145097.     172000\n 4      145097.     244000\n 5      190647.     189900\n 6      190647.     195500\n 7      324229.     213500\n 8      324229.     191500\n 9      324229.     236500\n10      190647.     189000\n# ‚Ñπ 2,920 more rows\n\n\nAnd we see that it works as intended, we can pull out the exact levels using the tidy() method\n\nrec_target |&gt;\n  tidy(1)\n\n# A tibble: 29 √ó 4\n   level                value terms        id               \n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;            \n 1 North_Ames         145097. Neighborhood lencode_glm_Bp5vK\n 2 College_Creek      201803. Neighborhood lencode_glm_Bp5vK\n 3 Old_Town           123992. Neighborhood lencode_glm_Bp5vK\n 4 Edwards            130843. Neighborhood lencode_glm_Bp5vK\n 5 Somerset           229707. Neighborhood lencode_glm_Bp5vK\n 6 Northridge_Heights 322018. Neighborhood lencode_glm_Bp5vK\n 7 Gilbert            190647. Neighborhood lencode_glm_Bp5vK\n 8 Sawyer             136751. Neighborhood lencode_glm_Bp5vK\n 9 Northwest_Ames     188407. Neighborhood lencode_glm_Bp5vK\n10 Sawyer_West        184070. Neighborhood lencode_glm_Bp5vK\n# ‚Ñπ 19 more rows\n\n\nto apply smoothing we can use the step_lencode_mixed() step in the same way\n\nrec_target_smooth &lt;- recipe(Sale_Price ~ Neighborhood, data = ames) |&gt;\n  step_lencode_mixed(Neighborhood, outcome = vars(Sale_Price)) |&gt;\n  prep()\n\nrec_target_smooth |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Neighborhood Sale_Price\n          &lt;dbl&gt;      &lt;int&gt;\n 1      145156.     215000\n 2      145156.     105000\n 3      145156.     172000\n 4      145156.     244000\n 5      190633.     189900\n 6      190633.     195500\n 7      322591.     213500\n 8      322591.     191500\n 9      322591.     236500\n10      190633.     189000\n# ‚Ñπ 2,920 more rows\n\n\nWe see that these values are slightly different than the values we had earlier\n\nrec_target_smooth |&gt;\n  tidy(1)\n\n# A tibble: 29 √ó 4\n   level                value terms        id                 \n   &lt;chr&gt;                &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;              \n 1 North_Ames         145156. Neighborhood lencode_mixed_RUieL\n 2 College_Creek      201769. Neighborhood lencode_mixed_RUieL\n 3 Old_Town           124154. Neighborhood lencode_mixed_RUieL\n 4 Edwards            131021. Neighborhood lencode_mixed_RUieL\n 5 Somerset           229563. Neighborhood lencode_mixed_RUieL\n 6 Northridge_Heights 321519. Neighborhood lencode_mixed_RUieL\n 7 Gilbert            190633. Neighborhood lencode_mixed_RUieL\n 8 Sawyer             136956. Neighborhood lencode_mixed_RUieL\n 9 Northwest_Ames     188401. Neighborhood lencode_mixed_RUieL\n10 Sawyer_West        184085. Neighborhood lencode_mixed_RUieL\n# ‚Ñπ 19 more rows"
  },
  {
    "objectID": "categorical-target.html#python-examples",
    "href": "categorical-target.html#python-examples",
    "title": "22¬† Target Encoding",
    "section": "22.3 Python Examples",
    "text": "22.3 Python Examples\nhttps://contrib.scikit-learn.org/category_encoders/targetencoder.html\n\n\n\n\nMicci-Barreca, Daniele. 2001. ‚ÄúA Preprocessing Scheme for High-Cardinality Categorical Attributes in Classification and Prediction Problems.‚Äù SIGKDD Explor. Newsl. 3 (1): 27‚Äì32. https://doi.org/10.1145/507533.507538."
  },
  {
    "objectID": "categorical-hashing.html#pros-and-cons",
    "href": "categorical-hashing.html#pros-and-cons",
    "title": "23¬† Hashing Encoding",
    "section": "23.1 Pros and Cons",
    "text": "23.1 Pros and Cons\n\n23.1.1 Pros\n\nComputationally fast\nAllows for a fixed number of output columns\ngives less sparse output than dummy encoding\n\n\n\n23.1.2 Cons\n\nLoss of interpretability\nStill gives quite sparse output"
  },
  {
    "objectID": "categorical-hashing.html#r-examples",
    "href": "categorical-hashing.html#r-examples",
    "title": "23¬† Hashing Encoding",
    "section": "23.2 R Examples",
    "text": "23.2 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a higher cardinality data set for this\n\n\nWe will be using the ames data set for these examples. The step_dummy_hash() function from the textrecipes package allows us to perform hashing encoding.\n\nlibrary(recipes)\nlibrary(textrecipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |&gt;\n  select(Sale_Price, Exterior_1st)\n\n# A tibble: 2,930 √ó 2\n   Sale_Price Exterior_1st\n        &lt;int&gt; &lt;fct&gt;       \n 1     215000 BrkFace     \n 2     105000 VinylSd     \n 3     172000 Wd Sdng     \n 4     244000 BrkFace     \n 5     189900 VinylSd     \n 6     195500 VinylSd     \n 7     213500 CemntBd     \n 8     191500 HdBoard     \n 9     236500 CemntBd     \n10     189000 VinylSd     \n# ‚Ñπ 2,920 more rows\n\n\nWe will be using the step_dummy_hash() step for this. For illustrative purposes, we will be creating 8 columns, where in practice you would likely want this value higher.\n\ndummy_rec &lt;- recipe(Sale_Price ~ Exterior_1st, data = ames) |&gt;\n  step_dummy_hash(Exterior_1st, num_terms = 8) |&gt;\n  prep()\n\ndummy_rec |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 2,930\nColumns: 9\n$ Sale_Price               &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 19550‚Ä¶\n$ dummyhash_Exterior_1st_1 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, -1, -1, 0, -1,‚Ä¶\n$ dummyhash_Exterior_1st_2 &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ dummyhash_Exterior_1st_3 &lt;int&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ dummyhash_Exterior_1st_4 &lt;int&gt; -1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ dummyhash_Exterior_1st_5 &lt;int&gt; 0, 0, 0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0‚Ä¶\n$ dummyhash_Exterior_1st_6 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ dummyhash_Exterior_1st_7 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ dummyhash_Exterior_1st_8 &lt;int&gt; 0, -1, 0, 0, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0‚Ä¶"
  },
  {
    "objectID": "categorical-hashing.html#python-examples",
    "href": "categorical-hashing.html#python-examples",
    "title": "23¬† Hashing Encoding",
    "section": "23.3 Python Examples",
    "text": "23.3 Python Examples"
  },
  {
    "objectID": "categorical-leaveoneout.html#pros-and-cons",
    "href": "categorical-leaveoneout.html#pros-and-cons",
    "title": "24¬† üèóÔ∏è Leave One Out Encoding",
    "section": "24.1 Pros and Cons",
    "text": "24.1 Pros and Cons\n\n24.1.1 Pros\n\n\n24.1.2 Cons"
  },
  {
    "objectID": "categorical-leaveoneout.html#r-examples",
    "href": "categorical-leaveoneout.html#r-examples",
    "title": "24¬† üèóÔ∏è Leave One Out Encoding",
    "section": "24.2 R Examples",
    "text": "24.2 R Examples"
  },
  {
    "objectID": "categorical-leaveoneout.html#python-examples",
    "href": "categorical-leaveoneout.html#python-examples",
    "title": "24¬† üèóÔ∏è Leave One Out Encoding",
    "section": "24.3 Python Examples",
    "text": "24.3 Python Examples"
  },
  {
    "objectID": "categorical-leaf.html#pros-and-cons",
    "href": "categorical-leaf.html#pros-and-cons",
    "title": "25¬† üèóÔ∏è Leaf Encoding",
    "section": "25.1 Pros and Cons",
    "text": "25.1 Pros and Cons\n\n25.1.1 Pros\n\n\n25.1.2 Cons"
  },
  {
    "objectID": "categorical-leaf.html#r-examples",
    "href": "categorical-leaf.html#r-examples",
    "title": "25¬† üèóÔ∏è Leaf Encoding",
    "section": "25.2 R Examples",
    "text": "25.2 R Examples"
  },
  {
    "objectID": "categorical-leaf.html#python-examples",
    "href": "categorical-leaf.html#python-examples",
    "title": "25¬† üèóÔ∏è Leaf Encoding",
    "section": "25.3 Python Examples",
    "text": "25.3 Python Examples"
  },
  {
    "objectID": "categorical-glmm.html#pros-and-cons",
    "href": "categorical-glmm.html#pros-and-cons",
    "title": "26¬† üèóÔ∏è GLMM Encoding",
    "section": "26.1 Pros and Cons",
    "text": "26.1 Pros and Cons\n\n26.1.1 Pros\n\n\n26.1.2 Cons"
  },
  {
    "objectID": "categorical-glmm.html#r-examples",
    "href": "categorical-glmm.html#r-examples",
    "title": "26¬† üèóÔ∏è GLMM Encoding",
    "section": "26.2 R Examples",
    "text": "26.2 R Examples"
  },
  {
    "objectID": "categorical-glmm.html#python-examples",
    "href": "categorical-glmm.html#python-examples",
    "title": "26¬† üèóÔ∏è GLMM Encoding",
    "section": "26.3 Python Examples",
    "text": "26.3 Python Examples"
  },
  {
    "objectID": "categorical-catboost.html#pros-and-cons",
    "href": "categorical-catboost.html#pros-and-cons",
    "title": "27¬† üèóÔ∏è Catboost Encoding",
    "section": "27.1 Pros and Cons",
    "text": "27.1 Pros and Cons\n\n27.1.1 Pros\n\n\n27.1.2 Cons"
  },
  {
    "objectID": "categorical-catboost.html#r-examples",
    "href": "categorical-catboost.html#r-examples",
    "title": "27¬† üèóÔ∏è Catboost Encoding",
    "section": "27.2 R Examples",
    "text": "27.2 R Examples"
  },
  {
    "objectID": "categorical-catboost.html#python-examples",
    "href": "categorical-catboost.html#python-examples",
    "title": "27¬† üèóÔ∏è Catboost Encoding",
    "section": "27.3 Python Examples",
    "text": "27.3 Python Examples"
  },
  {
    "objectID": "categorical-woe.html#pros-and-cons",
    "href": "categorical-woe.html#pros-and-cons",
    "title": "28¬† üèóÔ∏è Weight of Evidence Encoding",
    "section": "28.1 Pros and Cons",
    "text": "28.1 Pros and Cons\n\n28.1.1 Pros\n\n\n28.1.2 Cons"
  },
  {
    "objectID": "categorical-woe.html#r-examples",
    "href": "categorical-woe.html#r-examples",
    "title": "28¬† üèóÔ∏è Weight of Evidence Encoding",
    "section": "28.2 R Examples",
    "text": "28.2 R Examples"
  },
  {
    "objectID": "categorical-woe.html#python-examples",
    "href": "categorical-woe.html#python-examples",
    "title": "28¬† üèóÔ∏è Weight of Evidence Encoding",
    "section": "28.3 Python Examples",
    "text": "28.3 Python Examples"
  },
  {
    "objectID": "categorical-jamesstein.html#pros-and-cons",
    "href": "categorical-jamesstein.html#pros-and-cons",
    "title": "29¬† üèóÔ∏è James-Stein Encoding",
    "section": "29.1 Pros and Cons",
    "text": "29.1 Pros and Cons\n\n29.1.1 Pros\n\n\n29.1.2 Cons"
  },
  {
    "objectID": "categorical-jamesstein.html#r-examples",
    "href": "categorical-jamesstein.html#r-examples",
    "title": "29¬† üèóÔ∏è James-Stein Encoding",
    "section": "29.2 R Examples",
    "text": "29.2 R Examples"
  },
  {
    "objectID": "categorical-jamesstein.html#python-examples",
    "href": "categorical-jamesstein.html#python-examples",
    "title": "29¬† üèóÔ∏è James-Stein Encoding",
    "section": "29.3 Python Examples",
    "text": "29.3 Python Examples"
  },
  {
    "objectID": "categorical-mestimator.html#pros-and-cons",
    "href": "categorical-mestimator.html#pros-and-cons",
    "title": "30¬† üèóÔ∏è M-Estimator Encoding",
    "section": "30.1 Pros and Cons",
    "text": "30.1 Pros and Cons\n\n30.1.1 Pros\n\n\n30.1.2 Cons"
  },
  {
    "objectID": "categorical-mestimator.html#r-examples",
    "href": "categorical-mestimator.html#r-examples",
    "title": "30¬† üèóÔ∏è M-Estimator Encoding",
    "section": "30.2 R Examples",
    "text": "30.2 R Examples"
  },
  {
    "objectID": "categorical-mestimator.html#python-examples",
    "href": "categorical-mestimator.html#python-examples",
    "title": "30¬† üèóÔ∏è M-Estimator Encoding",
    "section": "30.3 Python Examples",
    "text": "30.3 Python Examples"
  },
  {
    "objectID": "categorical-thermometer.html#pros-and-cons",
    "href": "categorical-thermometer.html#pros-and-cons",
    "title": "31¬† üèóÔ∏è Thermometer Encoding",
    "section": "31.1 Pros and Cons",
    "text": "31.1 Pros and Cons\n\n31.1.1 Pros\n\n\n31.1.2 Cons"
  },
  {
    "objectID": "categorical-thermometer.html#r-examples",
    "href": "categorical-thermometer.html#r-examples",
    "title": "31¬† üèóÔ∏è Thermometer Encoding",
    "section": "31.2 R Examples",
    "text": "31.2 R Examples"
  },
  {
    "objectID": "categorical-thermometer.html#python-examples",
    "href": "categorical-thermometer.html#python-examples",
    "title": "31¬† üèóÔ∏è Thermometer Encoding",
    "section": "31.3 Python Examples",
    "text": "31.3 Python Examples"
  },
  {
    "objectID": "categorical-quantile.html#pros-and-cons",
    "href": "categorical-quantile.html#pros-and-cons",
    "title": "32¬† üèóÔ∏è Quantile Encoding",
    "section": "32.1 Pros and Cons",
    "text": "32.1 Pros and Cons\n\n32.1.1 Pros\n\n\n32.1.2 Cons"
  },
  {
    "objectID": "categorical-quantile.html#r-examples",
    "href": "categorical-quantile.html#r-examples",
    "title": "32¬† üèóÔ∏è Quantile Encoding",
    "section": "32.2 R Examples",
    "text": "32.2 R Examples"
  },
  {
    "objectID": "categorical-quantile.html#python-examples",
    "href": "categorical-quantile.html#python-examples",
    "title": "32¬† üèóÔ∏è Quantile Encoding",
    "section": "32.3 Python Examples",
    "text": "32.3 Python Examples"
  },
  {
    "objectID": "categorical-summary.html#pros-and-cons",
    "href": "categorical-summary.html#pros-and-cons",
    "title": "33¬† üèóÔ∏è Summary Encoding",
    "section": "33.1 Pros and Cons",
    "text": "33.1 Pros and Cons\n\n33.1.1 Pros\n\n\n33.1.2 Cons"
  },
  {
    "objectID": "categorical-summary.html#r-examples",
    "href": "categorical-summary.html#r-examples",
    "title": "33¬† üèóÔ∏è Summary Encoding",
    "section": "33.2 R Examples",
    "text": "33.2 R Examples"
  },
  {
    "objectID": "categorical-summary.html#python-examples",
    "href": "categorical-summary.html#python-examples",
    "title": "33¬† üèóÔ∏è Summary Encoding",
    "section": "33.3 Python Examples",
    "text": "33.3 Python Examples"
  },
  {
    "objectID": "categorical-collapse.html#pros-and-cons",
    "href": "categorical-collapse.html#pros-and-cons",
    "title": "34¬† Collapsing Categories",
    "section": "34.1 Pros and Cons",
    "text": "34.1 Pros and Cons\n\n34.1.1 Pros\n\nEasy to perform and verify\nComputationally fast\n\n\n\n34.1.2 Cons\n\nMust be tuned\nCan produce counterintuitive results"
  },
  {
    "objectID": "categorical-collapse.html#r-examples",
    "href": "categorical-collapse.html#r-examples",
    "title": "34¬† Collapsing Categories",
    "section": "34.2 R Examples",
    "text": "34.2 R Examples\nMethods to collapse categorical levels can be found in the recipes package with step_other() and the embed package in step_collapse_cart() and step_collapse_stringdist().\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set for examples\n\n\nOthering can be done using the step_other() function, it uses the argument threshold to determine the cutoff used to turn levels into \"other\" or not.\n\nlibrary(recipes)\nlibrary(embed)\n\ndata(ames, package = \"modeldata\")\n\nrec_other &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_other(Exterior_2nd, threshold = 0.1) |&gt;\n  prep()\n\nrec_other |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 other            215000\n 2 VinylSd          105000\n 3 Wd Sdng          172000\n 4 other            244000\n 5 VinylSd          189900\n 6 VinylSd          195500\n 7 other            213500\n 8 HdBoard          191500\n 9 other            236500\n10 VinylSd          189000\n# ‚Ñπ 2,920 more rows\n\n\nselecting a higher threshold turns more levels into \"other\".\n\nrec_other &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_other(Exterior_2nd, threshold = 0.5) |&gt;\n  prep()\n\nrec_other |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 other            215000\n 2 VinylSd          105000\n 3 other            172000\n 4 other            244000\n 5 VinylSd          189900\n 6 VinylSd          195500\n 7 other            213500\n 8 other            191500\n 9 other            236500\n10 VinylSd          189000\n# ‚Ñπ 2,920 more rows\n\n\nfor the more advanced methods, we turn to the embed package. To collapse levels by their string distance, we use the step_collapse_stringdist(). By default, you control it with the distance argument.\n\nrec_stringdist &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_collapse_stringdist(Exterior_2nd, distance = 5) |&gt;\n  prep()\n\nrec_stringdist |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 Plywood          215000\n 2 MetalSd          105000\n 3 AsbShng          172000\n 4 Brk Cmn          244000\n 5 MetalSd          189900\n 6 MetalSd          195500\n 7 CmentBd          213500\n 8 HdBoard          191500\n 9 CmentBd          236500\n10 MetalSd          189000\n# ‚Ñπ 2,920 more rows\n\n\nUnsurprisingly, there are almost a dozen different ways to calculate the distance between two strings. Most are supported and can be changed using the method argument, and further controlled using the options argument.\n\nrec_stringdist &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_collapse_stringdist(Exterior_2nd, \n                           distance = 0.75, \n                           method = \"cosine\", \n                           options = list(q = 2)) |&gt;\n  prep()\n\nrec_stringdist |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Exterior_2nd Sale_Price\n   &lt;fct&gt;             &lt;int&gt;\n 1 Plywood          215000\n 2 MetalSd          105000\n 3 AsbShng          172000\n 4 Brk Cmn          244000\n 5 MetalSd          189900\n 6 MetalSd          195500\n 7 CmentBd          213500\n 8 HdBoard          191500\n 9 CmentBd          236500\n10 MetalSd          189000\n# ‚Ñπ 2,920 more rows\n\n\nLastly, we have the tree-based method, this is done using the step_collapse_cart() function. For this to work, you need to select an outcome variable using the outcome argument. With cost_complexity and min_n as arguments to change the shape of the tree.\n\nrec_cart &lt;- recipe(Sale_Price ~ Exterior_2nd, data = ames) |&gt;\n  step_collapse_cart(Exterior_2nd, outcome = vars(Sale_Price)) |&gt;\n  prep()\n\nrec_cart |&gt;\n  bake(new_data = NULL)\n\n# A tibble: 2,930 √ó 2\n   Exterior_2nd   Sale_Price\n   &lt;fct&gt;               &lt;int&gt;\n 1 Exterior_2nd_5     215000\n 2 Exterior_2nd_7     105000\n 3 Exterior_2nd_3     172000\n 4 Exterior_2nd_6     244000\n 5 Exterior_2nd_7     189900\n 6 Exterior_2nd_7     195500\n 7 Exterior_2nd_8     213500\n 8 Exterior_2nd_5     191500\n 9 Exterior_2nd_8     236500\n10 Exterior_2nd_7     189000\n# ‚Ñπ 2,920 more rows"
  },
  {
    "objectID": "categorical-collapse.html#python-examples",
    "href": "categorical-collapse.html#python-examples",
    "title": "34¬† Collapsing Categories",
    "section": "34.3 Python Examples",
    "text": "34.3 Python Examples\nhttps://github.com/skrub-data/skrub"
  },
  {
    "objectID": "categorical-combination.html#pros-and-cons",
    "href": "categorical-combination.html#pros-and-cons",
    "title": "35¬† üèóÔ∏è Combination",
    "section": "35.1 Pros and Cons",
    "text": "35.1 Pros and Cons\n\n35.1.1 Pros\n\n\n35.1.2 Cons"
  },
  {
    "objectID": "categorical-combination.html#r-examples",
    "href": "categorical-combination.html#r-examples",
    "title": "35¬† üèóÔ∏è Combination",
    "section": "35.2 R Examples",
    "text": "35.2 R Examples"
  },
  {
    "objectID": "categorical-combination.html#python-examples",
    "href": "categorical-combination.html#python-examples",
    "title": "35¬† üèóÔ∏è Combination",
    "section": "35.3 Python Examples",
    "text": "35.3 Python Examples"
  },
  {
    "objectID": "categorical-multi-dummy.html#pros-and-cons",
    "href": "categorical-multi-dummy.html#pros-and-cons",
    "title": "36¬† üèóÔ∏è Multi-Dummy encoding",
    "section": "36.1 Pros and Cons",
    "text": "36.1 Pros and Cons\n\n36.1.1 Pros\n\n\n36.1.2 Cons"
  },
  {
    "objectID": "categorical-multi-dummy.html#r-examples",
    "href": "categorical-multi-dummy.html#r-examples",
    "title": "36¬† üèóÔ∏è Multi-Dummy encoding",
    "section": "36.2 R Examples",
    "text": "36.2 R Examples"
  },
  {
    "objectID": "categorical-multi-dummy.html#python-examples",
    "href": "categorical-multi-dummy.html#python-examples",
    "title": "36¬† üèóÔ∏è Multi-Dummy encoding",
    "section": "36.3 Python Examples",
    "text": "36.3 Python Examples"
  },
  {
    "objectID": "datetime.html",
    "href": "datetime.html",
    "title": "37¬† Overview",
    "section": "",
    "text": "Date and datetime variables are another type of data that can be quite common. This is different than time as we talked about in Chapter¬†111, as when we talk about time series data, it is typically a series of data points that are related, and we use that inherent structure of the data as the basis for the modeling problem. Here we are talking about predictors, that happen be to expressed as a date or datetime field.\nThese types of data could be the day where the sale took place, or when the taxi started and ended its trip. In some of these cases, it wouldn‚Äôt make sense to treat it as a time-series model, but we still want to be able to pull out valuable information. In many cases, date and datetime variables will be treated as text fields if they are unparsed, and as fancy integer representations. When encoded they typically use integers to denote time since a specific reference point.\nIf the date and datetime variables were used directly in a modeling function it would at best use the underlying integer representation, which is unlikely to be useful since it just denotes chronological time. At worst the modeling function will error and complain.\nThe chapters in this section are going to assume that you have parsed the date and datetime variables and that we are working with those directly.\nWhen we talk about extraction in Chapter¬†38, what we will be doing is extracting components of the data. This can be things like year, month, day, hour, minutes and seconds. There are more complicated things like ‚ÄúIs this a holiday?‚Äù or ‚ÄúIs it a weekend?‚Äù. After that, we will go over some more complicated features in Chapter¬†39. These features will mostly be based on the extraction features from earlier. But it can be things like ‚Äúclosest holiday‚Äù and ‚Äúhow long since last Monday‚Äù. Lastly, we will talk about how many of these features work in a very circular way in Chapter¬†40. Naturally, if we were to model using hours of the day, 1 hour before midnight and 1 hour after midnight are close to the clock but not numerically."
  },
  {
    "objectID": "datetime-extraction.html#pros-and-cons",
    "href": "datetime-extraction.html#pros-and-cons",
    "title": "38¬† Value Extraction",
    "section": "38.1 Pros and Cons",
    "text": "38.1 Pros and Cons\n\n38.1.1 Pros\n\nFast and easy computations\nCan provide good results\n\n\n\n38.1.2 Cons\n\nThe numerical features generated are all increasing with time linearly\nThere are a lot of extractions, and they correlate quite a bit"
  },
  {
    "objectID": "datetime-extraction.html#r-examples",
    "href": "datetime-extraction.html#r-examples",
    "title": "38¬† Value Extraction",
    "section": "38.2 R Examples",
    "text": "38.2 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\nlibrary(recipes)\n\nhotel_bookings |&gt;\n  select(reservation_status_date)\n\n# A tibble: 119,390 √ó 1\n   reservation_status_date\n   &lt;date&gt;                 \n 1 2015-07-01             \n 2 2015-07-01             \n 3 2015-07-02             \n 4 2015-07-02             \n 5 2015-07-03             \n 6 2015-07-03             \n 7 2015-07-03             \n 8 2015-07-03             \n 9 2015-05-06             \n10 2015-04-22             \n# ‚Ñπ 119,380 more rows\n\n\n{recipes} provide two steps for date time extraction. step_date() handles dates, and step_time() handles the sub-day time features. The steps work the same way, so we will only show how step_date() works here. A couple of features are selected by default,\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date)\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 5\n$ reservation_status_date       &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, 201‚Ä¶\n$ is_canceled                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0‚Ä¶\n$ reservation_status_date_dow   &lt;fct&gt; Wed, Wed, Thu, Thu, Fri, Fri, Fri, Fri, ‚Ä¶\n$ reservation_status_date_month &lt;fct&gt; Jul, Jul, Jul, Jul, Jul, Jul, Jul, Jul, ‚Ä¶\n$ reservation_status_date_year  &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015‚Ä¶\n\n\nBut you can use the features argument to specify other types as well\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date, \n            features = c( \"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                          \"quarter\", \"dow\", \"month\"))\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 10\n$ reservation_status_date          &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, ‚Ä¶\n$ is_canceled                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0‚Ä¶\n$ reservation_status_date_year     &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2‚Ä¶\n$ reservation_status_date_doy      &lt;int&gt; 182, 182, 183, 183, 184, 184, 184, 18‚Ä¶\n$ reservation_status_date_week     &lt;int&gt; 26, 26, 27, 27, 27, 27, 27, 27, 18, 1‚Ä¶\n$ reservation_status_date_decimal  &lt;dbl&gt; 2015.496, 2015.496, 2015.499, 2015.49‚Ä¶\n$ reservation_status_date_semester &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ reservation_status_date_quarter  &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3‚Ä¶\n$ reservation_status_date_dow      &lt;fct&gt; Wed, Wed, Thu, Thu, Fri, Fri, Fri, Fr‚Ä¶\n$ reservation_status_date_month    &lt;fct&gt; Jul, Jul, Jul, Jul, Jul, Jul, Jul, Ju‚Ä¶\n\n\nfeatures that can be categorical will be so by default, but can be turned off by setting label = FALSE.\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_date(reservation_status_date, \n            features = c( \"year\", \"doy\", \"week\", \"decimal\", \"semester\", \n                          \"quarter\", \"dow\", \"month\"), label = FALSE)\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 10\n$ reservation_status_date          &lt;date&gt; 2015-07-01, 2015-07-01, 2015-07-02, ‚Ä¶\n$ is_canceled                      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0‚Ä¶\n$ reservation_status_date_year     &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2‚Ä¶\n$ reservation_status_date_doy      &lt;int&gt; 182, 182, 183, 183, 184, 184, 184, 18‚Ä¶\n$ reservation_status_date_week     &lt;int&gt; 26, 26, 27, 27, 27, 27, 27, 27, 18, 1‚Ä¶\n$ reservation_status_date_decimal  &lt;dbl&gt; 2015.496, 2015.496, 2015.499, 2015.49‚Ä¶\n$ reservation_status_date_semester &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2‚Ä¶\n$ reservation_status_date_quarter  &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 3‚Ä¶\n$ reservation_status_date_dow      &lt;int&gt; 4, 4, 5, 5, 6, 6, 6, 6, 4, 4, 3, 1, 1‚Ä¶\n$ reservation_status_date_month    &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 5, 4, 6, 7, 7‚Ä¶\n\n\nIf we want to extract holiday features, we can use the step_holiday() function, which uses the {timeDate} library. With known holidays listed in timeDate::listHolidays().\n\ndate_rec &lt;- recipe(is_canceled ~ reservation_status_date, \n                   data = hotel_bookings) |&gt;\n  step_holiday(reservation_status_date, \n               holidays = c(\"BoxingDay\", \"CAFamilyDay\", \"JPConstitutionDay\"))\n\ndate_rec |&gt;\n  prep() |&gt;\n  bake(new_data = NULL) |&gt;\n  glimpse()\n\nRows: 119,390\nColumns: 5\n$ reservation_status_date                   &lt;date&gt; 2015-07-01, 2015-07-01, 201‚Ä¶\n$ is_canceled                               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ reservation_status_date_BoxingDay         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ reservation_status_date_CAFamilyDay       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ reservation_status_date_JPConstitutionDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶"
  },
  {
    "objectID": "datetime-extraction.html#python-examples",
    "href": "datetime-extraction.html#python-examples",
    "title": "38¬† Value Extraction",
    "section": "38.3 Python Examples",
    "text": "38.3 Python Examples"
  },
  {
    "objectID": "datetime-advanced.html#pros-and-cons",
    "href": "datetime-advanced.html#pros-and-cons",
    "title": "39¬† Advanced Features",
    "section": "39.1 Pros and Cons",
    "text": "39.1 Pros and Cons\n\n39.1.1 Pros\n\n\n39.1.2 Cons"
  },
  {
    "objectID": "datetime-advanced.html#r-examples",
    "href": "datetime-advanced.html#r-examples",
    "title": "39¬† Advanced Features",
    "section": "39.2 R Examples",
    "text": "39.2 R Examples"
  },
  {
    "objectID": "datetime-advanced.html#python-examples",
    "href": "datetime-advanced.html#python-examples",
    "title": "39¬† Advanced Features",
    "section": "39.3 Python Examples",
    "text": "39.3 Python Examples"
  },
  {
    "objectID": "datetime-circular.html#pros-and-cons",
    "href": "datetime-circular.html#pros-and-cons",
    "title": "40¬† Circular Features",
    "section": "40.1 Pros and Cons",
    "text": "40.1 Pros and Cons\n\n40.1.1 Pros\n\nThe nature of datetime variables naturally leads to circular patterns\n\n\n\n40.1.2 Cons\n\nExtra care and work need to be done to find the right period"
  },
  {
    "objectID": "datetime-circular.html#r-examples",
    "href": "datetime-circular.html#r-examples",
    "title": "40¬† Circular Features",
    "section": "40.2 R Examples",
    "text": "40.2 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nAdd recipes steps"
  },
  {
    "objectID": "datetime-circular.html#python-examples",
    "href": "datetime-circular.html#python-examples",
    "title": "40¬† Circular Features",
    "section": "40.3 Python Examples",
    "text": "40.3 Python Examples"
  },
  {
    "objectID": "missing.html#imputation",
    "href": "missing.html#imputation",
    "title": "41¬† Overview",
    "section": "41.1 Imputation",
    "text": "41.1 Imputation\nOne of the most common ways of dealing with missing values is to fill them in with some values. The types of methods that do this can be split into two groups. Simple imputation in Chapter¬†42 is when you use the values in the variable to impute its missing values, which is where mean and mode imputation are found. Anything more complicated than this will be found in Chapter¬†43. This is where multiple columns are used to determine the type of imputation needed."
  },
  {
    "objectID": "missing.html#indication",
    "href": "missing.html#indication",
    "title": "41¬† Overview",
    "section": "41.2 Indication",
    "text": "41.2 Indication\nIf you suspect that the data is not missing at random, it might be worthwhile to include the missingness as an indicator in your data. We will see how we can do that in Chapter¬†44."
  },
  {
    "objectID": "missing.html#removal",
    "href": "missing.html#removal",
    "title": "41¬† Overview",
    "section": "41.3 Removal",
    "text": "41.3 Removal\nAs a last resort, you might want to remove variables or rows with missing data, we will see how that is done in Chapter¬†45. This chapter is put last in this section, as it is generally not the preferred action, and all other avenues should be considered before removal is done."
  },
  {
    "objectID": "missing-simple.html#pros-and-cons",
    "href": "missing-simple.html#pros-and-cons",
    "title": "42¬† Simple Imputation",
    "section": "42.1 Pros and Cons",
    "text": "42.1 Pros and Cons\n\n42.1.1 Pros\n\nFast computationally\nEasy to explain what was done\n\n\n\n42.1.2 Cons\n\nDoesn‚Äôt preserve relationship between predictors\nreducing the variance and standard deviation of the data\nunlikely to help performance"
  },
  {
    "objectID": "missing-simple.html#r-examples",
    "href": "missing-simple.html#r-examples",
    "title": "42¬† Simple Imputation",
    "section": "42.2 R Examples",
    "text": "42.2 R Examples\nThe recipes package contains several steps. It includes the steps step_impute_mean(), step_impute_median() and step_impute_mode() which imputes by the mean, median and mode respectively.\n\n\n\n\n\n\nTODO\n\n\n\nfind a good data set with missing values\n\n\n\nlibrary(recipes)\nlibrary(modeldata)\n\nimpute_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_impute_mean(contains(\"Area\")) |&gt;\n  step_impute_median(contains(\"_SF\")) |&gt;\n  step_impute_mode(all_nominal_predictors()) |&gt;\n  prep()\n\nWe can use the tidy() function to find the estimated mean\n\nimpute_rec |&gt;\n  tidy(1)\n\n# A tibble: 5 √ó 3\n  terms         value id               \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;            \n1 Lot_Area     10148  impute_mean_CnWw4\n2 Mas_Vnr_Area   101. impute_mean_CnWw4\n3 Gr_Liv_Area   1500  impute_mean_CnWw4\n4 Garage_Area    473. impute_mean_CnWw4\n5 Pool_Area        2  impute_mean_CnWw4\n\n\nestimated median\n\nimpute_rec |&gt;\n  tidy(2)\n\n# A tibble: 8 √ó 3\n  terms         value id                 \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;              \n1 BsmtFin_SF_1     3  impute_median_UoO9a\n2 BsmtFin_SF_2     0  impute_median_UoO9a\n3 Bsmt_Unf_SF    466. impute_median_UoO9a\n4 Total_Bsmt_SF  990  impute_median_UoO9a\n5 First_Flr_SF  1084  impute_median_UoO9a\n6 Second_Flr_SF    0  impute_median_UoO9a\n7 Wood_Deck_SF     0  impute_median_UoO9a\n8 Open_Porch_SF   27  impute_median_UoO9a\n\n\nand estimated mode\n\nimpute_rec |&gt;\n  tidy(3)\n\n# A tibble: 40 √ó 3\n   terms        value                               id               \n   &lt;chr&gt;        &lt;chr&gt;                               &lt;chr&gt;            \n 1 MS_SubClass  One_Story_1946_and_Newer_All_Styles impute_mode_I3pVC\n 2 MS_Zoning    Residential_Low_Density             impute_mode_I3pVC\n 3 Street       Pave                                impute_mode_I3pVC\n 4 Alley        No_Alley_Access                     impute_mode_I3pVC\n 5 Lot_Shape    Regular                             impute_mode_I3pVC\n 6 Land_Contour Lvl                                 impute_mode_I3pVC\n 7 Utilities    AllPub                              impute_mode_I3pVC\n 8 Lot_Config   Inside                              impute_mode_I3pVC\n 9 Land_Slope   Gtl                                 impute_mode_I3pVC\n10 Neighborhood North_Ames                          impute_mode_I3pVC\n# ‚Ñπ 30 more rows\n\n\n\n\n\n\n\n\nTODO\n\n\n\nwait for the distribution step"
  },
  {
    "objectID": "missing-simple.html#python-examples",
    "href": "missing-simple.html#python-examples",
    "title": "42¬† Simple Imputation",
    "section": "42.3 Python Examples",
    "text": "42.3 Python Examples"
  },
  {
    "objectID": "missing-model.html#pros-and-cons",
    "href": "missing-model.html#pros-and-cons",
    "title": "43¬† Model Based Imputation",
    "section": "43.1 Pros and Cons",
    "text": "43.1 Pros and Cons\n\n43.1.1 Pros\n\nLikely get better performance than simple imputation\n\n\n\n43.1.2 Cons\n\nMore complex model\nlower interpretability"
  },
  {
    "objectID": "missing-model.html#r-examples",
    "href": "missing-model.html#r-examples",
    "title": "43¬† Model Based Imputation",
    "section": "43.2 R Examples",
    "text": "43.2 R Examples\nThere are a number of steps in the recipes package that fall under this category. Within that, we have step_impute_bag(), step_impute_knn(), and step_impute_linear().\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nBelow we are showing how we can impute using a K-nearest neighbor model using step_impute_knn(). We specify the variable to impute on first, and then with impute_with we specify which variables are used as predictors in the model.\n\nlibrary(recipes)\n\nimpute_knn_rec &lt;- recipe(mpg ~ ., data = mtcars) |&gt;\n  step_impute_knn(disp, neighbors = 1, impute_with = imp_vars(vs, am, hp, drat))\n\nimpute_knn_rec |&gt;\n  prep() |&gt;\n  juice()\n\n# A tibble: 32 √ó 11\n     cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb   mpg\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     6  160    110  3.9   2.62  16.5     0     1     4     4  21  \n 2     6  160    110  3.9   2.88  17.0     0     1     4     4  21  \n 3     4  108     93  3.85  2.32  18.6     1     1     4     1  22.8\n 4     6  258    110  3.08  3.22  19.4     1     0     3     1  21.4\n 5     8  360    175  3.15  3.44  17.0     0     0     3     2  18.7\n 6     6  225    105  2.76  3.46  20.2     1     0     3     1  18.1\n 7     8  360    245  3.21  3.57  15.8     0     0     3     4  14.3\n 8     4  147.    62  3.69  3.19  20       1     0     4     2  24.4\n 9     4  141.    95  3.92  3.15  22.9     1     0     4     2  22.8\n10     6  168.   123  3.92  3.44  18.3     1     0     4     4  19.2\n# ‚Ñπ 22 more rows"
  },
  {
    "objectID": "missing-model.html#python-examples",
    "href": "missing-model.html#python-examples",
    "title": "43¬† Model Based Imputation",
    "section": "43.3 Python Examples",
    "text": "43.3 Python Examples"
  },
  {
    "objectID": "missing-indicator.html#pros-and-cons",
    "href": "missing-indicator.html#pros-and-cons",
    "title": "44¬† Missing Values Indicators",
    "section": "44.1 Pros and Cons",
    "text": "44.1 Pros and Cons\n\n44.1.1 Pros\n\nNo performance harm when added to variables with no missing data\nSimple and interpretable\n\n\n\n44.1.2 Cons\n\nWill produce zero variance columns when used on data with no missing values\nCan create a sizable increase in data set size"
  },
  {
    "objectID": "missing-indicator.html#r-examples",
    "href": "missing-indicator.html#r-examples",
    "title": "44¬† Missing Values Indicators",
    "section": "44.2 R Examples",
    "text": "44.2 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nFrom the recipes package, can we use the step_indicate_na() function to create indicator variables based on missing data\n\nlibrary(recipes)\n\nna_ind_rec &lt;- recipe(mpg ~ disp + vs + am, data = mtcars) |&gt;\n  step_indicate_na(all_predictors()) |&gt;\n  prep()\n\n\nna_ind_rec |&gt;\n  bake(new_data = mtcars)\n\n# A tibble: 32 √ó 7\n    disp    vs    am   mpg na_ind_disp na_ind_vs na_ind_am\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;       &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n 1  160      0     1  21             0         0         0\n 2  160      0     1  21             0         0         0\n 3  108      1     1  22.8           0         0         0\n 4  258      1     0  21.4           0         0         0\n 5  360      0     0  18.7           0         0         0\n 6  225      1     0  18.1           0         0         0\n 7  360      0     0  14.3           0         0         0\n 8  147.     1     0  24.4           0         0         0\n 9  141.     1     0  22.8           0         0         0\n10  168.     1     0  19.2           0         0         0\n# ‚Ñπ 22 more rows"
  },
  {
    "objectID": "missing-indicator.html#python-examples",
    "href": "missing-indicator.html#python-examples",
    "title": "44¬† Missing Values Indicators",
    "section": "44.3 Python Examples",
    "text": "44.3 Python Examples"
  },
  {
    "objectID": "missing-remove.html#pros-and-cons",
    "href": "missing-remove.html#pros-and-cons",
    "title": "45¬† Remove Missing Values",
    "section": "45.1 Pros and Cons",
    "text": "45.1 Pros and Cons\n\n45.1.1 Pros\n\nWill sometimes be necessary\n\n\n\n45.1.2 Cons\n\nExtreme care has to be taken\nLoss of data"
  },
  {
    "objectID": "missing-remove.html#r-examples",
    "href": "missing-remove.html#r-examples",
    "title": "45¬† Remove Missing Values",
    "section": "45.2 R Examples",
    "text": "45.2 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind data set\n\n\nWe can use the step_naomit() function from the recipes package to remove any observation that contains missing values.\n\nlibrary(recipes)\n\nnaomit_rec &lt;- recipe(~., data = mtcars) |&gt;\n  step_naomit(all_predictors()) |&gt;\n  prep()\n\nbake(naomit_rec, new_data = NULL)\n\n# A tibble: 32 √ó 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ‚Ñπ 22 more rows\n\n\n\n\n\n\n\n\nTODO\n\n\n\nwait for thresholded observation removal\n\n\nThere is the step_filter_missing() function from the recipes package that removes predictors with more missing values than the specified threshold\n\nlibrary(recipes)\n\nfilter_missing_rec &lt;- recipe(~., data = mtcars) |&gt;\n  step_filter_missing(all_predictors()) |&gt;\n  prep()\n\nbake(filter_missing_rec, new_data = NULL)\n\n# A tibble: 32 √ó 11\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3     1\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3     2\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3     1\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3     4\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4     2\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4     4\n# ‚Ñπ 22 more rows"
  },
  {
    "objectID": "missing-remove.html#python-examples",
    "href": "missing-remove.html#python-examples",
    "title": "45¬† Remove Missing Values",
    "section": "45.3 Python Examples",
    "text": "45.3 Python Examples"
  },
  {
    "objectID": "text.html#text-cleaning",
    "href": "text.html#text-cleaning",
    "title": "46¬† Overview",
    "section": "46.1 Text cleaning",
    "text": "46.1 Text cleaning\nIn Chapter¬†48, we will look over the ways we take raw text and get it ready for later tasks. This work deals with encoding issues, standardization, and cases and sometimes you need to get rid of a lot of unwanted chunks."
  },
  {
    "objectID": "text.html#tokenization",
    "href": "text.html#tokenization",
    "title": "46¬† Overview",
    "section": "46.2 Tokenization",
    "text": "46.2 Tokenization\nOnce the text is cleaned, we need to split it into a smaller unit of information such that we can count it, this is called tokenization and we will visit that in Chapter¬†49."
  },
  {
    "objectID": "text.html#modifying-tokens",
    "href": "text.html#modifying-tokens",
    "title": "46¬† Overview",
    "section": "46.3 Modifying tokens",
    "text": "46.3 Modifying tokens\nOnce you have the data as tokens, one of the things you might want to do is modify them in various ways. This could be things like changing the endings to words or changing the words entirely. We see examples of this in Chapter¬†50 and Chapter¬†51."
  },
  {
    "objectID": "text.html#filtering-tokens",
    "href": "text.html#filtering-tokens",
    "title": "46¬† Overview",
    "section": "46.4 Filtering tokens",
    "text": "46.4 Filtering tokens\nThe tokens you create might not all be of the same quality. Depending on your choice of tokenizer, there will be reasons for you to remove some of the tokens you have created. We see examples of this in Chapter¬†52 and Chapter¬†53."
  },
  {
    "objectID": "text.html#counting-tokens",
    "href": "text.html#counting-tokens",
    "title": "46¬† Overview",
    "section": "46.5 Counting tokens",
    "text": "46.5 Counting tokens\nWe have gotten to the end of the line and we are ready to turn the tokens into numeric variables we can use. There are many different ways we look at them in Chapter¬†54, Chapter¬†55, Chapter¬†56, Chapter¬†57, and Chapter¬†58."
  },
  {
    "objectID": "text.html#embeddings",
    "href": "text.html#embeddings",
    "title": "46¬† Overview",
    "section": "46.6 Embeddings",
    "text": "46.6 Embeddings\nAnother way to use text is to work with embeddings, this is another powerful tool that can give you good performance. We look at some of them in Chapter¬†59 and Chapter¬†60."
  },
  {
    "objectID": "text-manual.html#pros-and-cons",
    "href": "text-manual.html#pros-and-cons",
    "title": "47¬† Manual Text Features",
    "section": "47.1 Pros and Cons",
    "text": "47.1 Pros and Cons\n\n47.1.1 Pros\n\nClear and actionable features\nHigh interpretability\n\n\n\n47.1.2 Cons\n\nCan be time-consuming to create\nComputational speed depends on the feature\nWill likely need to"
  },
  {
    "objectID": "text-manual.html#r-examples",
    "href": "text-manual.html#r-examples",
    "title": "47¬† Manual Text Features",
    "section": "47.2 R Examples",
    "text": "47.2 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nfind a better data set\n\n\nThe textfeatures package is one package in R that contains a bunch of general features that may or may not be useful.\n\nlibrary(textfeatures)\nlibrary(modeldata)\n\ntextfeatures(modeldata::tate_text$medium, word_dims = 0, \n             verbose = FALSE) |&gt;\n  dplyr::glimpse()\n\nRows: 4,284\nColumns: 34\n$ n_urls           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_uq_urls        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_hashtags       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_uq_hashtags    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_mentions       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_uq_mentions    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_chars          &lt;dbl&gt; 1.39951429, -0.91391731, -0.91391731, -0.91391731, -0‚Ä¶\n$ n_uq_chars       &lt;dbl&gt; 1.3463720, -0.2656168, -0.2656168, -0.2656168, -0.585‚Ä¶\n$ n_commas         &lt;dbl&gt; 1.2867430, -0.6470182, -0.6470182, -0.6470182, -0.647‚Ä¶\n$ n_digits         &lt;dbl&gt; -0.2800874, -0.2800874, -0.2800874, -0.2800874, -0.28‚Ä¶\n$ n_exclaims       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_extraspaces    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_lowers         &lt;dbl&gt; 1.348546506, -0.912127069, -0.912127069, -0.912127069‚Ä¶\n$ n_lowersp        &lt;dbl&gt; -1.0518721, 0.1014681, 0.1014681, 0.1014681, 0.352584‚Ä¶\n$ n_periods        &lt;dbl&gt; -0.04324894, -0.04324894, -0.04324894, -0.04324894, -‚Ä¶\n$ n_words          &lt;dbl&gt; 1.3593949, -0.7937823, -0.7937823, -0.7937823, -0.162‚Ä¶\n$ n_uq_words       &lt;dbl&gt; 1.4230658, -0.7920930, -0.7920930, -0.7920930, -0.142‚Ä¶\n$ n_caps           &lt;dbl&gt; -0.04050572, -0.04050572, -0.04050572, -0.04050572, -‚Ä¶\n$ n_nonasciis      &lt;dbl&gt; -0.02646899, -0.02646899, -0.02646899, -0.02646899, -‚Ä¶\n$ n_puncts         &lt;dbl&gt; 5.6233563, -0.2031327, -0.2031327, -0.2031327, -0.203‚Ä¶\n$ n_capsp          &lt;dbl&gt; -1.2508524, 0.8890397, 0.8890397, 0.8890397, 0.538919‚Ä¶\n$ n_charsperword   &lt;dbl&gt; 1.09976675, -0.87544061, -0.87544061, -0.87544061, -1‚Ä¶\n$ sent_afinn       &lt;dbl&gt; 0.01511448, 0.01511448, 0.01511448, 0.01511448, 0.015‚Ä¶\n$ sent_bing        &lt;dbl&gt; -0.07864915, -0.07864915, -0.07864915, -0.07864915, -‚Ä¶\n$ sent_syuzhet     &lt;dbl&gt; -0.1334035, -0.1334035, -0.1334035, -0.1334035, -0.13‚Ä¶\n$ sent_vader       &lt;dbl&gt; -0.06711618, -0.06711618, -0.06711618, -0.06711618, -‚Ä¶\n$ n_polite         &lt;dbl&gt; 0.05597655, 0.05597655, 0.05597655, 0.05597655, 0.055‚Ä¶\n$ n_first_person   &lt;dbl&gt; -0.01527831, -0.01527831, -0.01527831, -0.01527831, -‚Ä¶\n$ n_first_personp  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_second_person  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_second_personp &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_third_person   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_tobe           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ n_prepositions   &lt;dbl&gt; -2.3324219, 0.3482094, 0.3482094, 0.3482094, 0.348209‚Ä¶\n\n\n\n\n\n\n\n\nTODO\n\n\n\nCome up with domain-specific examples"
  },
  {
    "objectID": "text-manual.html#python-examples",
    "href": "text-manual.html#python-examples",
    "title": "47¬† Manual Text Features",
    "section": "47.3 Python Examples",
    "text": "47.3 Python Examples"
  },
  {
    "objectID": "text-cleaning.html#pros-and-cons",
    "href": "text-cleaning.html#pros-and-cons",
    "title": "48¬† üèóÔ∏è Text cleaning",
    "section": "48.1 Pros and Cons",
    "text": "48.1 Pros and Cons\n\n48.1.1 Pros\n\nWhen applied correctly, can lead to boosts in insights into the data\n\n\n\n48.1.2 Cons\n\nCan be a quite manual process which will likely be domain specific"
  },
  {
    "objectID": "text-cleaning.html#r-examples",
    "href": "text-cleaning.html#r-examples",
    "title": "48¬† üèóÔ∏è Text cleaning",
    "section": "48.2 R Examples",
    "text": "48.2 R Examples\n\n\n\n\n\n\nTODO\n\n\n\nFind a data set that isn‚Äôt clean\n\n\nWe will use the step_text_normalization() function from the {textrecipes} package to perform unicode normalization, which defaults to the NFC normalization form.\n\nlibrary(textrecipes)\n\nLoading required package: recipes\n\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nsample_data &lt;- tibble(text = c(\"sch\\U00f6n\", \"scho\\U0308n\"))\n\nsample_data |&gt;\n  count(text)\n\n# A tibble: 2 √ó 2\n  text      n\n  &lt;chr&gt; &lt;int&gt;\n1 schoÃàn     1\n2 sch√∂n     1\n\nrec &lt;- recipe(~., data = sample_data) |&gt;\n  step_text_normalization(text) |&gt;\n  prep()\n\nrec |&gt;\n  bake(new_data = NULL) |&gt;\n  count(text)\n\n# A tibble: 1 √ó 2\n  text      n\n  &lt;fct&gt; &lt;int&gt;\n1 sch√∂n     2"
  },
  {
    "objectID": "text-cleaning.html#python-examples",
    "href": "text-cleaning.html#python-examples",
    "title": "48¬† üèóÔ∏è Text cleaning",
    "section": "48.3 Python Examples",
    "text": "48.3 Python Examples"
  },
  {
    "objectID": "text-tokenization.html#pros-and-cons",
    "href": "text-tokenization.html#pros-and-cons",
    "title": "49¬† üèóÔ∏è Tokenization",
    "section": "49.1 Pros and Cons",
    "text": "49.1 Pros and Cons\n\n49.1.1 Pros\n\n\n49.1.2 Cons"
  },
  {
    "objectID": "text-tokenization.html#r-examples",
    "href": "text-tokenization.html#r-examples",
    "title": "49¬† üèóÔ∏è Tokenization",
    "section": "49.2 R Examples",
    "text": "49.2 R Examples"
  },
  {
    "objectID": "text-tokenization.html#python-examples",
    "href": "text-tokenization.html#python-examples",
    "title": "49¬† üèóÔ∏è Tokenization",
    "section": "49.3 Python Examples",
    "text": "49.3 Python Examples"
  },
  {
    "objectID": "text-stemming.html#pros-and-cons",
    "href": "text-stemming.html#pros-and-cons",
    "title": "50¬† üèóÔ∏è Stemming",
    "section": "50.1 Pros and Cons",
    "text": "50.1 Pros and Cons\n\n50.1.1 Pros\n\n\n50.1.2 Cons"
  },
  {
    "objectID": "text-stemming.html#r-examples",
    "href": "text-stemming.html#r-examples",
    "title": "50¬† üèóÔ∏è Stemming",
    "section": "50.2 R Examples",
    "text": "50.2 R Examples"
  },
  {
    "objectID": "text-stemming.html#python-examples",
    "href": "text-stemming.html#python-examples",
    "title": "50¬† üèóÔ∏è Stemming",
    "section": "50.3 Python Examples",
    "text": "50.3 Python Examples"
  },
  {
    "objectID": "text-ngrams.html#pros-and-cons",
    "href": "text-ngrams.html#pros-and-cons",
    "title": "51¬† üèóÔ∏è N-grams",
    "section": "51.1 Pros and Cons",
    "text": "51.1 Pros and Cons\n\n51.1.1 Pros\n\n\n51.1.2 Cons"
  },
  {
    "objectID": "text-ngrams.html#r-examples",
    "href": "text-ngrams.html#r-examples",
    "title": "51¬† üèóÔ∏è N-grams",
    "section": "51.2 R Examples",
    "text": "51.2 R Examples"
  },
  {
    "objectID": "text-ngrams.html#python-examples",
    "href": "text-ngrams.html#python-examples",
    "title": "51¬† üèóÔ∏è N-grams",
    "section": "51.3 Python Examples",
    "text": "51.3 Python Examples"
  },
  {
    "objectID": "text-stopwords.html#pros-and-cons",
    "href": "text-stopwords.html#pros-and-cons",
    "title": "52¬† üèóÔ∏è Stop words",
    "section": "52.1 Pros and Cons",
    "text": "52.1 Pros and Cons\n\n52.1.1 Pros\n\n\n52.1.2 Cons"
  },
  {
    "objectID": "text-stopwords.html#r-examples",
    "href": "text-stopwords.html#r-examples",
    "title": "52¬† üèóÔ∏è Stop words",
    "section": "52.2 R Examples",
    "text": "52.2 R Examples"
  },
  {
    "objectID": "text-stopwords.html#python-examples",
    "href": "text-stopwords.html#python-examples",
    "title": "52¬† üèóÔ∏è Stop words",
    "section": "52.3 Python Examples",
    "text": "52.3 Python Examples"
  },
  {
    "objectID": "text-filter.html#pros-and-cons",
    "href": "text-filter.html#pros-and-cons",
    "title": "53¬† üèóÔ∏è Token filter",
    "section": "53.1 Pros and Cons",
    "text": "53.1 Pros and Cons\n\n53.1.1 Pros\n\n\n53.1.2 Cons"
  },
  {
    "objectID": "text-filter.html#r-examples",
    "href": "text-filter.html#r-examples",
    "title": "53¬† üèóÔ∏è Token filter",
    "section": "53.2 R Examples",
    "text": "53.2 R Examples"
  },
  {
    "objectID": "text-filter.html#python-examples",
    "href": "text-filter.html#python-examples",
    "title": "53¬† üèóÔ∏è Token filter",
    "section": "53.3 Python Examples",
    "text": "53.3 Python Examples"
  },
  {
    "objectID": "text-tf.html#pros-and-cons",
    "href": "text-tf.html#pros-and-cons",
    "title": "54¬† üèóÔ∏è Term Frequency",
    "section": "54.1 Pros and Cons",
    "text": "54.1 Pros and Cons\n\n54.1.1 Pros\n\n\n54.1.2 Cons"
  },
  {
    "objectID": "text-tf.html#r-examples",
    "href": "text-tf.html#r-examples",
    "title": "54¬† üèóÔ∏è Term Frequency",
    "section": "54.2 R Examples",
    "text": "54.2 R Examples"
  },
  {
    "objectID": "text-tf.html#python-examples",
    "href": "text-tf.html#python-examples",
    "title": "54¬† üèóÔ∏è Term Frequency",
    "section": "54.3 Python Examples",
    "text": "54.3 Python Examples"
  },
  {
    "objectID": "text-tfidf.html#pros-and-cons",
    "href": "text-tfidf.html#pros-and-cons",
    "title": "55¬† üèóÔ∏è TF-IDF",
    "section": "55.1 Pros and Cons",
    "text": "55.1 Pros and Cons\n\n55.1.1 Pros\n\n\n55.1.2 Cons"
  },
  {
    "objectID": "text-tfidf.html#r-examples",
    "href": "text-tfidf.html#r-examples",
    "title": "55¬† üèóÔ∏è TF-IDF",
    "section": "55.2 R Examples",
    "text": "55.2 R Examples"
  },
  {
    "objectID": "text-tfidf.html#python-examples",
    "href": "text-tfidf.html#python-examples",
    "title": "55¬† üèóÔ∏è TF-IDF",
    "section": "55.3 Python Examples",
    "text": "55.3 Python Examples"
  },
  {
    "objectID": "text-hashing.html#pros-and-cons",
    "href": "text-hashing.html#pros-and-cons",
    "title": "56¬† üèóÔ∏è Token Hashing",
    "section": "56.1 Pros and Cons",
    "text": "56.1 Pros and Cons\n\n56.1.1 Pros\n\n\n56.1.2 Cons"
  },
  {
    "objectID": "text-hashing.html#r-examples",
    "href": "text-hashing.html#r-examples",
    "title": "56¬† üèóÔ∏è Token Hashing",
    "section": "56.2 R Examples",
    "text": "56.2 R Examples"
  },
  {
    "objectID": "text-hashing.html#python-examples",
    "href": "text-hashing.html#python-examples",
    "title": "56¬† üèóÔ∏è Token Hashing",
    "section": "56.3 Python Examples",
    "text": "56.3 Python Examples"
  },
  {
    "objectID": "text-onehot.html#pros-and-cons",
    "href": "text-onehot.html#pros-and-cons",
    "title": "57¬† üèóÔ∏è Sequence Encoding",
    "section": "57.1 Pros and Cons",
    "text": "57.1 Pros and Cons\n\n57.1.1 Pros\n\n\n57.1.2 Cons"
  },
  {
    "objectID": "text-onehot.html#r-examples",
    "href": "text-onehot.html#r-examples",
    "title": "57¬† üèóÔ∏è Sequence Encoding",
    "section": "57.2 R Examples",
    "text": "57.2 R Examples"
  },
  {
    "objectID": "text-onehot.html#python-examples",
    "href": "text-onehot.html#python-examples",
    "title": "57¬† üèóÔ∏è Sequence Encoding",
    "section": "57.3 Python Examples",
    "text": "57.3 Python Examples"
  },
  {
    "objectID": "text-lda.html#pros-and-cons",
    "href": "text-lda.html#pros-and-cons",
    "title": "58¬† üèóÔ∏è LDA",
    "section": "58.1 Pros and Cons",
    "text": "58.1 Pros and Cons\n\n58.1.1 Pros\n\n\n58.1.2 Cons"
  },
  {
    "objectID": "text-lda.html#r-examples",
    "href": "text-lda.html#r-examples",
    "title": "58¬† üèóÔ∏è LDA",
    "section": "58.2 R Examples",
    "text": "58.2 R Examples"
  },
  {
    "objectID": "text-lda.html#python-examples",
    "href": "text-lda.html#python-examples",
    "title": "58¬† üèóÔ∏è LDA",
    "section": "58.3 Python Examples",
    "text": "58.3 Python Examples"
  },
  {
    "objectID": "text-word2vec.html#pros-and-cons",
    "href": "text-word2vec.html#pros-and-cons",
    "title": "59¬† üèóÔ∏è word2vec",
    "section": "59.1 Pros and Cons",
    "text": "59.1 Pros and Cons\n\n59.1.1 Pros\n\n\n59.1.2 Cons"
  },
  {
    "objectID": "text-word2vec.html#r-examples",
    "href": "text-word2vec.html#r-examples",
    "title": "59¬† üèóÔ∏è word2vec",
    "section": "59.2 R Examples",
    "text": "59.2 R Examples"
  },
  {
    "objectID": "text-word2vec.html#python-examples",
    "href": "text-word2vec.html#python-examples",
    "title": "59¬† üèóÔ∏è word2vec",
    "section": "59.3 Python Examples",
    "text": "59.3 Python Examples"
  },
  {
    "objectID": "text-bert.html#pros-and-cons",
    "href": "text-bert.html#pros-and-cons",
    "title": "60¬† üèóÔ∏è BERT",
    "section": "60.1 Pros and Cons",
    "text": "60.1 Pros and Cons\n\n60.1.1 Pros\n\n\n60.1.2 Cons"
  },
  {
    "objectID": "text-bert.html#r-examples",
    "href": "text-bert.html#r-examples",
    "title": "60¬† üèóÔ∏è BERT",
    "section": "60.2 R Examples",
    "text": "60.2 R Examples"
  },
  {
    "objectID": "text-bert.html#python-examples",
    "href": "text-bert.html#python-examples",
    "title": "60¬† üèóÔ∏è BERT",
    "section": "60.3 Python Examples",
    "text": "60.3 Python Examples"
  },
  {
    "objectID": "circular.html",
    "href": "circular.html",
    "title": "61¬† Overview",
    "section": "",
    "text": "When working with numeric variables, you see many different kinds of relationships. Positive, negative, non-linear. But a special type of relationship is the circular type. It is in essence a non-linear relationship, but specifically it relies on the assumption that the beginning of the domain behaves the same way as the end. Another assumption in this type of data is that the domain of values is restricted on the left and right sides.\nTypical examples of this are type-based, such as time of day, day of the week, day of the month, and day of the year. If we have an effect, we would imagine that the end and beginning are similar. Another example is directions, 1¬∞ of a circle is very close to 359¬∞ in reality. The goal of the chapters in this section is to use transformations to make sure they are close numerically in your model.\n\n\n\n\n\n\nTODO\n\n\n\nfind a good example of this\n\n\nThere are 2 main ways we can handle this. Harmonic calculations using trigonometric functions will be showcased in Chapter¬†62, Essentially what happens is that we are mapping the features to the unit circle in 2 dimensions. Another intriguing type of method is using periodic indicators such as splines or other methods. It doesn‚Äôt have to be splines, but if you carefully set the range of the indicators, you can get good performance. Splines are covered in Chapter¬†63, and the other more general methods are covered in Chapter¬†64."
  },
  {
    "objectID": "circular-trig.html#pros-and-cons",
    "href": "circular-trig.html#pros-and-cons",
    "title": "62¬† üèóÔ∏è Trigonometric",
    "section": "62.1 Pros and Cons",
    "text": "62.1 Pros and Cons\n\n62.1.1 Pros\n\n\n62.1.2 Cons"
  },
  {
    "objectID": "circular-trig.html#r-examples",
    "href": "circular-trig.html#r-examples",
    "title": "62¬† üèóÔ∏è Trigonometric",
    "section": "62.2 R Examples",
    "text": "62.2 R Examples"
  },
  {
    "objectID": "circular-trig.html#python-examples",
    "href": "circular-trig.html#python-examples",
    "title": "62¬† üèóÔ∏è Trigonometric",
    "section": "62.3 Python Examples",
    "text": "62.3 Python Examples"
  },
  {
    "objectID": "circular-splines.html#pros-and-cons",
    "href": "circular-splines.html#pros-and-cons",
    "title": "63¬† üèóÔ∏è Period Splines",
    "section": "63.1 Pros and Cons",
    "text": "63.1 Pros and Cons\n\n63.1.1 Pros\n\n\n63.1.2 Cons"
  },
  {
    "objectID": "circular-splines.html#r-examples",
    "href": "circular-splines.html#r-examples",
    "title": "63¬† üèóÔ∏è Period Splines",
    "section": "63.2 R Examples",
    "text": "63.2 R Examples"
  },
  {
    "objectID": "circular-splines.html#python-examples",
    "href": "circular-splines.html#python-examples",
    "title": "63¬† üèóÔ∏è Period Splines",
    "section": "63.3 Python Examples",
    "text": "63.3 Python Examples"
  },
  {
    "objectID": "circular-indicators.html#pros-and-cons",
    "href": "circular-indicators.html#pros-and-cons",
    "title": "64¬† üèóÔ∏è Periodic Indicators",
    "section": "64.1 Pros and Cons",
    "text": "64.1 Pros and Cons\n\n64.1.1 Pros\n\n\n64.1.2 Cons"
  },
  {
    "objectID": "circular-indicators.html#r-examples",
    "href": "circular-indicators.html#r-examples",
    "title": "64¬† üèóÔ∏è Periodic Indicators",
    "section": "64.2 R Examples",
    "text": "64.2 R Examples"
  },
  {
    "objectID": "circular-indicators.html#python-examples",
    "href": "circular-indicators.html#python-examples",
    "title": "64¬† üèóÔ∏è Periodic Indicators",
    "section": "64.3 Python Examples",
    "text": "64.3 Python Examples"
  },
  {
    "objectID": "too-many.html#non-zero-variance-filtering",
    "href": "too-many.html#non-zero-variance-filtering",
    "title": "65¬† Overview",
    "section": "65.1 Non-zero Variance filtering",
    "text": "65.1 Non-zero Variance filtering\nThese types of methods are quite simple, we remove variables that take a few number of values. If the value is always 1 then it doesn‚Äôt have any information in it and we should remove it. If the variables are almost always the same we might want to remove them. We look at these methods in Chapter¬†66."
  },
  {
    "objectID": "too-many.html#dimensionality-reduction",
    "href": "too-many.html#dimensionality-reduction",
    "title": "65¬† Overview",
    "section": "65.2 Dimensionality reduction",
    "text": "65.2 Dimensionality reduction\nThe bulk of the chapter will be in this category. This book categorizes dimensionality reduction methods as methods where a calculation is done on several features, with the same or fewer features being returned. Remember that we only look at methods that can be used in predictive settings, hence we won‚Äôt be talking about t-distributed stochastic neighbor embedding t-SNE.\n\nPrincipal Component Analysis (PCA) Chapter¬†67\nPCA variants Chapter¬†68\nIndependent Component Analysis (ICA) Chapter¬†69\nNon-negative matrix factorization (NMF) Chapter¬†70\nLinear discriminant analysis (LDA) Chapter¬†71\nGeneralized discriminant analysis (GDA) Chapter¬†72\nAutoencoders Chapter¬†73\nUniform Manifold Approximation and Projection (UMAP) Chapter¬†74\nISOMAP Chapter¬†75"
  },
  {
    "objectID": "too-many.html#feature-selection",
    "href": "too-many.html#feature-selection",
    "title": "65¬† Overview",
    "section": "65.3 Feature selection",
    "text": "65.3 Feature selection\nFeature selection on the other hand finds which variables to keep or remove. And then you act on that. This can be done in a couple of different ways. Filter-based approaches are covered in Chapter¬†76, these methods give each feature a score or rank, and then you use this information to select variables. There are many different ways to get these rankings and many will be covered in the chapter. Wrapper-based approaches are covered in Chapter¬†77. These methods iteratively look at subsets of data to try to find the best set. Their main downside is they tend to add a lot of computational overhead as you need to fit your model many times. Lastly, we have embedded methods which will be covered in Chapter¬†78. These methods use more advanced methods, sometimes other models, to do the feature selection."
  },
  {
    "objectID": "too-many-zv.html#pros-and-cons",
    "href": "too-many-zv.html#pros-and-cons",
    "title": "66¬† Zero Variance Filter",
    "section": "66.1 Pros and Cons",
    "text": "66.1 Pros and Cons\n\n66.1.1 Pros\n\nRemoving zero variance predictors should provide no downside\nFaster and smaller models\nEasy to explain and execute\n\n\n\n66.1.2 Cons\n\nRemoval of near-zero predictors requires care to avoid removing useful predictors"
  },
  {
    "objectID": "too-many-zv.html#r-examples",
    "href": "too-many-zv.html#r-examples",
    "title": "66¬† Zero Variance Filter",
    "section": "66.2 R Examples",
    "text": "66.2 R Examples\nWe will use the step_zv() and step_nzv() steps which are used to remove zero variance and near-zero variance preditors respectively.\n\n\n\n\n\n\nTODO\n\n\n\nfind a good data set\n\n\nBelow we are using the step_zv() function to remove\n\nlibrary(recipes)\n\ndata(\"ames\", package = \"modeldata\")\n\nzv_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_zv(all_predictors()) |&gt;\n  prep()\n\nWe can use the tidy() method to find out which variables were removed\n\nzv_rec |&gt;\n  tidy(1)\n\n# A tibble: 0 √ó 2\n# ‚Ñπ 2 variables: terms &lt;chr&gt;, id &lt;chr&gt;\n\n\nWe can remove non-zero variance predictors in the same manner using step_nzv()\n\nnzv_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_nzv(all_predictors()) |&gt;\n  prep()\n\nnzv_rec |&gt;\n  tidy(1)\n\n# A tibble: 21 √ó 2\n   terms          id       \n   &lt;chr&gt;          &lt;chr&gt;    \n 1 Street         nzv_RUieL\n 2 Alley          nzv_RUieL\n 3 Land_Contour   nzv_RUieL\n 4 Utilities      nzv_RUieL\n 5 Land_Slope     nzv_RUieL\n 6 Condition_2    nzv_RUieL\n 7 Roof_Matl      nzv_RUieL\n 8 Bsmt_Cond      nzv_RUieL\n 9 BsmtFin_Type_2 nzv_RUieL\n10 BsmtFin_SF_2   nzv_RUieL\n# ‚Ñπ 11 more rows"
  },
  {
    "objectID": "too-many-zv.html#python-examples",
    "href": "too-many-zv.html#python-examples",
    "title": "66¬† Zero Variance Filter",
    "section": "66.3 Python Examples",
    "text": "66.3 Python Examples"
  },
  {
    "objectID": "too-many-pca.html#pros-and-cons",
    "href": "too-many-pca.html#pros-and-cons",
    "title": "67¬† üèóÔ∏è Principal Component Analysis",
    "section": "67.1 Pros and Cons",
    "text": "67.1 Pros and Cons\n\n67.1.1 Pros\n\n\n67.1.2 Cons"
  },
  {
    "objectID": "too-many-pca.html#r-examples",
    "href": "too-many-pca.html#r-examples",
    "title": "67¬† üèóÔ∏è Principal Component Analysis",
    "section": "67.2 R Examples",
    "text": "67.2 R Examples"
  },
  {
    "objectID": "too-many-pca.html#python-examples",
    "href": "too-many-pca.html#python-examples",
    "title": "67¬† üèóÔ∏è Principal Component Analysis",
    "section": "67.3 Python Examples",
    "text": "67.3 Python Examples"
  },
  {
    "objectID": "too-many-pca-variants.html#pros-and-cons",
    "href": "too-many-pca-variants.html#pros-and-cons",
    "title": "68¬† üèóÔ∏è Principal Component Analysis Variants",
    "section": "68.1 Pros and Cons",
    "text": "68.1 Pros and Cons\n\n68.1.1 Pros\n\n\n68.1.2 Cons"
  },
  {
    "objectID": "too-many-pca-variants.html#r-examples",
    "href": "too-many-pca-variants.html#r-examples",
    "title": "68¬† üèóÔ∏è Principal Component Analysis Variants",
    "section": "68.2 R Examples",
    "text": "68.2 R Examples"
  },
  {
    "objectID": "too-many-pca-variants.html#python-examples",
    "href": "too-many-pca-variants.html#python-examples",
    "title": "68¬† üèóÔ∏è Principal Component Analysis Variants",
    "section": "68.3 Python Examples",
    "text": "68.3 Python Examples"
  },
  {
    "objectID": "too-many-ica.html#pros-and-cons",
    "href": "too-many-ica.html#pros-and-cons",
    "title": "69¬† üèóÔ∏è Independent Component Analysis",
    "section": "69.1 Pros and Cons",
    "text": "69.1 Pros and Cons\n\n69.1.1 Pros\n\n\n69.1.2 Cons"
  },
  {
    "objectID": "too-many-ica.html#r-examples",
    "href": "too-many-ica.html#r-examples",
    "title": "69¬† üèóÔ∏è Independent Component Analysis",
    "section": "69.2 R Examples",
    "text": "69.2 R Examples"
  },
  {
    "objectID": "too-many-ica.html#python-examples",
    "href": "too-many-ica.html#python-examples",
    "title": "69¬† üèóÔ∏è Independent Component Analysis",
    "section": "69.3 Python Examples",
    "text": "69.3 Python Examples"
  },
  {
    "objectID": "too-many-nmf.html#pros-and-cons",
    "href": "too-many-nmf.html#pros-and-cons",
    "title": "70¬† üèóÔ∏è Non-negative matrix factorization",
    "section": "70.1 Pros and Cons",
    "text": "70.1 Pros and Cons\n\n70.1.1 Pros\n\n\n70.1.2 Cons"
  },
  {
    "objectID": "too-many-nmf.html#r-examples",
    "href": "too-many-nmf.html#r-examples",
    "title": "70¬† üèóÔ∏è Non-negative matrix factorization",
    "section": "70.2 R Examples",
    "text": "70.2 R Examples"
  },
  {
    "objectID": "too-many-nmf.html#python-examples",
    "href": "too-many-nmf.html#python-examples",
    "title": "70¬† üèóÔ∏è Non-negative matrix factorization",
    "section": "70.3 Python Examples",
    "text": "70.3 Python Examples"
  },
  {
    "objectID": "too-many-lda.html#pros-and-cons",
    "href": "too-many-lda.html#pros-and-cons",
    "title": "71¬† üèóÔ∏è Linear discriminant analysis",
    "section": "71.1 Pros and Cons",
    "text": "71.1 Pros and Cons\n\n71.1.1 Pros\n\n\n71.1.2 Cons"
  },
  {
    "objectID": "too-many-lda.html#r-examples",
    "href": "too-many-lda.html#r-examples",
    "title": "71¬† üèóÔ∏è Linear discriminant analysis",
    "section": "71.2 R Examples",
    "text": "71.2 R Examples"
  },
  {
    "objectID": "too-many-lda.html#python-examples",
    "href": "too-many-lda.html#python-examples",
    "title": "71¬† üèóÔ∏è Linear discriminant analysis",
    "section": "71.3 Python Examples",
    "text": "71.3 Python Examples"
  },
  {
    "objectID": "too-many-gda.html#pros-and-cons",
    "href": "too-many-gda.html#pros-and-cons",
    "title": "72¬† üèóÔ∏è Generalized discriminant analysis",
    "section": "72.1 Pros and Cons",
    "text": "72.1 Pros and Cons\n\n72.1.1 Pros\n\n\n72.1.2 Cons"
  },
  {
    "objectID": "too-many-gda.html#r-examples",
    "href": "too-many-gda.html#r-examples",
    "title": "72¬† üèóÔ∏è Generalized discriminant analysis",
    "section": "72.2 R Examples",
    "text": "72.2 R Examples"
  },
  {
    "objectID": "too-many-gda.html#python-examples",
    "href": "too-many-gda.html#python-examples",
    "title": "72¬† üèóÔ∏è Generalized discriminant analysis",
    "section": "72.3 Python Examples",
    "text": "72.3 Python Examples"
  },
  {
    "objectID": "too-many-autoencoder.html#pros-and-cons",
    "href": "too-many-autoencoder.html#pros-and-cons",
    "title": "73¬† üèóÔ∏è Autoencoders",
    "section": "73.1 Pros and Cons",
    "text": "73.1 Pros and Cons\n\n73.1.1 Pros\n\n\n73.1.2 Cons"
  },
  {
    "objectID": "too-many-autoencoder.html#r-examples",
    "href": "too-many-autoencoder.html#r-examples",
    "title": "73¬† üèóÔ∏è Autoencoders",
    "section": "73.2 R Examples",
    "text": "73.2 R Examples"
  },
  {
    "objectID": "too-many-autoencoder.html#python-examples",
    "href": "too-many-autoencoder.html#python-examples",
    "title": "73¬† üèóÔ∏è Autoencoders",
    "section": "73.3 Python Examples",
    "text": "73.3 Python Examples"
  },
  {
    "objectID": "too-many-umap.html#pros-and-cons",
    "href": "too-many-umap.html#pros-and-cons",
    "title": "74¬† üèóÔ∏è Uniform Manifold Approximation and Projection",
    "section": "74.1 Pros and Cons",
    "text": "74.1 Pros and Cons\n\n74.1.1 Pros\n\n\n74.1.2 Cons"
  },
  {
    "objectID": "too-many-umap.html#r-examples",
    "href": "too-many-umap.html#r-examples",
    "title": "74¬† üèóÔ∏è Uniform Manifold Approximation and Projection",
    "section": "74.2 R Examples",
    "text": "74.2 R Examples"
  },
  {
    "objectID": "too-many-umap.html#python-examples",
    "href": "too-many-umap.html#python-examples",
    "title": "74¬† üèóÔ∏è Uniform Manifold Approximation and Projection",
    "section": "74.3 Python Examples",
    "text": "74.3 Python Examples"
  },
  {
    "objectID": "too-many-isomap.html#pros-and-cons",
    "href": "too-many-isomap.html#pros-and-cons",
    "title": "75¬† üèóÔ∏è ISOMAP",
    "section": "75.1 Pros and Cons",
    "text": "75.1 Pros and Cons\n\n75.1.1 Pros\n\n\n75.1.2 Cons"
  },
  {
    "objectID": "too-many-isomap.html#r-examples",
    "href": "too-many-isomap.html#r-examples",
    "title": "75¬† üèóÔ∏è ISOMAP",
    "section": "75.2 R Examples",
    "text": "75.2 R Examples"
  },
  {
    "objectID": "too-many-isomap.html#python-examples",
    "href": "too-many-isomap.html#python-examples",
    "title": "75¬† üèóÔ∏è ISOMAP",
    "section": "75.3 Python Examples",
    "text": "75.3 Python Examples"
  },
  {
    "objectID": "too-many-filter.html#pros-and-cons",
    "href": "too-many-filter.html#pros-and-cons",
    "title": "76¬† üèóÔ∏è Filter based feature selection",
    "section": "76.1 Pros and Cons",
    "text": "76.1 Pros and Cons\n\n76.1.1 Pros\n\n\n76.1.2 Cons"
  },
  {
    "objectID": "too-many-filter.html#r-examples",
    "href": "too-many-filter.html#r-examples",
    "title": "76¬† üèóÔ∏è Filter based feature selection",
    "section": "76.2 R Examples",
    "text": "76.2 R Examples"
  },
  {
    "objectID": "too-many-filter.html#python-examples",
    "href": "too-many-filter.html#python-examples",
    "title": "76¬† üèóÔ∏è Filter based feature selection",
    "section": "76.3 Python Examples",
    "text": "76.3 Python Examples"
  },
  {
    "objectID": "too-many-wrapper.html#pros-and-cons",
    "href": "too-many-wrapper.html#pros-and-cons",
    "title": "77¬† üèóÔ∏è Wrapper based feature selection",
    "section": "77.1 Pros and Cons",
    "text": "77.1 Pros and Cons\n\n77.1.1 Pros\n\n\n77.1.2 Cons"
  },
  {
    "objectID": "too-many-wrapper.html#r-examples",
    "href": "too-many-wrapper.html#r-examples",
    "title": "77¬† üèóÔ∏è Wrapper based feature selection",
    "section": "77.2 R Examples",
    "text": "77.2 R Examples"
  },
  {
    "objectID": "too-many-wrapper.html#python-examples",
    "href": "too-many-wrapper.html#python-examples",
    "title": "77¬† üèóÔ∏è Wrapper based feature selection",
    "section": "77.3 Python Examples",
    "text": "77.3 Python Examples"
  },
  {
    "objectID": "too-many-embedded.html#pros-and-cons",
    "href": "too-many-embedded.html#pros-and-cons",
    "title": "78¬† üèóÔ∏è embedded based feature selection",
    "section": "78.1 Pros and Cons",
    "text": "78.1 Pros and Cons\n\n78.1.1 Pros\n\n\n78.1.2 Cons"
  },
  {
    "objectID": "too-many-embedded.html#r-examples",
    "href": "too-many-embedded.html#r-examples",
    "title": "78¬† üèóÔ∏è embedded based feature selection",
    "section": "78.2 R Examples",
    "text": "78.2 R Examples"
  },
  {
    "objectID": "too-many-embedded.html#python-examples",
    "href": "too-many-embedded.html#python-examples",
    "title": "78¬† üèóÔ∏è embedded based feature selection",
    "section": "78.3 Python Examples",
    "text": "78.3 Python Examples"
  },
  {
    "objectID": "correlated.html",
    "href": "correlated.html",
    "title": "79¬† Overview",
    "section": "",
    "text": "Correlation happens when two or more variables contain similar information. We typically refer to correlation when we talk about predictors. This can be a problem for some machine learning models as they don‚Äôt perform well with correlated predictors. There are many different ways to calculate the degree of correlation. And those details aren‚Äôt going to matter much right now. The important thing is that it can happen. Below we see such examples\n\n\n\n\n\nFigure¬†79.1: uncorrelated features and correlated feature\n\n\n\n\nThe reason why correlated features are bad for our models is that two correlated features have the potential to share information that is useful. Imagine we are working with strongly correlated variables. Furthermore, we propose that predictor_1 is highly predictive in our model, since predictor_1 and predictor_2 are correlated, we can conclude that predictor_2 would also be highly predictive. The problem then arises when one of these predictors is used, the other predictor will no longer be a predictor since they share their information. Another way to think about it is that we could replace these two predictors with just one predictor with minimal loss of information. This is one of the reasons why we sometimes want to do dimension reduction, as seen in Chapter¬†65.\nWe will see how we can use the correlation structure to figure out which variables we can eliminate, this is covered in Chapter¬†80. This is a more specialized version of the methods we cover in Chapter¬†76 as we are looking at correlation to determine which variables to remove rather than their relationship to the outcome.\nAnother set of methods that works well in anything PCA-related, which are covered in Chapter¬†67 and Chapter¬†68. The resulting data coming out of PCA will be uncorrelated."
  },
  {
    "objectID": "correlated-filter.html#pros-and-cons",
    "href": "correlated-filter.html#pros-and-cons",
    "title": "80¬† High Correlation Filter",
    "section": "80.1 Pros and Cons",
    "text": "80.1 Pros and Cons\n\n80.1.1 Pros\n\nComputationally simple and fast\nEasily explainable. ‚ÄúPredictors were removed‚Äù\nWill lead to a faster and simpler model\n\n\n\n80.1.2 Cons\n\nCan be hard to justify. ‚ÄúWhy was this predictor kept instead of this one?‚Äù\nWill lead to loss of signal and performance, with the hope that this loss is kept minimal"
  },
  {
    "objectID": "correlated-filter.html#r-examples",
    "href": "correlated-filter.html#r-examples",
    "title": "80¬† High Correlation Filter",
    "section": "80.2 R Examples",
    "text": "80.2 R Examples\nWe will use the ames data set from {modeldata} in this example. The {recipes} step step_corrr() performs the simple correlation filter described at the beginning of this chapter.\n\nlibrary(recipes)\nlibrary(modeldata)\n\ncorr_rec &lt;- recipe(Sale_Price ~ ., data = ames) |&gt;\n  step_corr(all_numeric_predictors(), threshold = 0.75) |&gt;\n  prep()\n\nWe can see the variables that were removed with the tidy() method\n\ncorr_rec |&gt;\n  tidy(1)\n\n# A tibble: 3 √ó 2\n  terms        id        \n  &lt;chr&gt;        &lt;chr&gt;     \n1 First_Flr_SF corr_Bp5vK\n2 Gr_Liv_Area  corr_Bp5vK\n3 Garage_Cars  corr_Bp5vK\n\n\nWe can see that when we lower this threshold to the extreme, more predictors are removed.\n\nrecipe(Sale_Price ~ ., data = ames) |&gt;\n  step_corr(all_numeric_predictors(), threshold = 0.25) |&gt;\n  prep() |&gt;\n  tidy(1)\n\n# A tibble: 13 √ó 2\n   terms          id        \n   &lt;chr&gt;          &lt;chr&gt;     \n 1 Bsmt_Unf_SF    corr_RUieL\n 2 Total_Bsmt_SF  corr_RUieL\n 3 First_Flr_SF   corr_RUieL\n 4 Gr_Liv_Area    corr_RUieL\n 5 Bsmt_Full_Bath corr_RUieL\n 6 Full_Bath      corr_RUieL\n 7 TotRms_AbvGrd  corr_RUieL\n 8 Fireplaces     corr_RUieL\n 9 Garage_Cars    corr_RUieL\n10 Garage_Area    corr_RUieL\n11 Year_Built     corr_RUieL\n12 Second_Flr_SF  corr_RUieL\n13 Year_Remod_Add corr_RUieL"
  },
  {
    "objectID": "correlated-filter.html#python-examples",
    "href": "correlated-filter.html#python-examples",
    "title": "80¬† High Correlation Filter",
    "section": "80.3 Python Examples",
    "text": "80.3 Python Examples"
  },
  {
    "objectID": "outliers.html",
    "href": "outliers.html",
    "title": "81¬† Overview",
    "section": "",
    "text": "When we talk about outliers, we mean values that are different from the rest of the values. This is typically seen as extreme values. Let us talk about it with an example first. Below is the famous Ames housing data set. We have plotted the living area against the sale price.\n\n\n\n\n\nFigure¬†81.1: Two groups of points don‚Äôt appear close to the main group of points.\n\n\n\n\nTwo groups of observations appear to be quite far away from the rest of the points. We are in luck as these points are discussed in the data directory. The relevant quote is shown below:\n\nThere are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don‚Äôt represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately).\n\nThese types of points would typically be called outliers. They are different enough from the rest of the observations. ‚ÄúDifferent enough‚Äù is by itself hard to define. And we won‚Äôt try in this book. What we will show is some ways that people define it and let you decide what is best for your data about treatment.\nIn this care, we had enough domain knowledge to be able to determine the reason for these points to be outliers, and how to deal with them. We won‚Äôt always be this lucky. These points were outliers in 2 ways. 3 of the points didn‚Äôt include the full price of the house, so it could be classified as an error. If we think of this data set as ‚Äúhouses with know full prices‚Äù then we could exclude them as not fitting that criteria. The other two houses are outliers in a purely numerical sense. They take values that are much more different than the rest of the observations.\nWhen you systematically remove observations, regardless of whether you think they are outliers or not. Then you are limiting the domain where your model is expected to work. This may be fine or not, it will depend on the specific problem you working on. But be very careful not to remove actual observations from your data.\nWe have a handful of different ways to deal with outliers. the first choice is to not. Some model types don‚Äôt care about outliers that much. Anything that uses distances is very affected by outliers. Tree-based models don‚Äôt. Some other preprocessing method such as Chapter¬†10 also isn‚Äôt affected by outliers.\nIf you are planning on handling outliers you want to start by identifying them. In Chapter¬†82 we look at how we can identify and remove outliers. In Section¬†1.4 we cover numerical transformations that lessen the effect that outliers have on the distribution. Instead of removing the specific observation that has outliers, or transforming the whole variable, we can choose to only modify the value of the outliers. We look at methods on how to do that in Chapter¬†83. In addition to all of these methods, we can also add additional indicator variables to denote whether an observation is an outlier or not, this is covered in Chapter¬†84.\nLastly, it might be appropriate to treat the outliers as a separate data set and fit a specific model to that part of the data."
  },
  {
    "objectID": "outliers-remove.html#pros-and-cons",
    "href": "outliers-remove.html#pros-and-cons",
    "title": "82¬† üèóÔ∏è Removal",
    "section": "82.1 Pros and Cons",
    "text": "82.1 Pros and Cons\n\n82.1.1 Pros\n\n\n82.1.2 Cons"
  },
  {
    "objectID": "outliers-remove.html#r-examples",
    "href": "outliers-remove.html#r-examples",
    "title": "82¬† üèóÔ∏è Removal",
    "section": "82.2 R Examples",
    "text": "82.2 R Examples"
  },
  {
    "objectID": "outliers-remove.html#python-examples",
    "href": "outliers-remove.html#python-examples",
    "title": "82¬† üèóÔ∏è Removal",
    "section": "82.3 Python Examples",
    "text": "82.3 Python Examples"
  },
  {
    "objectID": "outliers-imputation.html#pros-and-cons",
    "href": "outliers-imputation.html#pros-and-cons",
    "title": "83¬† üèóÔ∏è Imputation",
    "section": "83.1 Pros and Cons",
    "text": "83.1 Pros and Cons\n\n83.1.1 Pros\n\n\n83.1.2 Cons"
  },
  {
    "objectID": "outliers-imputation.html#r-examples",
    "href": "outliers-imputation.html#r-examples",
    "title": "83¬† üèóÔ∏è Imputation",
    "section": "83.2 R Examples",
    "text": "83.2 R Examples"
  },
  {
    "objectID": "outliers-imputation.html#python-examples",
    "href": "outliers-imputation.html#python-examples",
    "title": "83¬† üèóÔ∏è Imputation",
    "section": "83.3 Python Examples",
    "text": "83.3 Python Examples"
  },
  {
    "objectID": "outliers-indicate.html#pros-and-cons",
    "href": "outliers-indicate.html#pros-and-cons",
    "title": "84¬† üèóÔ∏è Indicate",
    "section": "84.1 Pros and Cons",
    "text": "84.1 Pros and Cons\n\n84.1.1 Pros\n\n\n84.1.2 Cons"
  },
  {
    "objectID": "outliers-indicate.html#r-examples",
    "href": "outliers-indicate.html#r-examples",
    "title": "84¬† üèóÔ∏è Indicate",
    "section": "84.2 R Examples",
    "text": "84.2 R Examples"
  },
  {
    "objectID": "outliers-indicate.html#python-examples",
    "href": "outliers-indicate.html#python-examples",
    "title": "84¬† üèóÔ∏è Indicate",
    "section": "84.3 Python Examples",
    "text": "84.3 Python Examples"
  },
  {
    "objectID": "imbalenced.html",
    "href": "imbalenced.html",
    "title": "85¬† Overview",
    "section": "",
    "text": "Whether to keep this chapter in this book or not was considered, as the methods in this section are borderline feature engineering methods.\nThe potential problem with imbalanced data can most easily be seen in a classification setting. Propose we want to predict if an incoming email is spam or not. Furthermore, we assume that the spam rate is low and around 1% of the incoming emails. If you are not careful, you can easily end up with a model that predicts ‚Äúnot spam‚Äù all the time, since it will be correct 100% of the time. This is a common scenario, and it happens all the time. One culprit could be that there isn‚Äôt enough information in the minority class to be able to distinguish it from the majority class. And that is okay, not all modeling problems are easy. But your model will do its best anyway.\nThere are several different ways to handle imbalanced data, we will list all the ways in broad strokes, and then cover the methods that we could count as feature engineering.\n\nusing the right performance metrics\nweights\nloss functions\nCalibrating the predictions\nSampling the data, can be done naively and with more advanced methods\n\nThe example above showcases why accuracy as a metric isn‚Äôt a good choice when the classes are not uniformly represented. So you can look at other metrics, such as precision, recall, ROC-AUC, or Brier score. You will need to know what metric works best for your project.\nAnother way we can handle this is by adding weights, either to the observations directly, or in the modeling framework as class weights. Giving your minority class high enough weight forces your model to consider them.\nRelated to the last point, some methods allow you the user to pass in custom objective functions, this can also be beneficial.\nEven if your model performs badly by default, your classification model might still have good separation, just not around the 50% cut-off point. Changing the threshold is another way you can overcome an imbalanced data set.\nLastly, and the ways that will be covered in this book is a sampling of the data. There are several different methods that we will cover in this book. These methods cluster somehow, so for some groups we only explain the general idea.\nWe can split these methods into two groups, under-sampling methods and over-sampling methods. In the under-sampling method, we are removing observations and in the over-sampling we are adding observations. Adding observations is usually done by basing the new observations on the existing observations, exactly or by interpolation.\nOver-sampling methods we will cover are:\n\nUp-sampling in Chapter¬†86\nROSE in Chapter¬†87\nSMOTE in Chapter¬†88\nSMITE variants in Chapter¬†89\nBorderline Smote in Chapter¬†90\nAdaptive Synthetic Algorithm in Chapter¬†91\n\nUnder-sampling methods we will cover are:\n\nDown-sampling in Chapter¬†92\nNearMiss in Chapter¬†93\nTomek Links in Chapter¬†94\nCondensed Nearest Neighbor in Chapter¬†95\nEdited Nearest Neighbor in Chapter¬†96\nInstance Hardness Threshold in Chapter¬†97\nOne-Sided Selection Chapter¬†98\n\nSome methods do these methods together. We won‚Äôt consider those methods by themselves and instead let you know that you can do over-sampling followed by under-sampling if you choose."
  },
  {
    "objectID": "imbalenced-upsample.html#pros-and-cons",
    "href": "imbalenced-upsample.html#pros-and-cons",
    "title": "86¬† üèóÔ∏è Up-sampling",
    "section": "86.1 Pros and Cons",
    "text": "86.1 Pros and Cons\n\n86.1.1 Pros\n\n\n86.1.2 Cons"
  },
  {
    "objectID": "imbalenced-upsample.html#r-examples",
    "href": "imbalenced-upsample.html#r-examples",
    "title": "86¬† üèóÔ∏è Up-sampling",
    "section": "86.2 R Examples",
    "text": "86.2 R Examples"
  },
  {
    "objectID": "imbalenced-upsample.html#python-examples",
    "href": "imbalenced-upsample.html#python-examples",
    "title": "86¬† üèóÔ∏è Up-sampling",
    "section": "86.3 Python Examples",
    "text": "86.3 Python Examples"
  },
  {
    "objectID": "imbalenced-rose.html#pros-and-cons",
    "href": "imbalenced-rose.html#pros-and-cons",
    "title": "87¬† üèóÔ∏è ROSE",
    "section": "87.1 Pros and Cons",
    "text": "87.1 Pros and Cons\n\n87.1.1 Pros\n\n\n87.1.2 Cons"
  },
  {
    "objectID": "imbalenced-rose.html#r-examples",
    "href": "imbalenced-rose.html#r-examples",
    "title": "87¬† üèóÔ∏è ROSE",
    "section": "87.2 R Examples",
    "text": "87.2 R Examples"
  },
  {
    "objectID": "imbalenced-rose.html#python-examples",
    "href": "imbalenced-rose.html#python-examples",
    "title": "87¬† üèóÔ∏è ROSE",
    "section": "87.3 Python Examples",
    "text": "87.3 Python Examples"
  },
  {
    "objectID": "imbalenced-smote.html#pros-and-cons",
    "href": "imbalenced-smote.html#pros-and-cons",
    "title": "88¬† üèóÔ∏è SMOTE",
    "section": "88.1 Pros and Cons",
    "text": "88.1 Pros and Cons\n\n88.1.1 Pros\n\n\n88.1.2 Cons"
  },
  {
    "objectID": "imbalenced-smote.html#r-examples",
    "href": "imbalenced-smote.html#r-examples",
    "title": "88¬† üèóÔ∏è SMOTE",
    "section": "88.2 R Examples",
    "text": "88.2 R Examples"
  },
  {
    "objectID": "imbalenced-smote.html#python-examples",
    "href": "imbalenced-smote.html#python-examples",
    "title": "88¬† üèóÔ∏è SMOTE",
    "section": "88.3 Python Examples",
    "text": "88.3 Python Examples"
  },
  {
    "objectID": "imbalenced-smote-variants.html#pros-and-cons",
    "href": "imbalenced-smote-variants.html#pros-and-cons",
    "title": "89¬† üèóÔ∏è SMOTE Variants",
    "section": "89.1 Pros and Cons",
    "text": "89.1 Pros and Cons\n\n89.1.1 Pros\n\n\n89.1.2 Cons"
  },
  {
    "objectID": "imbalenced-smote-variants.html#r-examples",
    "href": "imbalenced-smote-variants.html#r-examples",
    "title": "89¬† üèóÔ∏è SMOTE Variants",
    "section": "89.2 R Examples",
    "text": "89.2 R Examples"
  },
  {
    "objectID": "imbalenced-smote-variants.html#python-examples",
    "href": "imbalenced-smote-variants.html#python-examples",
    "title": "89¬† üèóÔ∏è SMOTE Variants",
    "section": "89.3 Python Examples",
    "text": "89.3 Python Examples"
  },
  {
    "objectID": "imbalenced-borderline-smote.html#pros-and-cons",
    "href": "imbalenced-borderline-smote.html#pros-and-cons",
    "title": "90¬† üèóÔ∏è Borderline SMOTE",
    "section": "90.1 Pros and Cons",
    "text": "90.1 Pros and Cons\n\n90.1.1 Pros\n\n\n90.1.2 Cons"
  },
  {
    "objectID": "imbalenced-borderline-smote.html#r-examples",
    "href": "imbalenced-borderline-smote.html#r-examples",
    "title": "90¬† üèóÔ∏è Borderline SMOTE",
    "section": "90.2 R Examples",
    "text": "90.2 R Examples"
  },
  {
    "objectID": "imbalenced-borderline-smote.html#python-examples",
    "href": "imbalenced-borderline-smote.html#python-examples",
    "title": "90¬† üèóÔ∏è Borderline SMOTE",
    "section": "90.3 Python Examples",
    "text": "90.3 Python Examples"
  },
  {
    "objectID": "imbalenced-adasyn.html#pros-and-cons",
    "href": "imbalenced-adasyn.html#pros-and-cons",
    "title": "91¬† üèóÔ∏è Adaptive Synthetic Algorithm",
    "section": "91.1 Pros and Cons",
    "text": "91.1 Pros and Cons\n\n91.1.1 Pros\n\n\n91.1.2 Cons"
  },
  {
    "objectID": "imbalenced-adasyn.html#r-examples",
    "href": "imbalenced-adasyn.html#r-examples",
    "title": "91¬† üèóÔ∏è Adaptive Synthetic Algorithm",
    "section": "91.2 R Examples",
    "text": "91.2 R Examples"
  },
  {
    "objectID": "imbalenced-adasyn.html#python-examples",
    "href": "imbalenced-adasyn.html#python-examples",
    "title": "91¬† üèóÔ∏è Adaptive Synthetic Algorithm",
    "section": "91.3 Python Examples",
    "text": "91.3 Python Examples"
  },
  {
    "objectID": "imbalenced-downsample.html#pros-and-cons",
    "href": "imbalenced-downsample.html#pros-and-cons",
    "title": "92¬† üèóÔ∏è Down-Sampling",
    "section": "92.1 Pros and Cons",
    "text": "92.1 Pros and Cons\n\n92.1.1 Pros\n\n\n92.1.2 Cons"
  },
  {
    "objectID": "imbalenced-downsample.html#r-examples",
    "href": "imbalenced-downsample.html#r-examples",
    "title": "92¬† üèóÔ∏è Down-Sampling",
    "section": "92.2 R Examples",
    "text": "92.2 R Examples"
  },
  {
    "objectID": "imbalenced-downsample.html#python-examples",
    "href": "imbalenced-downsample.html#python-examples",
    "title": "92¬† üèóÔ∏è Down-Sampling",
    "section": "92.3 Python Examples",
    "text": "92.3 Python Examples"
  },
  {
    "objectID": "imbalenced-nearmiss.html#pros-and-cons",
    "href": "imbalenced-nearmiss.html#pros-and-cons",
    "title": "93¬† üèóÔ∏è Near-Miss",
    "section": "93.1 Pros and Cons",
    "text": "93.1 Pros and Cons\n\n93.1.1 Pros\n\n\n93.1.2 Cons"
  },
  {
    "objectID": "imbalenced-nearmiss.html#r-examples",
    "href": "imbalenced-nearmiss.html#r-examples",
    "title": "93¬† üèóÔ∏è Near-Miss",
    "section": "93.2 R Examples",
    "text": "93.2 R Examples"
  },
  {
    "objectID": "imbalenced-nearmiss.html#python-examples",
    "href": "imbalenced-nearmiss.html#python-examples",
    "title": "93¬† üèóÔ∏è Near-Miss",
    "section": "93.3 Python Examples",
    "text": "93.3 Python Examples"
  },
  {
    "objectID": "imbalenced-tomek.html#pros-and-cons",
    "href": "imbalenced-tomek.html#pros-and-cons",
    "title": "94¬† üèóÔ∏è Tomek Links",
    "section": "94.1 Pros and Cons",
    "text": "94.1 Pros and Cons\n\n94.1.1 Pros\n\n\n94.1.2 Cons"
  },
  {
    "objectID": "imbalenced-tomek.html#r-examples",
    "href": "imbalenced-tomek.html#r-examples",
    "title": "94¬† üèóÔ∏è Tomek Links",
    "section": "94.2 R Examples",
    "text": "94.2 R Examples"
  },
  {
    "objectID": "imbalenced-tomek.html#python-examples",
    "href": "imbalenced-tomek.html#python-examples",
    "title": "94¬† üèóÔ∏è Tomek Links",
    "section": "94.3 Python Examples",
    "text": "94.3 Python Examples"
  },
  {
    "objectID": "imbalenced-cnn.html#pros-and-cons",
    "href": "imbalenced-cnn.html#pros-and-cons",
    "title": "95¬† üèóÔ∏è Condensed Nearest Neighbor",
    "section": "95.1 Pros and Cons",
    "text": "95.1 Pros and Cons\n\n95.1.1 Pros\n\n\n95.1.2 Cons"
  },
  {
    "objectID": "imbalenced-cnn.html#r-examples",
    "href": "imbalenced-cnn.html#r-examples",
    "title": "95¬† üèóÔ∏è Condensed Nearest Neighbor",
    "section": "95.2 R Examples",
    "text": "95.2 R Examples"
  },
  {
    "objectID": "imbalenced-cnn.html#python-examples",
    "href": "imbalenced-cnn.html#python-examples",
    "title": "95¬† üèóÔ∏è Condensed Nearest Neighbor",
    "section": "95.3 Python Examples",
    "text": "95.3 Python Examples"
  },
  {
    "objectID": "imbalenced-enn.html#pros-and-cons",
    "href": "imbalenced-enn.html#pros-and-cons",
    "title": "96¬† üèóÔ∏è Edited Nearest Neighbor",
    "section": "96.1 Pros and Cons",
    "text": "96.1 Pros and Cons\n\n96.1.1 Pros\n\n\n96.1.2 Cons"
  },
  {
    "objectID": "imbalenced-enn.html#r-examples",
    "href": "imbalenced-enn.html#r-examples",
    "title": "96¬† üèóÔ∏è Edited Nearest Neighbor",
    "section": "96.2 R Examples",
    "text": "96.2 R Examples"
  },
  {
    "objectID": "imbalenced-enn.html#python-examples",
    "href": "imbalenced-enn.html#python-examples",
    "title": "96¬† üèóÔ∏è Edited Nearest Neighbor",
    "section": "96.3 Python Examples",
    "text": "96.3 Python Examples"
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#pros-and-cons",
    "href": "imbalenced-hardness-threshold.html#pros-and-cons",
    "title": "97¬† üèóÔ∏è Instance Hardness Threshold",
    "section": "97.1 Pros and Cons",
    "text": "97.1 Pros and Cons\n\n97.1.1 Pros\n\n\n97.1.2 Cons"
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#r-examples",
    "href": "imbalenced-hardness-threshold.html#r-examples",
    "title": "97¬† üèóÔ∏è Instance Hardness Threshold",
    "section": "97.2 R Examples",
    "text": "97.2 R Examples"
  },
  {
    "objectID": "imbalenced-hardness-threshold.html#python-examples",
    "href": "imbalenced-hardness-threshold.html#python-examples",
    "title": "97¬† üèóÔ∏è Instance Hardness Threshold",
    "section": "97.3 Python Examples",
    "text": "97.3 Python Examples"
  },
  {
    "objectID": "imbalenced-one-sided.html#pros-and-cons",
    "href": "imbalenced-one-sided.html#pros-and-cons",
    "title": "98¬† üèóÔ∏è One Sided Selection",
    "section": "98.1 Pros and Cons",
    "text": "98.1 Pros and Cons\n\n98.1.1 Pros\n\n\n98.1.2 Cons"
  },
  {
    "objectID": "imbalenced-one-sided.html#r-examples",
    "href": "imbalenced-one-sided.html#r-examples",
    "title": "98¬† üèóÔ∏è One Sided Selection",
    "section": "98.2 R Examples",
    "text": "98.2 R Examples"
  },
  {
    "objectID": "imbalenced-one-sided.html#python-examples",
    "href": "imbalenced-one-sided.html#python-examples",
    "title": "98¬† üèóÔ∏è One Sided Selection",
    "section": "98.3 Python Examples",
    "text": "98.3 Python Examples"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "99¬† Overview",
    "section": "",
    "text": "Even though we try to cover as many methods as we can in this book. There will always be more methods. This can be types of data that don‚Äôt fit in any of the other sections in this book, or because they are too domain-specific. Nevertheless, this chapter will cover a few methods and techniques we want to show, that don‚Äôt fit anywhere else. The purpose of these chapters is not solely to teach you about these methods directly, but to try to broaden where you try to find information\n\nWorking with ID variables in Chapter¬†100\nWorking with colors in Chapter¬†101\nWorking with zip codes in Chapter¬†102\nWorking with emails in Chapter¬†103"
  },
  {
    "objectID": "miscellaneous-id.html#pros-and-cons",
    "href": "miscellaneous-id.html#pros-and-cons",
    "title": "100¬† üèóÔ∏è IDs",
    "section": "100.1 Pros and Cons",
    "text": "100.1 Pros and Cons\n\n100.1.1 Pros\n\n\n100.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-id.html#r-examples",
    "href": "miscellaneous-id.html#r-examples",
    "title": "100¬† üèóÔ∏è IDs",
    "section": "100.2 R Examples",
    "text": "100.2 R Examples"
  },
  {
    "objectID": "miscellaneous-id.html#python-examples",
    "href": "miscellaneous-id.html#python-examples",
    "title": "100¬† üèóÔ∏è IDs",
    "section": "100.3 Python Examples",
    "text": "100.3 Python Examples"
  },
  {
    "objectID": "miscellaneous-color.html#pros-and-cons",
    "href": "miscellaneous-color.html#pros-and-cons",
    "title": "101¬† üèóÔ∏è Colors",
    "section": "101.1 Pros and Cons",
    "text": "101.1 Pros and Cons\n\n101.1.1 Pros\n\n\n101.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-color.html#r-examples",
    "href": "miscellaneous-color.html#r-examples",
    "title": "101¬† üèóÔ∏è Colors",
    "section": "101.2 R Examples",
    "text": "101.2 R Examples"
  },
  {
    "objectID": "miscellaneous-color.html#python-examples",
    "href": "miscellaneous-color.html#python-examples",
    "title": "101¬† üèóÔ∏è Colors",
    "section": "101.3 Python Examples",
    "text": "101.3 Python Examples"
  },
  {
    "objectID": "miscellaneous-zipcodes.html#pros-and-cons",
    "href": "miscellaneous-zipcodes.html#pros-and-cons",
    "title": "102¬† üèóÔ∏è Zip Codes",
    "section": "102.1 Pros and Cons",
    "text": "102.1 Pros and Cons\n\n102.1.1 Pros\n\n\n102.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-zipcodes.html#r-examples",
    "href": "miscellaneous-zipcodes.html#r-examples",
    "title": "102¬† üèóÔ∏è Zip Codes",
    "section": "102.2 R Examples",
    "text": "102.2 R Examples"
  },
  {
    "objectID": "miscellaneous-zipcodes.html#python-examples",
    "href": "miscellaneous-zipcodes.html#python-examples",
    "title": "102¬† üèóÔ∏è Zip Codes",
    "section": "102.3 Python Examples",
    "text": "102.3 Python Examples"
  },
  {
    "objectID": "miscellaneous-email.html#pros-and-cons",
    "href": "miscellaneous-email.html#pros-and-cons",
    "title": "103¬† üèóÔ∏è Emails",
    "section": "103.1 Pros and Cons",
    "text": "103.1 Pros and Cons\n\n103.1.1 Pros\n\n\n103.1.2 Cons"
  },
  {
    "objectID": "miscellaneous-email.html#r-examples",
    "href": "miscellaneous-email.html#r-examples",
    "title": "103¬† üèóÔ∏è Emails",
    "section": "103.2 R Examples",
    "text": "103.2 R Examples"
  },
  {
    "objectID": "miscellaneous-email.html#python-examples",
    "href": "miscellaneous-email.html#python-examples",
    "title": "103¬† üèóÔ∏è Emails",
    "section": "103.3 Python Examples",
    "text": "103.3 Python Examples"
  },
  {
    "objectID": "spatial.html",
    "href": "spatial.html",
    "title": "104¬† Overview",
    "section": "",
    "text": "When we talk about spatial and geospatial feature engineering, we want to focus on the transformation and enrichment of the data set, based on spatial information. Typically this will be longitude and latitude-based, with the additional information being based on areas and regions such as cities or countries.\nWhat sets these methods apart from some of the other methods we see in this book, is that they almost always require a reference data set to be able to perform the calculations. If you want to find the closest city to an observation, you need a data set of all the cities and their location. For all the methods in this section, the reader is expected to know how to gather this reference material for their problem.\nWe will split this data up into two types of methods, depending on your spatial information. point based methods and shape based methods.\nIn point-based methods, you know the location of your observation, and you calculate where it is in relationship to something else. You could look at distances by finding the distance to a fixed point, or multiple points as covered in Chapter¬†105. You could find the nearest of something as covered in Chapter¬†106. These two methods are different sides of the same coin. Another thing we could do is count the number of occurrences within a given distance or region. This is covered in Chapter¬†107. By knowing the location of something we are also able to query certain types of information such as ‚Äúheight from sea‚Äù, ‚Äúrainfall in inches in 2000‚Äù and so on. We cover these types of methods in Chapter¬†108. We can also expand some of these concepts and look at spatial embeddings, we will be covered in Chapter¬†109.\nIn shape-based methods, you don‚Äôt just have the positioning of your observation, but also its shape. This can be any shape; line, polygon, or circle. The methods seen in point-based methods can be applied to shape-based methods, we just need to be a little more careful when performing the calculations. Since we are given the shape of our observation, there are characteristics we can extract from those that might be useful. We look at how we can incorporate that information in Chapter¬†110."
  },
  {
    "objectID": "spatial-distance.html#pros-and-cons",
    "href": "spatial-distance.html#pros-and-cons",
    "title": "105¬† üèóÔ∏è Spatial Distance",
    "section": "105.1 Pros and Cons",
    "text": "105.1 Pros and Cons\n\n105.1.1 Pros\n\n\n105.1.2 Cons"
  },
  {
    "objectID": "spatial-distance.html#r-examples",
    "href": "spatial-distance.html#r-examples",
    "title": "105¬† üèóÔ∏è Spatial Distance",
    "section": "105.2 R Examples",
    "text": "105.2 R Examples"
  },
  {
    "objectID": "spatial-distance.html#python-examples",
    "href": "spatial-distance.html#python-examples",
    "title": "105¬† üèóÔ∏è Spatial Distance",
    "section": "105.3 Python Examples",
    "text": "105.3 Python Examples"
  },
  {
    "objectID": "spatial-nearest.html#pros-and-cons",
    "href": "spatial-nearest.html#pros-and-cons",
    "title": "106¬† üèóÔ∏è Spatial Nearest",
    "section": "106.1 Pros and Cons",
    "text": "106.1 Pros and Cons\n\n106.1.1 Pros\n\n\n106.1.2 Cons"
  },
  {
    "objectID": "spatial-nearest.html#r-examples",
    "href": "spatial-nearest.html#r-examples",
    "title": "106¬† üèóÔ∏è Spatial Nearest",
    "section": "106.2 R Examples",
    "text": "106.2 R Examples"
  },
  {
    "objectID": "spatial-nearest.html#python-examples",
    "href": "spatial-nearest.html#python-examples",
    "title": "106¬† üèóÔ∏è Spatial Nearest",
    "section": "106.3 Python Examples",
    "text": "106.3 Python Examples"
  },
  {
    "objectID": "spatial-count.html#pros-and-cons",
    "href": "spatial-count.html#pros-and-cons",
    "title": "107¬† üèóÔ∏è Spatial Count",
    "section": "107.1 Pros and Cons",
    "text": "107.1 Pros and Cons\n\n107.1.1 Pros\n\n\n107.1.2 Cons"
  },
  {
    "objectID": "spatial-count.html#r-examples",
    "href": "spatial-count.html#r-examples",
    "title": "107¬† üèóÔ∏è Spatial Count",
    "section": "107.2 R Examples",
    "text": "107.2 R Examples"
  },
  {
    "objectID": "spatial-count.html#python-examples",
    "href": "spatial-count.html#python-examples",
    "title": "107¬† üèóÔ∏è Spatial Count",
    "section": "107.3 Python Examples",
    "text": "107.3 Python Examples"
  },
  {
    "objectID": "spatial-query.html#pros-and-cons",
    "href": "spatial-query.html#pros-and-cons",
    "title": "108¬† üèóÔ∏è Spatial Query",
    "section": "108.1 Pros and Cons",
    "text": "108.1 Pros and Cons\n\n108.1.1 Pros\n\n\n108.1.2 Cons"
  },
  {
    "objectID": "spatial-query.html#r-examples",
    "href": "spatial-query.html#r-examples",
    "title": "108¬† üèóÔ∏è Spatial Query",
    "section": "108.2 R Examples",
    "text": "108.2 R Examples"
  },
  {
    "objectID": "spatial-query.html#python-examples",
    "href": "spatial-query.html#python-examples",
    "title": "108¬† üèóÔ∏è Spatial Query",
    "section": "108.3 Python Examples",
    "text": "108.3 Python Examples"
  },
  {
    "objectID": "spatial-embedding.html#pros-and-cons",
    "href": "spatial-embedding.html#pros-and-cons",
    "title": "109¬† üèóÔ∏è Spatial Embedding",
    "section": "109.1 Pros and Cons",
    "text": "109.1 Pros and Cons\n\n109.1.1 Pros\n\n\n109.1.2 Cons"
  },
  {
    "objectID": "spatial-embedding.html#r-examples",
    "href": "spatial-embedding.html#r-examples",
    "title": "109¬† üèóÔ∏è Spatial Embedding",
    "section": "109.2 R Examples",
    "text": "109.2 R Examples"
  },
  {
    "objectID": "spatial-embedding.html#python-examples",
    "href": "spatial-embedding.html#python-examples",
    "title": "109¬† üèóÔ∏è Spatial Embedding",
    "section": "109.3 Python Examples",
    "text": "109.3 Python Examples"
  },
  {
    "objectID": "spatial-characteristics.html#pros-and-cons",
    "href": "spatial-characteristics.html#pros-and-cons",
    "title": "110¬† üèóÔ∏è Spatial Characteristics",
    "section": "110.1 Pros and Cons",
    "text": "110.1 Pros and Cons\n\n110.1.1 Pros\n\n\n110.1.2 Cons"
  },
  {
    "objectID": "spatial-characteristics.html#r-examples",
    "href": "spatial-characteristics.html#r-examples",
    "title": "110¬† üèóÔ∏è Spatial Characteristics",
    "section": "110.2 R Examples",
    "text": "110.2 R Examples"
  },
  {
    "objectID": "spatial-characteristics.html#python-examples",
    "href": "spatial-characteristics.html#python-examples",
    "title": "110¬† üèóÔ∏è Spatial Characteristics",
    "section": "110.3 Python Examples",
    "text": "110.3 Python Examples"
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "111¬† Overview",
    "section": "",
    "text": "A very common type of data is time-series data. This is data where the observations are taken across time with time stamps. This data will inherently be correlated with itself as the same measurement for the same unit likely isn‚Äôt going to change too much. Time series data is typically modeled using different methods than other predictive modeling for this reason.\nNevertheless, there are still some methods that will be useful for us. It is important to note that there is a time component to this data, which is what we are trying to predict. The observations can happen at regular intervals, once a day, or irregular intervals, each time the engine errors. Different types of data use different types of models and make the feature engineering a little different depending on what you are trying to do.\nThis chapter assumes the user knows how to use time series data and the precautions that are needed when working with data of this type.\n\n\n\n\n\n\nTODO\n\n\n\nadd a link to appropriate resources\n\n\nOne area of work is modifying the sequence itself. Some of these transformations can be taken from Chapter¬†1. However, some metrics are specific to time series data. Likewise, some metrics are similar to what we do in Chapter¬†41 and Chapter¬†81, but special care needs to be taken with time series data.\n\nSmoothing in Chapter¬†112\nSliding window transformations in Chapter¬†113\nLog Interval Transformation in Chapter¬†114\nTime series-specific handling of missing values in Chapter¬†115\nTime series-specific handling of outliers in Chapter¬†116\n\nAnother area of methods is where we extract or modify the sequence or its data to create new variables, this is done by considering the time component. This will result in breaking about the time component in a decomposition kind of way, or by getting information out of the values themselves. One such example of the latter is found in the datetime section Chapter¬†37. Particularly holiday extraction as seen in Chapter¬†38.\n\nDifferences in Chapter¬†117\nLagging features in Chapter¬†118\nrolling window features in Chapter¬†119\nexpanding window features in Chapter¬†120\nFourier decomposition in Chapter¬†121\nWavelet decomposition in Chapter¬†122"
  },
  {
    "objectID": "time-series-smooth.html#pros-and-cons",
    "href": "time-series-smooth.html#pros-and-cons",
    "title": "112¬† üèóÔ∏è Smoothing",
    "section": "112.1 Pros and Cons",
    "text": "112.1 Pros and Cons\n\n112.1.1 Pros\n\n\n112.1.2 Cons"
  },
  {
    "objectID": "time-series-smooth.html#r-examples",
    "href": "time-series-smooth.html#r-examples",
    "title": "112¬† üèóÔ∏è Smoothing",
    "section": "112.2 R Examples",
    "text": "112.2 R Examples"
  },
  {
    "objectID": "time-series-smooth.html#python-examples",
    "href": "time-series-smooth.html#python-examples",
    "title": "112¬† üèóÔ∏è Smoothing",
    "section": "112.3 Python Examples",
    "text": "112.3 Python Examples"
  },
  {
    "objectID": "time-series-sliding.html#pros-and-cons",
    "href": "time-series-sliding.html#pros-and-cons",
    "title": "113¬† üèóÔ∏è Sliding",
    "section": "113.1 Pros and Cons",
    "text": "113.1 Pros and Cons\n\n113.1.1 Pros\n\n\n113.1.2 Cons"
  },
  {
    "objectID": "time-series-sliding.html#r-examples",
    "href": "time-series-sliding.html#r-examples",
    "title": "113¬† üèóÔ∏è Sliding",
    "section": "113.2 R Examples",
    "text": "113.2 R Examples"
  },
  {
    "objectID": "time-series-sliding.html#python-examples",
    "href": "time-series-sliding.html#python-examples",
    "title": "113¬† üèóÔ∏è Sliding",
    "section": "113.3 Python Examples",
    "text": "113.3 Python Examples"
  },
  {
    "objectID": "time-series-log-interval.html#pros-and-cons",
    "href": "time-series-log-interval.html#pros-and-cons",
    "title": "114¬† üèóÔ∏è Log Interval",
    "section": "114.1 Pros and Cons",
    "text": "114.1 Pros and Cons\n\n114.1.1 Pros\n\n\n114.1.2 Cons"
  },
  {
    "objectID": "time-series-log-interval.html#r-examples",
    "href": "time-series-log-interval.html#r-examples",
    "title": "114¬† üèóÔ∏è Log Interval",
    "section": "114.2 R Examples",
    "text": "114.2 R Examples"
  },
  {
    "objectID": "time-series-log-interval.html#python-examples",
    "href": "time-series-log-interval.html#python-examples",
    "title": "114¬† üèóÔ∏è Log Interval",
    "section": "114.3 Python Examples",
    "text": "114.3 Python Examples"
  },
  {
    "objectID": "time-series-missing.html#pros-and-cons",
    "href": "time-series-missing.html#pros-and-cons",
    "title": "115¬† üèóÔ∏è Time series Missing values",
    "section": "115.1 Pros and Cons",
    "text": "115.1 Pros and Cons\n\n115.1.1 Pros\n\n\n115.1.2 Cons"
  },
  {
    "objectID": "time-series-missing.html#r-examples",
    "href": "time-series-missing.html#r-examples",
    "title": "115¬† üèóÔ∏è Time series Missing values",
    "section": "115.2 R Examples",
    "text": "115.2 R Examples"
  },
  {
    "objectID": "time-series-missing.html#python-examples",
    "href": "time-series-missing.html#python-examples",
    "title": "115¬† üèóÔ∏è Time series Missing values",
    "section": "115.3 Python Examples",
    "text": "115.3 Python Examples"
  },
  {
    "objectID": "time-series-outliers.html#pros-and-cons",
    "href": "time-series-outliers.html#pros-and-cons",
    "title": "116¬† üèóÔ∏è Time Series outliers",
    "section": "116.1 Pros and Cons",
    "text": "116.1 Pros and Cons\n\n116.1.1 Pros\n\n\n116.1.2 Cons"
  },
  {
    "objectID": "time-series-outliers.html#r-examples",
    "href": "time-series-outliers.html#r-examples",
    "title": "116¬† üèóÔ∏è Time Series outliers",
    "section": "116.2 R Examples",
    "text": "116.2 R Examples"
  },
  {
    "objectID": "time-series-outliers.html#python-examples",
    "href": "time-series-outliers.html#python-examples",
    "title": "116¬† üèóÔ∏è Time Series outliers",
    "section": "116.3 Python Examples",
    "text": "116.3 Python Examples"
  },
  {
    "objectID": "time-series-diff.html#pros-and-cons",
    "href": "time-series-diff.html#pros-and-cons",
    "title": "117¬† üèóÔ∏è Differences",
    "section": "117.1 Pros and Cons",
    "text": "117.1 Pros and Cons\n\n117.1.1 Pros\n\n\n117.1.2 Cons"
  },
  {
    "objectID": "time-series-diff.html#r-examples",
    "href": "time-series-diff.html#r-examples",
    "title": "117¬† üèóÔ∏è Differences",
    "section": "117.2 R Examples",
    "text": "117.2 R Examples"
  },
  {
    "objectID": "time-series-diff.html#python-examples",
    "href": "time-series-diff.html#python-examples",
    "title": "117¬† üèóÔ∏è Differences",
    "section": "117.3 Python Examples",
    "text": "117.3 Python Examples"
  },
  {
    "objectID": "time-series-lag.html#pros-and-cons",
    "href": "time-series-lag.html#pros-and-cons",
    "title": "118¬† üèóÔ∏è Lagging features",
    "section": "118.1 Pros and Cons",
    "text": "118.1 Pros and Cons\n\n118.1.1 Pros\n\n\n118.1.2 Cons"
  },
  {
    "objectID": "time-series-lag.html#r-examples",
    "href": "time-series-lag.html#r-examples",
    "title": "118¬† üèóÔ∏è Lagging features",
    "section": "118.2 R Examples",
    "text": "118.2 R Examples"
  },
  {
    "objectID": "time-series-lag.html#python-examples",
    "href": "time-series-lag.html#python-examples",
    "title": "118¬† üèóÔ∏è Lagging features",
    "section": "118.3 Python Examples",
    "text": "118.3 Python Examples"
  },
  {
    "objectID": "time-series-rolling-window.html#pros-and-cons",
    "href": "time-series-rolling-window.html#pros-and-cons",
    "title": "119¬† üèóÔ∏è Rolling Window",
    "section": "119.1 Pros and Cons",
    "text": "119.1 Pros and Cons\n\n119.1.1 Pros\n\n\n119.1.2 Cons"
  },
  {
    "objectID": "time-series-rolling-window.html#r-examples",
    "href": "time-series-rolling-window.html#r-examples",
    "title": "119¬† üèóÔ∏è Rolling Window",
    "section": "119.2 R Examples",
    "text": "119.2 R Examples"
  },
  {
    "objectID": "time-series-rolling-window.html#python-examples",
    "href": "time-series-rolling-window.html#python-examples",
    "title": "119¬† üèóÔ∏è Rolling Window",
    "section": "119.3 Python Examples",
    "text": "119.3 Python Examples"
  },
  {
    "objectID": "time-series-expanding-window.html#pros-and-cons",
    "href": "time-series-expanding-window.html#pros-and-cons",
    "title": "120¬† üèóÔ∏è Expanding Window",
    "section": "120.1 Pros and Cons",
    "text": "120.1 Pros and Cons\n\n120.1.1 Pros\n\n\n120.1.2 Cons"
  },
  {
    "objectID": "time-series-expanding-window.html#r-examples",
    "href": "time-series-expanding-window.html#r-examples",
    "title": "120¬† üèóÔ∏è Expanding Window",
    "section": "120.2 R Examples",
    "text": "120.2 R Examples"
  },
  {
    "objectID": "time-series-expanding-window.html#python-examples",
    "href": "time-series-expanding-window.html#python-examples",
    "title": "120¬† üèóÔ∏è Expanding Window",
    "section": "120.3 Python Examples",
    "text": "120.3 Python Examples"
  },
  {
    "objectID": "time-series-fourier.html#pros-and-cons",
    "href": "time-series-fourier.html#pros-and-cons",
    "title": "121¬† üèóÔ∏è Fourier Features",
    "section": "121.1 Pros and Cons",
    "text": "121.1 Pros and Cons\n\n121.1.1 Pros\n\n\n121.1.2 Cons"
  },
  {
    "objectID": "time-series-fourier.html#r-examples",
    "href": "time-series-fourier.html#r-examples",
    "title": "121¬† üèóÔ∏è Fourier Features",
    "section": "121.2 R Examples",
    "text": "121.2 R Examples"
  },
  {
    "objectID": "time-series-fourier.html#python-examples",
    "href": "time-series-fourier.html#python-examples",
    "title": "121¬† üèóÔ∏è Fourier Features",
    "section": "121.3 Python Examples",
    "text": "121.3 Python Examples"
  },
  {
    "objectID": "time-series-wavelet.html#pros-and-cons",
    "href": "time-series-wavelet.html#pros-and-cons",
    "title": "122¬† üèóÔ∏è Wavelet",
    "section": "122.1 Pros and Cons",
    "text": "122.1 Pros and Cons\n\n122.1.1 Pros\n\n\n122.1.2 Cons"
  },
  {
    "objectID": "time-series-wavelet.html#r-examples",
    "href": "time-series-wavelet.html#r-examples",
    "title": "122¬† üèóÔ∏è Wavelet",
    "section": "122.2 R Examples",
    "text": "122.2 R Examples"
  },
  {
    "objectID": "time-series-wavelet.html#python-examples",
    "href": "time-series-wavelet.html#python-examples",
    "title": "122¬† üèóÔ∏è Wavelet",
    "section": "122.3 Python Examples",
    "text": "122.3 Python Examples"
  },
  {
    "objectID": "image.html#feature-extraction",
    "href": "image.html#feature-extraction",
    "title": "123¬† Overview",
    "section": "123.1 Feature Extraction",
    "text": "123.1 Feature Extraction\nIn the extraction setting, we take the images and try to extract smaller, hopefully smaller vectors of information. These could be simple statistics or larger and more complicated methods. One does not need to do this right away, and sometimes it is beneficial to apply some of the image modification methods below before doing the extraction.\n\nEdge detection and corner detection in Chapter¬†124\ntexture analysis in Chapter¬†125"
  },
  {
    "objectID": "image.html#image-modification",
    "href": "image.html#image-modification",
    "title": "123¬† Overview",
    "section": "123.2 Image Modification",
    "text": "123.2 Image Modification\nSometimes the images you get will not be in the best shape for your task at hand. This could be for various reasons. Applying color changes of different kinds can help highlight the important parts of the image, such that later preprocessing steps or models have an easier time picking up on it. Likewise, you might need to scale the data to help the model and well as reduce noise. Lastly, you will most likely need to resize your images as many deep-learning image modes work on fixed input sizes.\n\nGrayscale conversion in Chapter¬†126\ncolor modifications in Chapter¬†127\nnoise reduction in Chapter¬†128\nValue normalization in Chapter¬†129\nresizing in Chapter¬†130"
  },
  {
    "objectID": "image.html#augmentation",
    "href": "image.html#augmentation",
    "title": "123¬† Overview",
    "section": "123.3 Augmentation",
    "text": "123.3 Augmentation\nA common trick when working with image data is to do augmentation. What we mean by that, is that we do different kinds of transformations to generate new images that contain the same information but in different ways. It creates a larger data set. With the hopes of increasing the performance and generalization. Being able to detect cat pictures regardless if they are centered in the image or not.\n\nChanging brightness in Chapter¬†131\nShifting, Flipping, Rotation in Chapter¬†132\nCropping and scaling in Chapter¬†133"
  },
  {
    "objectID": "image.html#embeddings",
    "href": "image.html#embeddings",
    "title": "123¬† Overview",
    "section": "123.4 Embeddings",
    "text": "123.4 Embeddings\nWe can also take advantage of transfer learning. People have fit image deep learning models on many images before us. And some of these trained models can be reused for us. We will look at that in Chapter¬†134."
  },
  {
    "objectID": "image-edge-corner.html#pros-and-cons",
    "href": "image-edge-corner.html#pros-and-cons",
    "title": "124¬† üèóÔ∏è Edge and corner detection",
    "section": "124.1 Pros and Cons",
    "text": "124.1 Pros and Cons\n\n124.1.1 Pros\n\n\n124.1.2 Cons"
  },
  {
    "objectID": "image-edge-corner.html#r-examples",
    "href": "image-edge-corner.html#r-examples",
    "title": "124¬† üèóÔ∏è Edge and corner detection",
    "section": "124.2 R Examples",
    "text": "124.2 R Examples"
  },
  {
    "objectID": "image-edge-corner.html#python-examples",
    "href": "image-edge-corner.html#python-examples",
    "title": "124¬† üèóÔ∏è Edge and corner detection",
    "section": "124.3 Python Examples",
    "text": "124.3 Python Examples"
  },
  {
    "objectID": "image-texture.html#pros-and-cons",
    "href": "image-texture.html#pros-and-cons",
    "title": "125¬† üèóÔ∏è Texture Analysis",
    "section": "125.1 Pros and Cons",
    "text": "125.1 Pros and Cons\n\n125.1.1 Pros\n\n\n125.1.2 Cons"
  },
  {
    "objectID": "image-texture.html#r-examples",
    "href": "image-texture.html#r-examples",
    "title": "125¬† üèóÔ∏è Texture Analysis",
    "section": "125.2 R Examples",
    "text": "125.2 R Examples"
  },
  {
    "objectID": "image-texture.html#python-examples",
    "href": "image-texture.html#python-examples",
    "title": "125¬† üèóÔ∏è Texture Analysis",
    "section": "125.3 Python Examples",
    "text": "125.3 Python Examples"
  },
  {
    "objectID": "image-grayscale.html#pros-and-cons",
    "href": "image-grayscale.html#pros-and-cons",
    "title": "126¬† üèóÔ∏è Greyscale conversion",
    "section": "126.1 Pros and Cons",
    "text": "126.1 Pros and Cons\n\n126.1.1 Pros\n\n\n126.1.2 Cons"
  },
  {
    "objectID": "image-grayscale.html#r-examples",
    "href": "image-grayscale.html#r-examples",
    "title": "126¬† üèóÔ∏è Greyscale conversion",
    "section": "126.2 R Examples",
    "text": "126.2 R Examples"
  },
  {
    "objectID": "image-grayscale.html#python-examples",
    "href": "image-grayscale.html#python-examples",
    "title": "126¬† üèóÔ∏è Greyscale conversion",
    "section": "126.3 Python Examples",
    "text": "126.3 Python Examples"
  },
  {
    "objectID": "image-colors.html#pros-and-cons",
    "href": "image-colors.html#pros-and-cons",
    "title": "127¬† üèóÔ∏è Color Modifications",
    "section": "127.1 Pros and Cons",
    "text": "127.1 Pros and Cons\n\n127.1.1 Pros\n\n\n127.1.2 Cons"
  },
  {
    "objectID": "image-colors.html#r-examples",
    "href": "image-colors.html#r-examples",
    "title": "127¬† üèóÔ∏è Color Modifications",
    "section": "127.2 R Examples",
    "text": "127.2 R Examples"
  },
  {
    "objectID": "image-colors.html#python-examples",
    "href": "image-colors.html#python-examples",
    "title": "127¬† üèóÔ∏è Color Modifications",
    "section": "127.3 Python Examples",
    "text": "127.3 Python Examples"
  },
  {
    "objectID": "image-noise.html#pros-and-cons",
    "href": "image-noise.html#pros-and-cons",
    "title": "128¬† üèóÔ∏è Noise Reduction",
    "section": "128.1 Pros and Cons",
    "text": "128.1 Pros and Cons\n\n128.1.1 Pros\n\n\n128.1.2 Cons"
  },
  {
    "objectID": "image-noise.html#r-examples",
    "href": "image-noise.html#r-examples",
    "title": "128¬† üèóÔ∏è Noise Reduction",
    "section": "128.2 R Examples",
    "text": "128.2 R Examples"
  },
  {
    "objectID": "image-noise.html#python-examples",
    "href": "image-noise.html#python-examples",
    "title": "128¬† üèóÔ∏è Noise Reduction",
    "section": "128.3 Python Examples",
    "text": "128.3 Python Examples"
  },
  {
    "objectID": "image-normalization.html#pros-and-cons",
    "href": "image-normalization.html#pros-and-cons",
    "title": "129¬† üèóÔ∏è Value Normalization",
    "section": "129.1 Pros and Cons",
    "text": "129.1 Pros and Cons\n\n129.1.1 Pros\n\n\n129.1.2 Cons"
  },
  {
    "objectID": "image-normalization.html#r-examples",
    "href": "image-normalization.html#r-examples",
    "title": "129¬† üèóÔ∏è Value Normalization",
    "section": "129.2 R Examples",
    "text": "129.2 R Examples"
  },
  {
    "objectID": "image-normalization.html#python-examples",
    "href": "image-normalization.html#python-examples",
    "title": "129¬† üèóÔ∏è Value Normalization",
    "section": "129.3 Python Examples",
    "text": "129.3 Python Examples"
  },
  {
    "objectID": "image-resize.html#pros-and-cons",
    "href": "image-resize.html#pros-and-cons",
    "title": "130¬† üèóÔ∏è Resizing",
    "section": "130.1 Pros and Cons",
    "text": "130.1 Pros and Cons\n\n130.1.1 Pros\n\n\n130.1.2 Cons"
  },
  {
    "objectID": "image-resize.html#r-examples",
    "href": "image-resize.html#r-examples",
    "title": "130¬† üèóÔ∏è Resizing",
    "section": "130.2 R Examples",
    "text": "130.2 R Examples"
  },
  {
    "objectID": "image-resize.html#python-examples",
    "href": "image-resize.html#python-examples",
    "title": "130¬† üèóÔ∏è Resizing",
    "section": "130.3 Python Examples",
    "text": "130.3 Python Examples"
  },
  {
    "objectID": "image-brightness.html#pros-and-cons",
    "href": "image-brightness.html#pros-and-cons",
    "title": "131¬† üèóÔ∏è Changing Brightness",
    "section": "131.1 Pros and Cons",
    "text": "131.1 Pros and Cons\n\n131.1.1 Pros\n\n\n131.1.2 Cons"
  },
  {
    "objectID": "image-brightness.html#r-examples",
    "href": "image-brightness.html#r-examples",
    "title": "131¬† üèóÔ∏è Changing Brightness",
    "section": "131.2 R Examples",
    "text": "131.2 R Examples"
  },
  {
    "objectID": "image-brightness.html#python-examples",
    "href": "image-brightness.html#python-examples",
    "title": "131¬† üèóÔ∏è Changing Brightness",
    "section": "131.3 Python Examples",
    "text": "131.3 Python Examples"
  },
  {
    "objectID": "image-shift-flip-rotate.html#pros-and-cons",
    "href": "image-shift-flip-rotate.html#pros-and-cons",
    "title": "132¬† üèóÔ∏è Shifting, Flipping, and Rotation",
    "section": "132.1 Pros and Cons",
    "text": "132.1 Pros and Cons\n\n132.1.1 Pros\n\n\n132.1.2 Cons"
  },
  {
    "objectID": "image-shift-flip-rotate.html#r-examples",
    "href": "image-shift-flip-rotate.html#r-examples",
    "title": "132¬† üèóÔ∏è Shifting, Flipping, and Rotation",
    "section": "132.2 R Examples",
    "text": "132.2 R Examples"
  },
  {
    "objectID": "image-shift-flip-rotate.html#python-examples",
    "href": "image-shift-flip-rotate.html#python-examples",
    "title": "132¬† üèóÔ∏è Shifting, Flipping, and Rotation",
    "section": "132.3 Python Examples",
    "text": "132.3 Python Examples"
  },
  {
    "objectID": "image-crop-scale.html#pros-and-cons",
    "href": "image-crop-scale.html#pros-and-cons",
    "title": "133¬† üèóÔ∏è Cropping and Scaling",
    "section": "133.1 Pros and Cons",
    "text": "133.1 Pros and Cons\n\n133.1.1 Pros\n\n\n133.1.2 Cons"
  },
  {
    "objectID": "image-crop-scale.html#r-examples",
    "href": "image-crop-scale.html#r-examples",
    "title": "133¬† üèóÔ∏è Cropping and Scaling",
    "section": "133.2 R Examples",
    "text": "133.2 R Examples"
  },
  {
    "objectID": "image-crop-scale.html#python-examples",
    "href": "image-crop-scale.html#python-examples",
    "title": "133¬† üèóÔ∏è Cropping and Scaling",
    "section": "133.3 Python Examples",
    "text": "133.3 Python Examples"
  },
  {
    "objectID": "image-embeddings.html#pros-and-cons",
    "href": "image-embeddings.html#pros-and-cons",
    "title": "134¬† üèóÔ∏è Image embeddings",
    "section": "134.1 Pros and Cons",
    "text": "134.1 Pros and Cons\n\n134.1.1 Pros\n\n\n134.1.2 Cons"
  },
  {
    "objectID": "image-embeddings.html#r-examples",
    "href": "image-embeddings.html#r-examples",
    "title": "134¬† üèóÔ∏è Image embeddings",
    "section": "134.2 R Examples",
    "text": "134.2 R Examples"
  },
  {
    "objectID": "image-embeddings.html#python-examples",
    "href": "image-embeddings.html#python-examples",
    "title": "134¬† üèóÔ∏è Image embeddings",
    "section": "134.3 Python Examples",
    "text": "134.3 Python Examples"
  },
  {
    "objectID": "relational.html",
    "href": "relational.html",
    "title": "135¬† Overview",
    "section": "",
    "text": "So far in this book, we have almost exclusively talked about tabular data. except for image data in Chapter¬†123, and to some degree time series data as seen in Chapter¬†111. The latter doesn‚Äôt quite count as is it typically stored in a tabular way. This section will talk about the scenario where you are working with more than 1 table. A situation that is quite common.\nWhen you have data, one way to store is in a database. You have several smaller tables with information. Once you want to do some kind of modeling, you go to your database and query out the data so you get it in a tabular format that our modeling tools will accept.\n\n\n\n\n\n\nTODO\n\n\n\nadd some kind of diagram here\n\n\nIt is at this stage we can use some feature engineering tricks. Propose that we are looking at daily sales targets for a number of different stores. There will be tables of the individual store performances, who works there, the items they carry and so on. As well as their past behavior. Using the knowledge of these cross tables can be very valuable. We will look at two ways to handle this. Manually in Chapter¬†136 and automatically in Chapter¬†137."
  },
  {
    "objectID": "relational-manual.html#pros-and-cons",
    "href": "relational-manual.html#pros-and-cons",
    "title": "136¬† üèóÔ∏è Manual",
    "section": "136.1 Pros and Cons",
    "text": "136.1 Pros and Cons\n\n136.1.1 Pros\n\n\n136.1.2 Cons"
  },
  {
    "objectID": "relational-manual.html#r-examples",
    "href": "relational-manual.html#r-examples",
    "title": "136¬† üèóÔ∏è Manual",
    "section": "136.2 R Examples",
    "text": "136.2 R Examples"
  },
  {
    "objectID": "relational-manual.html#python-examples",
    "href": "relational-manual.html#python-examples",
    "title": "136¬† üèóÔ∏è Manual",
    "section": "136.3 Python Examples",
    "text": "136.3 Python Examples"
  },
  {
    "objectID": "relational-auto.html#pros-and-cons",
    "href": "relational-auto.html#pros-and-cons",
    "title": "137¬† üèóÔ∏è Automatic",
    "section": "137.1 Pros and Cons",
    "text": "137.1 Pros and Cons\n\n137.1.1 Pros\n\n\n137.1.2 Cons"
  },
  {
    "objectID": "relational-auto.html#r-examples",
    "href": "relational-auto.html#r-examples",
    "title": "137¬† üèóÔ∏è Automatic",
    "section": "137.2 R Examples",
    "text": "137.2 R Examples"
  },
  {
    "objectID": "relational-auto.html#python-examples",
    "href": "relational-auto.html#python-examples",
    "title": "137¬† üèóÔ∏è Automatic",
    "section": "137.3 Python Examples",
    "text": "137.3 Python Examples"
  },
  {
    "objectID": "video.html",
    "href": "video.html",
    "title": "138¬† Overview",
    "section": "",
    "text": "This chapter is still tentative, if you have any methods you think would fit in this chapter, please post an issue in the Github repository.\nWIP"
  },
  {
    "objectID": "video-tmp.html#pros-and-cons",
    "href": "video-tmp.html#pros-and-cons",
    "title": "139¬† üèóÔ∏è Temporary",
    "section": "139.1 Pros and Cons",
    "text": "139.1 Pros and Cons\n\n139.1.1 Pros\n\n\n139.1.2 Cons"
  },
  {
    "objectID": "video-tmp.html#r-examples",
    "href": "video-tmp.html#r-examples",
    "title": "139¬† üèóÔ∏è Temporary",
    "section": "139.2 R Examples",
    "text": "139.2 R Examples"
  },
  {
    "objectID": "video-tmp.html#python-examples",
    "href": "video-tmp.html#python-examples",
    "title": "139¬† üèóÔ∏è Temporary",
    "section": "139.3 Python Examples",
    "text": "139.3 Python Examples"
  },
  {
    "objectID": "sound.html",
    "href": "sound.html",
    "title": "140¬† Overview",
    "section": "",
    "text": "This chapter is still tentative, if you have any methods you think would fit in this chapter, please post an issue in the Github repository.\nWIP"
  },
  {
    "objectID": "sound-tmp.html#pros-and-cons",
    "href": "sound-tmp.html#pros-and-cons",
    "title": "141¬† üèóÔ∏è Temporary",
    "section": "141.1 Pros and Cons",
    "text": "141.1 Pros and Cons\n\n141.1.1 Pros\n\n\n141.1.2 Cons"
  },
  {
    "objectID": "sound-tmp.html#r-examples",
    "href": "sound-tmp.html#r-examples",
    "title": "141¬† üèóÔ∏è Temporary",
    "section": "141.2 R Examples",
    "text": "141.2 R Examples"
  },
  {
    "objectID": "sound-tmp.html#python-examples",
    "href": "sound-tmp.html#python-examples",
    "title": "141¬† üèóÔ∏è Temporary",
    "section": "141.3 Python Examples",
    "text": "141.3 Python Examples"
  },
  {
    "objectID": "order.html",
    "href": "order.html",
    "title": "142¬† üèóÔ∏è Order of transformations",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "sparse.html",
    "href": "sparse.html",
    "title": "143¬† üèóÔ∏è What should you do if you have sparse data?",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "144¬† üèóÔ∏è How Different Models Deal With Input",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "145¬† üèóÔ∏è Summary",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Micci-Barreca, Daniele. 2001. ‚ÄúA Preprocessing Scheme for\nHigh-Cardinality Categorical Attributes in Classification and Prediction\nProblems.‚Äù SIGKDD Explor. Newsl. 3 (1): 27‚Äì32. https://doi.org/10.1145/507533.507538."
  }
]
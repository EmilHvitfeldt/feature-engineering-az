[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Feature Engineering A-Z",
    "section": "",
    "text": "Welcome to “Feature Engineering A-Z”! This book is written to be used as a reference guide to the different techniques. This is reflected in the chapter structure. Any question a practitioner is having should be answered by looking at the index and finding the right chapter.\nEach section tries to be as comprehensive as possible with the number of different methods/solutions that are presented. A section on dimensionality reduction should list all the practical methods that could be used, as well as a comparison between the methods to help the reader decide what would be most appropriate. This does not mean that all methods are recommended to use. A number of these methods have little and narrow use cases.\nWhenever possible each method will be accompanied by simple mathematical formulas and visualizations to illustrate the mechanics of the method.\nLastly, each section will include code snippets in showcasing how to implement the methods. Preferable in R and Python, Keras/PyTorch. This book is a methods book first, a coding book second."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "motivation.html",
    "href": "motivation.html",
    "title": "2  Motivation",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "modeling.html",
    "href": "modeling.html",
    "title": "3  Where does feature engineering fit into the modeling workflow?",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "how-to-deal-with.html",
    "href": "how-to-deal-with.html",
    "title": "How to Deal With …",
    "section": "",
    "text": "This book is structured according to the types of data and problems you will encounter. Each section specifies a type of data or problem, and each chapter details a method that can be useful in dealing with that type. So for example Chapter 4 contains methods that deal with numeric variables such as Chapter 5 and Chapter 12, and Chapter 16 contains methods that deal with categorical variables such as Chapter 19 and Chapter 30. There should be sections and chapters for most methods you will find in practice that isn’t too domain-specific.\nIt is because of this structure that this book is most suited as reference material, each time you encounter some data you are unsure how to deal with, you find the corresponding section and study the methods listed to see which would be best for your use case. This isn’t to say that you can’t read this book from end to end. In fact, the sections have been ordered roughly such that earlier chapters are broadly useful and later chapters touch on less used data types and problems."
  },
  {
    "objectID": "how-to-deal-with.html#sec-terminology",
    "href": "how-to-deal-with.html#sec-terminology",
    "title": "How to Deal With …",
    "section": "Terminology",
    "text": "Terminology\n\nObservations\n\n\nLearned\n\n\nSupervised / Unsupervised\n\n\nLevels\n\n\nLinear models"
  },
  {
    "objectID": "numeric.html",
    "href": "numeric.html",
    "title": "4  Overview",
    "section": "",
    "text": "Data can come in all shapes and sizes, but the most common one is the numeric variable. These are values such as age, height, deficit, price, and quantity. We call these quantitative variables and they are plentiful in most data sets and immediately usable in all statistical and machine learning models. That being said, it doesn’t mean that there are things that wouldn’t help to be done.\nThe following chapters will focus on different methods that follow 1-to-1 or 1-to-more transformations. These methods will mostly be applied one at a time to each variable. Methods that take many variables and return fewer variables such as dimensionality reduction methods which will be covered in Chapter 49.\nWhen we are working with a single variable at a time, there is a handful of problems we can run into. Identifying the problem and how each of the following methods tries to remedy the said problem is key to getting the most out of numeric variables.\nThe 4 main types of problems we deal with when working with individual numeric variables are:"
  },
  {
    "objectID": "numeric.html#distributional-problems",
    "href": "numeric.html#distributional-problems",
    "title": "4  Overview",
    "section": "4.1 Distributional problems",
    "text": "4.1 Distributional problems\n\nlogarithm\nsqrt\nBoxCox\nYeo-Johnson\nPercentile"
  },
  {
    "objectID": "numeric.html#sec-numeric-scaling-issues",
    "href": "numeric.html#sec-numeric-scaling-issues",
    "title": "4  Overview",
    "section": "4.2 Scaling issues",
    "text": "4.2 Scaling issues\nThe topic of feature scaling is used important and used widely in all of machine learning. This chapter will go over what feature scaling is and why we want to use it. The following chapters will each go over a different method of feature scaling.\n\n\n\n\n\n\nNote\n\n\n\nThere is some disagreement about the naming of these topics. These types of methods are called feature scaling and scaling in different fields. This book will call this general class of methods feature scaling and will make notes for each specific method and what other names they go by.\n\n\nIn this book, we will define feature scaling as an operation that modified variables using multiplication and addition. While broadly defined, the methods typically reduce to the following form:\n\\[\nX_{scaled} = \\dfrac{X - a}{b}\n\\tag{4.1}\\]\nThe main difference between the methods is how \\(a\\) and \\(b\\) are calculated. These methods are learned transformation. So we use the training data to derive the right values of \\(a\\) and \\(b\\), and then these values are used to perform the transformations when applied to new data. The different methods might differ on what property is desired for the transformed variables, same range or same spread, but they never change the distribution itself. The power transformations we saw in Chapter 7 and Chapter 8, distort the transformations, where these feature scalings essentially perform a “zooming” effect.\n\n\nTable 4.1: All feature scaling methods\n\n\n\n\n\n\nMethod\nDefinition\n\n\n\n\nCentering\n\\(X_{scaled} = X - \\text{mean}(X)\\)\n\n\nScaling\n\\(X_{scaled} = \\dfrac{X}{\\text{sd}(X)}\\)\n\n\nMax-Abs\n\\(X_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\\)\n\n\nNormalization\n\\(X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\)\n\n\nMin-Max\n\\(X_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\\)\n\n\nRobust\n\\(X_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)}\\)\n\n\n\n\nWe see here that all the methods in Table 4.1 follows the equation Equation 4.1. Sometimes \\(a\\) and \\(b\\) takes a value of 0, which is perfectly fine. Centering and scaling when used together is equal to normalization. They are kept separate in the table since they are sometimes used independently. Centering, scaling, and normalization will all be discussed in Chapter 10.\nThere are 2 main reasons why we want to perform feature scaling. Firstly, many different types of models take the magnitude of the variables into account when fitting the models, so having variables on different scales can be disadvantageous because some variables have high priorities. In turn, we get that the other variables have low priority. Models that work using Euclidean distances like KNN models are affected by this change. Regularized models such as lasso and ridge regression also need to be scaled since the regularization depends on the magnitude of the estimates. Secondly, some algorithms simply converge much faster when all the variables are on the same scale. These types of models produce the same fit, just at a slower pace than if you don’t scale the variables. Any algorithms using Gradient Descent fits into this category.\nTODO: Have KNN diagram show why this is important\nList which types of models need feature scaling. Should be a 2 column list. Left=name, right=comment %in% c(no effect, different fit, slow down)"
  },
  {
    "objectID": "numeric.html#non-linear-effect",
    "href": "numeric.html#non-linear-effect",
    "title": "4  Overview",
    "section": "4.3 Non-linear effect",
    "text": "4.3 Non-linear effect\n\nSplines\npolynomial"
  },
  {
    "objectID": "numeric.html#sec-numeric-outliers-issues",
    "href": "numeric.html#sec-numeric-outliers-issues",
    "title": "4  Overview",
    "section": "4.4 Outliers",
    "text": "4.4 Outliers"
  },
  {
    "objectID": "numeric.html#other",
    "href": "numeric.html#other",
    "title": "4  Overview",
    "section": "4.5 Other",
    "text": "4.5 Other\nThere are any number of transformations we can apply to numeric data, other functions include:\n\nhyperbolic\nRelu\ninverse\ninverse logit\nlogit"
  },
  {
    "objectID": "numeric-logarithms.html",
    "href": "numeric-logarithms.html",
    "title": "5  Logarithms",
    "section": "",
    "text": "Logarithms come as a tool to deal with highly skewed data. Consider the histogram below, it depicts the lot area for all the houses in the ames data set. It is highly skewed with the majority of areas being less than 10,000 with a handful greater than 50,000 with one over 200,000.\nThis data could cause some problems if we tried to use this variable in its unfinished state. We need to think about how numeric variables are being used in these models. Take a linear regression as an example. It fits under the assumption that there is a linear relationship between the response and the predictors. In other words, the model assumes that a 1000 increase in lot area is going to increase the sale price of the house by the same amount if it occurred to a 5000 lot area house or a 50000 lot area house. This might be a valid scenario for this housing market.Another possibility is that we are seeing diminishing returns, and each increase in lot area is going to cause a smaller increase in predicted sale price.\nThe main identity that is interesting when using logarithms for preprocessing is the multiplicative identity\n\\[\n\\log_{base}(xy) = \\log_{base}(x) + \\log_{base}(y)\n\\]\nThis identity allows us to consider the distances in a specific non-linear way. On the linear scale, we have that the distance between 1 and 2 is the same as 20 to 21 and 10,000 to 10,001. Such a relation is not always what we observe. Consider the relationship between excitement (response) and the number of audience members (predictor) at a concert. Adding a singular audience member is not going to have the same effect for all audience sizes. If we consider \\(y = 1.5\\) then we can look at what happens each time the audience size is increased by 50%. For an initial audience size of 100, the following happens\n\\[\n\\log_{10}(150) = \\log_{10}(100 \\cdot 1.5) = \\log_{10}(100) + \\log_{10}(1.5) \\approx 2 + 0.176\n\\]\nand for an initial audience size of 10,000, we get\n\\[\n\\log_{10}(15,000) = \\log_{10}(10,000 \\cdot 1.5) = \\log_{10}(10,000) + \\log_{10}(1.5) \\approx 4 + 0.176\n\\]\nAnd we see that by using a logarithmic transformation we can translate a multiplicative increase into an additive increase.\nThe logarithmic transformation is perfect for such a scenario. If we take a log2() transformation on our data, then we get a new interpretation of our model. Now each doubling (because we used log2) of the lot area is going to result in the same predicted increase in sale price. We have essentially turned out predictors to work on doublings rather than additions. Below is the same chart as before, but on a logarithmic scale using base 2.\nit is important to note that we are not trying to make the variable normally distributed. What we are trying to accomplish is to remove the skewed nature of the variable. Likewise, this method should not be used a variance reduction tool as that task is handled by doing normalization which we start exploring more in Section 4.2.\nThe full equation we use is\n\\[\ny = log_{base}(x + offset)\n\\]\nWhere \\(x\\) is the input, \\(y\\) is the result. The base in and of itself makes a difference, as the choice of base only matters in a multiplicative way. Common choices for bases are \\(e\\), 2, and 10. I find that using a base of 2 yields nice interpretive properties as an increase of 1 in \\(x\\) results in a doubling of \\(y\\).\nOne thing we haven’t touched on yet is that logarithms are only defined for positive input. The offset is introduced to deal with this problem. The offset will default to 0 in most software you will encounter. If you know that your variables contain values from -100 to 10000, you can set offset = 101 to make sure we never take the logarithm of a negative number. A common scenario is non-negative, which contains 0, setting offset = 0.5 helps in that case. The choice of offset you choose does matter a little bit. The offset should generally be smaller than the smallest non-negative value you have, otherwise, you will influence the proportions too much. Below we use an offset of 0.1, and the relationship we get out of this is that distance from 0 to 1, is roughly the same as multiplying by 10.\nOn the other hand, if we set the offset to 0.001, we get that the difference from 0 to 1 is the same as the distance from 1 to 1000, or as the same as multiplying by 1000.\nThe use of logarithms will have different effects on different types of models. Linear models will see a change in transformed variables since we can better model relationships between the response and dependent variable if the transformation leads to a more linear relationship. Tree-based models should in theory not be affected by the use of logarithms, however, some implementations use binning to reduce the number of splitting points for considerations. You will see a change if these bins are based on fixed-width intervals. This is expected to have minimal effect on the final model."
  },
  {
    "objectID": "numeric-logarithms.html#pros-and-cons",
    "href": "numeric-logarithms.html#pros-and-cons",
    "title": "5  Logarithms",
    "section": "5.1 Pros and Cons",
    "text": "5.1 Pros and Cons\n\n5.1.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data set alike\n\n\n\n5.1.2 Cons\n\nNeeds offset to deal with negative data\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn’t skewed. See the effect below on 10,000 uniformly distributed values"
  },
  {
    "objectID": "numeric-logarithms.html#r-examples",
    "href": "numeric-logarithms.html#r-examples",
    "title": "5  Logarithms",
    "section": "5.2 R Examples",
    "text": "5.2 R Examples\nWe will be using the ames data set for these examples.\n\names |>\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      <int>        <int>      <int>\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# … with 2,920 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nlog_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>\n  step_log(Lot_Area)\n\nlog_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      <dbl>      <int>\n 1    10.4      215000\n 2     9.36     105000\n 3     9.57     172000\n 4     9.32     244000\n 5     9.53     189900\n 6     9.21     195500\n 7     8.50     213500\n 8     8.52     191500\n 9     8.59     236500\n10     8.92     189000\n# … with 2,920 more rows\n\n\nThe base can be changed by setting the base argument.\n\nlog_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>\n  step_log(Lot_Area, base = 2)\n\nlog_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      <dbl>      <int>\n 1     15.0     215000\n 2     13.5     105000\n 3     13.8     172000\n 4     13.4     244000\n 5     13.8     189900\n 6     13.3     195500\n 7     12.3     213500\n 8     12.3     191500\n 9     12.4     236500\n10     12.9     189000\n# … with 2,920 more rows\n\n\nIf we have non-positive values, which we do in the Wood_Deck_SF variable because it has quite a lot of zeroes, we get -Inf which isn’t going to work.\n\nlog_rec <- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |>\n  step_log(Wood_Deck_SF)\n\nlog_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Wood_Deck_SF Sale_Price\n          <dbl>      <int>\n 1         5.35     215000\n 2         4.94     105000\n 3         5.97     172000\n 4      -Inf        244000\n 5         5.36     189900\n 6         5.89     195500\n 7      -Inf        213500\n 8      -Inf        191500\n 9         5.47     236500\n10         4.94     189000\n# … with 2,920 more rows\n\n\nSetting the offset argument helps us to deal with that problem.\n\nlog_rec <- recipe(Sale_Price ~ Wood_Deck_SF, data = ames) |>\n  step_log(Wood_Deck_SF, offset = 0.5)\n\nlog_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Wood_Deck_SF Sale_Price\n          <dbl>      <int>\n 1        5.35      215000\n 2        4.95      105000\n 3        5.98      172000\n 4       -0.693     244000\n 5        5.36      189900\n 6        5.89      195500\n 7       -0.693     213500\n 8       -0.693     191500\n 9        5.47      236500\n10        4.95      189000\n# … with 2,920 more rows"
  },
  {
    "objectID": "numeric-logarithms.html#python-examples",
    "href": "numeric-logarithms.html#python-examples",
    "title": "5  Logarithms",
    "section": "5.3 Python Examples",
    "text": "5.3 Python Examples\nTODO"
  },
  {
    "objectID": "numeric-sqrt.html",
    "href": "numeric-sqrt.html",
    "title": "6  Square Root",
    "section": "",
    "text": "As we saw in Chapter 5 about logarithms, we sometimes have to deal with highly skewed data. Square roots are another way to deal with this issue, with some different pros and cons that make it better to use us in some situations. We will spend our time in this section to talk about what those are.\nBelow is a histogram of the average daily rate of a number of hotel stays. It is clear to see that this is another case where the data is highly skewed, with many values close to zero, but a few in the thousands.\nThis variable contains some negative values with the smallest being -6.38. We wouldn’t want to throw out the negative values. And we could think of many situations where both negative and positive values are part of a skewed distribution, especially financial. Bank account balances, delivery times, etc etc.\nWe need a method that transforms the scale to un-skew that also works with negative data. The square root could be what we are looking for. By itself, it takes as its input a positive number and returns the number that when multiplied with itself equals the input. This has the desired shrinking effect, where larger values are shrunk more than smaller values. Additionally, since its domain is the positive numbers (0 is a special case since it maps to itself) we can mirror it to work on negative numbers in the same way it worked on positive numbers. This gives us the signed square root\n\\[\ny = \\text{sign}(x)\\sqrt{\\left| x \\right|}\n\\]\nBelow we see the results of applying the signed square root.\nit is important to note that we are not trying to make the variable normally distributed. What we are trying to accomplish is to remove the skewed nature of the variable. Likewise, this method should not be used a variance reduction tool as that task is handled by doing normalization which we start exploring more in Section 4.2.\nIt doesn’t have the same power to shrink large values as logarithms do, but it will seamlessly work with negative values and it would allow you to pick up on quadratic effects that you wouldn’t otherwise be able to pick up if you hadn’t applied the transformation. It also doesn’t have good inferential properties. It preserves the order of the numeric values, but it doesn’t give us a good way to interpret changes."
  },
  {
    "objectID": "numeric-sqrt.html#pros-and-cons",
    "href": "numeric-sqrt.html#pros-and-cons",
    "title": "6  Square Root",
    "section": "6.1 Pros and Cons",
    "text": "6.1 Pros and Cons\n\n6.1.1 Pros\n\nA non-trained operation, can easily be applied to training and testing data set alike\nCan be applied to all numbers, not just non-negative values\n\n\n\n6.1.2 Cons\n\nIt will leave regression coefficients virtually uninterpretable\nIs not a universal fix. While it can make skewed distributions less skewed. It has the opposite effect on a distribution that isn’t skewed"
  },
  {
    "objectID": "numeric-sqrt.html#r-examples",
    "href": "numeric-sqrt.html#r-examples",
    "title": "6  Square Root",
    "section": "6.2 R Examples",
    "text": "6.2 R Examples\nWe will be using the hotel_bookings data set for these examples.\n\nhotel_bookings |>\n  select(lead_time, adr)\n\n# A tibble: 119,390 × 2\n   lead_time   adr\n       <dbl> <dbl>\n 1       342    0 \n 2       737    0 \n 3         7   75 \n 4        13   75 \n 5        14   98 \n 6        14   98 \n 7         0  107 \n 8         9  103 \n 9        85   82 \n10        75  106.\n# … with 119,380 more rows\n\n\n{recipes} provides a step to perform logarithms, which out of the box uses \\(e\\) as the base with an offset of 0.\n\n# TODO use signed sqrt\nsqrt_rec <- recipe(lead_time ~ adr, data = hotel_bookings) |>\n  step_sqrt(adr)\n\nsqrt_rec |>\n  prep() |>\n  bake(new_data = NULL)\n\nWarning in sqrt(getElement(new_data, col_names[i])): NaNs produced\n\n\n# A tibble: 119,390 × 2\n     adr lead_time\n   <dbl>     <dbl>\n 1  0          342\n 2  0          737\n 3  8.66         7\n 4  8.66        13\n 5  9.90        14\n 6  9.90        14\n 7 10.3          0\n 8 10.1          9\n 9  9.06        85\n10 10.3         75\n# … with 119,380 more rows\n\n\n\n6.2.1 Python Examples"
  },
  {
    "objectID": "numeric-boxcox.html",
    "href": "numeric-boxcox.html",
    "title": "7  Box-Cox",
    "section": "",
    "text": "You have likely heard a lot of talk about having normally distributed predictors. This isn’t that common of an assumption, and having a fairly non-skewed symmetric predictor is often enough. Linear Discriminant Analysis assumes Gaussian data, and that is about it (TODO add a reference here). Still, it is worthwhile to have more symmetric predictors, and this is where the Box-Cox transformation comes into play.\nIn Chapter 5 on logarithms, we saw how they could be used to change the distribution of a variable. One of the downsides is that if we want to get closer to normality, it doesn’t do well unless applied to a log-normally distributed variable. The Box-Cox transformation tries to find an optimal power transformation. This method was originally intended to be used on the outcome of a model.\nIt works by using maximum likelihood estimation to estimate a transformation parameter \\(\\lambda\\) in the following equation that would optimize the normality of \\(x^*\\)\n\\[\nx^* = \\left\\{\n    \\begin{array}{ll}\n      \\dfrac{x^\\lambda - 1}{\\lambda \\tilde{x}^{\\lambda - 1}}, & \\lambda \\neq 0 \\\\\n      \\tilde{x} \\log x & \\lambda = 0\n    \\end{array}\n  \\right.\n\\]\nwhere \\(\\tilde{x}\\) is the geometric mean of \\(x\\). It is worth noting again, that what we are optimizing over is the value of \\(\\lambda\\). This is also a case of a trained preprocessing method when used on the predictors. We need to estimate the parameter \\(\\lambda\\) on the training data set, then use the estimated value to apply the transformation to the training and test data set to avoid data leakage. Lastly, Box-Cox only works with positive numbers. Take a look at Chapter 8 about the Yeo-Johnson method that tries to accomplish the same thing, and it works on positive as well as negative numbers.\nLet us see some examples of Box-Cox at work. Below is three different simulated distribution, before and after they have been transformed by Box-Cox.\nWe have the original distributions have some left or right skewed-ness. And the transformed columns look better, in the sense that they are less skewed and they are fairly symmetric around the center. Are they perfectly normal? No! but these transformations might be beneficial. The next set of distributions wasn’t so lucky.\nThe Box-Cox method isn’t magic and will only give you something more normally distributed if the distribution can be made more normally distributed by applying a power transformation.\nThe first distribution here is uniformly random. The resulting transformation ends up more skewed, even if only a little bit, than the original distribution because this method is not intended for this type of data. We are seeing similar results with the bi-modal distributions."
  },
  {
    "objectID": "numeric-boxcox.html#pros-and-cons",
    "href": "numeric-boxcox.html#pros-and-cons",
    "title": "7  Box-Cox",
    "section": "7.1 Pros and Cons",
    "text": "7.1 Pros and Cons\n\n7.1.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\n\n\n\n7.1.2 Cons\n\nDoesn’t work with negative values\nIsn’t a universal fix"
  },
  {
    "objectID": "numeric-boxcox.html#r-examples",
    "href": "numeric-boxcox.html#r-examples",
    "title": "7  Box-Cox",
    "section": "7.2 R Examples",
    "text": "7.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      <int>        <int>      <int>\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# … with 2,920 more rows\n\n\n{recipes} provides a step to perform Box-Cox transformations.\n\nboxcox_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>\n  step_BoxCox(Lot_Area) |>\n  prep()\n\nboxcox_rec |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      <dbl>      <int>\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# … with 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nboxcox_rec |>\n  tidy(1)\n\n# A tibble: 1 × 3\n  terms    value id          \n  <chr>    <dbl> <chr>       \n1 Lot_Area 0.129 BoxCox_3gJXR\n\n\n\n7.2.1 Python Examples"
  },
  {
    "objectID": "numeric-yeojohnson.html",
    "href": "numeric-yeojohnson.html",
    "title": "8  Yeo-Johnson",
    "section": "",
    "text": "You have likely heard a lot of talk about having normally distributed predictors. This isn’t that common of an assumption, and having a fairly non-skewed symmetric predictor is often enough. Linear Discriminant Analysis assumes Gaussian data, and that is about it (TODO add a reference here). Still, it is worthwhile to have more symmetric predictors, and this is where the Yeo-Johnson transformation comes into play.\nThis method is very similar to the Box-Cox method in Chapter 7, except it doesn’t have the restriction that the variable \\(x\\) needs to be positive.\nIt works by using maximum likelihood estimation to estimate a transformation parameter \\(\\lambda\\) in the following equation that would optimize the normality of \\(x^*\\)\n\\[\nx^* = \\left\\{\n    \\begin{array}{ll}\n      \\dfrac{(x + 1) ^ \\lambda - 1}{\\lambda}              & \\lambda \\neq 0, x \\geq 0 \\\\\n      \\log(x + 1)                                         & \\lambda =    0, x \\geq 0 \\\\\n      - \\dfrac{(-x + 1) ^ {2 - \\lambda} - 1}{2 - \\lambda} & \\lambda \\neq 2, x <    0 \\\\\n      - \\log(-x + 1)                                      & \\lambda =    2, x <    0\n    \\end{array}\n  \\right.\n\\] It is worth noting again, that what we are optimizing over is the value of \\(\\lambda\\). This is also a case of a trained preprocessing method when used on the predictors. We need to estimate the parameter \\(\\lambda\\) on the training data set, then use the estimated value to apply the transformation to the training and test data set to avoid data leakage.\nIf the values of \\(x\\) are strictly positive, then the Yeo-Johnson transformation is the same as the Box-Cox transformation of \\(x + 1\\), if the values of \\(x\\) are strictly negative then the transformation is the Box-Cox transformation of \\(-x + 1\\) with the power \\(2 - \\lambda\\). The interpretation of \\(\\lambda\\) isn’t as easy as for the Box-Cox method.\nLet us see some examples of Yeo-Johnson at work. Below is three different simulated distribution, before and after they have been transformed by Yeo-Johnson.\nWe have the original distributions have some left or right skewed-ness. And the transformed columns look better, in the sense that they are less skewed and they are fairly symmetric around the center. Are they perfectly normal? No! but these transformations might be beneficial. We also notice how these methods work, even when there are negative values.\nThe Yeo-Johnson method isn’t magic and will only give you something more normally distributed if the distribution can be made more normally distributed by applying the above formula would give you some more normally distributed values.\nThe first distribution here is uniformly random. The resulting transformation ends up more skewed, even if only a little bit, than the original distribution because this method is not intended for this type of data. We are seeing similar results with the bi-modal distributions."
  },
  {
    "objectID": "numeric-yeojohnson.html#pros-and-cons",
    "href": "numeric-yeojohnson.html#pros-and-cons",
    "title": "8  Yeo-Johnson",
    "section": "8.1 Pros and Cons",
    "text": "8.1 Pros and Cons\n\n8.1.1 Pros\n\nMore flexible than individually chosen power transformations such as logarithms and square roots\nCan handle negative values\n\n\n\n8.1.2 Cons\n\nIsn’t a universal fix"
  },
  {
    "objectID": "numeric-yeojohnson.html#r-examples",
    "href": "numeric-yeojohnson.html#r-examples",
    "title": "8  Yeo-Johnson",
    "section": "8.2 R Examples",
    "text": "8.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      <int>        <int>      <int>\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# … with 2,920 more rows\n\n\n{recipes} provides a step to perform Yeo-Johnson transformations, which out of the box uses \\(e\\) as the base with an offset of 0.\n\nyeojohnson_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>\n  step_YeoJohnson(Lot_Area) |>\n  prep()\n\nyeojohnson_rec |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      <dbl>      <int>\n 1     21.8     215000\n 2     18.2     105000\n 3     18.9     172000\n 4     18.1     244000\n 5     18.8     189900\n 6     17.7     195500\n 7     15.5     213500\n 8     15.5     191500\n 9     15.8     236500\n10     16.8     189000\n# … with 2,920 more rows\n\n\nWe can also pull out the value of the estimated \\(\\lambda\\) by using the tidy() method on the recipe step.\n\nyeojohnson_rec |>\n  tidy(1)\n\n# A tibble: 1 × 3\n  terms    value id              \n  <chr>    <dbl> <chr>           \n1 Lot_Area 0.129 YeoJohnson_3gJXR\n\n\n\n8.2.1 Python Examples"
  },
  {
    "objectID": "numeric-percentile.html",
    "href": "numeric-percentile.html",
    "title": "9  Percentile",
    "section": "",
    "text": "Percentile scaling (also sometimes called Rank Scaling) is a method where we apply a non-linear transformation to to our data where each value is the percentile of the training data.\nTODO: Add equation\nThis does a couple of things for us. It naturally constraints the transformed data into the range \\([0, 1]\\), and it deals with outlier values nicely in the sense that they don’t change the transformation that much. Moreover if the testing distribution is close to the training distribution then the transformed distribution would be approximately uniformly distributed between 0 and 1."
  },
  {
    "objectID": "numeric-percentile.html#pros-and-cons",
    "href": "numeric-percentile.html#pros-and-cons",
    "title": "9  Percentile",
    "section": "9.1 Pros and Cons",
    "text": "9.1 Pros and Cons\n\n9.1.1 Pros\n\nTransformation isn’t affected much by outliers\n\n\n\n9.1.2 Cons\n\nDoesn’t allow to exact reverse transformation\nIsn’t ideal if training data doesn’t have that many unique values"
  },
  {
    "objectID": "numeric-percentile.html#r-examples",
    "href": "numeric-percentile.html#r-examples",
    "title": "9  Percentile",
    "section": "9.2 R Examples",
    "text": "9.2 R Examples\nWe will be using the ames data set for these examples.\n\n\n\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Lot_Area, Wood_Deck_SF, Sale_Price)\n\n# A tibble: 2,930 × 3\n   Lot_Area Wood_Deck_SF Sale_Price\n      <int>        <int>      <int>\n 1    31770          210     215000\n 2    11622          140     105000\n 3    14267          393     172000\n 4    11160            0     244000\n 5    13830          212     189900\n 6     9978          360     195500\n 7     4920            0     213500\n 8     5005            0     191500\n 9     5389          237     236500\n10     7500          140     189000\n# … with 2,920 more rows\n\n\nThe {recipes} step to do this transformation is step_percentile(). It defaults to calculation 100 percentiles and using those to transform the data\n\npercentile_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>\n  step_percentile(Lot_Area) |>\n  prep()\n\npercentile_rec |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      <dbl>      <int>\n 1    0.989     215000\n 2    0.756     105000\n 3    0.898     172000\n 4    0.717     244000\n 5    0.883     189900\n 6    0.580     195500\n 7    0.104     213500\n 8    0.106     191500\n 9    0.120     236500\n10    0.259     189000\n# … with 2,920 more rows\n\n\nWe cna use the tidy() method to pull out what the specific values are for each percentile\n\npercentile_rec |>\n  tidy(1)\n\n# A tibble: 99 × 4\n   term     value percentile id              \n   <chr>    <dbl>      <dbl> <chr>           \n 1 Lot_Area 1300           0 percentile_Bp5vK\n 2 Lot_Area 1680           1 percentile_Bp5vK\n 3 Lot_Area 2040.          2 percentile_Bp5vK\n 4 Lot_Area 2362.          3 percentile_Bp5vK\n 5 Lot_Area 2779.          4 percentile_Bp5vK\n 6 Lot_Area 3188.          5 percentile_Bp5vK\n 7 Lot_Area 3674.          6 percentile_Bp5vK\n 8 Lot_Area 3901.          7 percentile_Bp5vK\n 9 Lot_Area 4122.          8 percentile_Bp5vK\n10 Lot_Area 4435           9 percentile_Bp5vK\n# … with 89 more rows\n\n\nYou are able to change the granularity by using the options argument. In this example we are calculation 500 points evenly spaced between 0 and 1, both inclusive.\n\npercentile500_rec <- recipe(Sale_Price ~ Lot_Area, data = ames) |>\n  step_percentile(Lot_Area, options = list(probs = (0:500)/500)) |>\n  prep()\n\npercentile500_rec |>\n  bake(new_data = NULL)\n\n# A tibble: 2,930 × 2\n   Lot_Area Sale_Price\n      <dbl>      <int>\n 1    0.989     215000\n 2    0.755     105000\n 3    0.899     172000\n 4    0.717     244000\n 5    0.884     189900\n 6    0.580     195500\n 7    0.103     213500\n 8    0.106     191500\n 9    0.118     236500\n10    0.254     189000\n# … with 2,920 more rows\n\n\nAnd we can see the more precise numbers.\n\npercentile500_rec |>\n  tidy(1)\n\n# A tibble: 457 × 4\n   term     value percentile id              \n   <chr>    <dbl>      <dbl> <chr>           \n 1 Lot_Area 1300         0   percentile_RUieL\n 2 Lot_Area 1487.        0.2 percentile_RUieL\n 3 Lot_Area 1531.        0.4 percentile_RUieL\n 4 Lot_Area 1605.        0.6 percentile_RUieL\n 5 Lot_Area 1680         0.8 percentile_RUieL\n 6 Lot_Area 1879.        1.4 percentile_RUieL\n 7 Lot_Area 1890         1.6 percentile_RUieL\n 8 Lot_Area 1946.        1.8 percentile_RUieL\n 9 Lot_Area 2040.        2   percentile_RUieL\n10 Lot_Area 2136.        2.2 percentile_RUieL\n# … with 447 more rows\n\n\nNotice how there are only 457 values in this output. This is happening because some percentile have been collapsed to save space since if the value for the 10.4 and 10.6 percentile is the same, we just store the 10.6 value.\n\n9.2.1 Python Examples"
  },
  {
    "objectID": "numeric-normalization.html",
    "href": "numeric-normalization.html",
    "title": "10  Normalization",
    "section": "",
    "text": "Normalization is a method where we modify a variable by subtracting the mean and dividing by the standard deviation\n\\[X_{scaled} = \\dfrac{X - \\text{mean}(X)}{\\text{sd}(X)}\\]\nPerforming this transformation means that the resulting variable will have a mean of 0 and a standard deviation and variance of 1. This method is a learned transformation. So we use the training data to derive the right values of \\(\\text{sd}(X)\\) and \\(\\text{mean}(X)\\) and then these values are used to perform the transformations when applied to new data. It is a common misconception that this transformation is done to make the data normally distributed. This transformation doesn’t change the distribution, it scales the values. Below is a figure Figure 10.1 that illustrates that point\nIn Figure 10.1 we see some distribution, before and after applying normalization to it. Both the original and transformed distribution are bimodal, and the transformed distribution is no more normal than the original. And that is fine because the transformation did its job by moving the values close to 0 and a specific spread, which in this case is a variance of 1."
  },
  {
    "objectID": "numeric-normalization.html#pros-and-cons",
    "href": "numeric-normalization.html#pros-and-cons",
    "title": "10  Normalization",
    "section": "10.1 Pros and Cons",
    "text": "10.1 Pros and Cons\n\n10.1.1 Pros\n\nIf you don’t have any severe outliers then you will rarely see any downsides to applying normalization\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n10.1.2 Cons\n\nNot all software solutions will not be helpful when applying this transformation to a constant variable. A division by 0 error is likely what you will see\nCannot be used with sparse data as it isn’t preserved because of the centering that is happening. If you only scale the data you don’t have a problem\nThis transformation is highly affected by outliers, as they affect the mean and standard deviation quite a lot\n\nBelow is the figure Figure 10.2 is an illustration of the effect by having a single high value. In this case, a single observation with the value 10000 moved the transformed distribution much tighter around zero. And all but removed the variance of the non-outliers.\n\n\n\n\n\nFigure 10.2: Outliers can have a big effect on the resulting distribution when applying normalization."
  },
  {
    "objectID": "numeric-normalization.html#r-examples",
    "href": "numeric-normalization.html#r-examples",
    "title": "10  Normalization",
    "section": "10.2 R Examples",
    "text": "10.2 R Examples\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <int>        <int>        <dbl>\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# … with 2,920 more rows\n\n\n{recipes} provides a step to perform scaling, centering, and normalization. They are called step_scale(), step_center() and step_normalize() respectively.\nBelow is an example using step_scale()\n\nscale_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_scale(all_numeric_predictors()) |>\n  prep()\n\nscale_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000    4.03          1.66        0.627\n 2     105000    1.47          1.11        0    \n 3     172000    1.81          3.11        0.605\n 4     244000    1.42          0           0    \n 5     189900    1.76          1.68        0    \n 6     195500    1.27          2.85        0.112\n 7     213500    0.624         0           0    \n 8     191500    0.635         0           0    \n 9     236500    0.684         1.88        0    \n10     189000    0.952         1.11        0    \n# … with 2,920 more rows\n\n\nWe can also pull out the value of the standard deviation for each variable that was affected using tidy()\n\nscale_rec |>\n  tidy(1)\n\n# A tibble: 33 × 3\n   terms            value id         \n   <chr>            <dbl> <chr>      \n 1 Lot_Frontage     33.5  scale_FGmgk\n 2 Lot_Area       7880.   scale_FGmgk\n 3 Year_Built       30.2  scale_FGmgk\n 4 Year_Remod_Add   20.9  scale_FGmgk\n 5 Mas_Vnr_Area    179.   scale_FGmgk\n 6 BsmtFin_SF_1      2.23 scale_FGmgk\n 7 BsmtFin_SF_2    169.   scale_FGmgk\n 8 Bsmt_Unf_SF     440.   scale_FGmgk\n 9 Total_Bsmt_SF   441.   scale_FGmgk\n10 First_Flr_SF    392.   scale_FGmgk\n# … with 23 more rows\n\n\nWe could also have used step_center() and step_scale() together in one recipe\n\ncenter_scale_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_center(all_numeric_predictors()) |>\n  step_scale(all_numeric_predictors()) |>\n  prep()\n\ncenter_scale_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# … with 2,920 more rows\n\n\nUsing tidy() we can see information about each step\n\ncenter_scale_rec |>\n  tidy()\n\n# A tibble: 2 × 6\n  number operation type   trained skip  id          \n   <int> <chr>     <chr>  <lgl>   <lgl> <chr>       \n1      1 step      center TRUE    FALSE center_tSRk5\n2      2 step      scale  TRUE    FALSE scale_kjP2v \n\n\nAnd we can pull out the means using tidy(1)\n\ncenter_scale_rec |>\n  tidy(1)\n\n# A tibble: 33 × 3\n   terms             value id          \n   <chr>             <dbl> <chr>       \n 1 Lot_Frontage      57.6  center_tSRk5\n 2 Lot_Area       10148.   center_tSRk5\n 3 Year_Built      1971.   center_tSRk5\n 4 Year_Remod_Add  1984.   center_tSRk5\n 5 Mas_Vnr_Area     101.   center_tSRk5\n 6 BsmtFin_SF_1       4.18 center_tSRk5\n 7 BsmtFin_SF_2      49.7  center_tSRk5\n 8 Bsmt_Unf_SF      559.   center_tSRk5\n 9 Total_Bsmt_SF   1051.   center_tSRk5\n10 First_Flr_SF    1160.   center_tSRk5\n# … with 23 more rows\n\n\nand the standard deviation using tidy(2)\n\ncenter_scale_rec |>\n  tidy(2)\n\n# A tibble: 33 × 3\n   terms            value id         \n   <chr>            <dbl> <chr>      \n 1 Lot_Frontage     33.5  scale_kjP2v\n 2 Lot_Area       7880.   scale_kjP2v\n 3 Year_Built       30.2  scale_kjP2v\n 4 Year_Remod_Add   20.9  scale_kjP2v\n 5 Mas_Vnr_Area    179.   scale_kjP2v\n 6 BsmtFin_SF_1      2.23 scale_kjP2v\n 7 BsmtFin_SF_2    169.   scale_kjP2v\n 8 Bsmt_Unf_SF     440.   scale_kjP2v\n 9 Total_Bsmt_SF   441.   scale_kjP2v\n10 First_Flr_SF    392.   scale_kjP2v\n# … with 23 more rows\n\n\nSince these steps often follow each other, we often use the step_normalize() as a shortcut to do both operations in one step\n\nscale_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_normalize(all_numeric_predictors()) |>\n  prep()\n\nscale_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000   2.74          0.920       0.0610\n 2     105000   0.187         0.366      -0.566 \n 3     172000   0.523         2.37        0.0386\n 4     244000   0.128        -0.742      -0.566 \n 5     189900   0.467         0.936      -0.566 \n 6     195500  -0.0216        2.11       -0.454 \n 7     213500  -0.663        -0.742      -0.566 \n 8     191500  -0.653        -0.742      -0.566 \n 9     236500  -0.604         1.13       -0.566 \n10     189000  -0.336         0.366      -0.566 \n# … with 2,920 more rows\n\n\nAnd we can still pull out the means and standard deviations using tidy()\n\nscale_rec |>\n  tidy(1) |>\n  filter(terms %in% c(\"Lot_Area\", \"Wood_Deck_SF\", \"Mas_Vnr_Area\"))\n\n# A tibble: 6 × 4\n  terms        statistic   value id             \n  <chr>        <chr>       <dbl> <chr>          \n1 Lot_Area     mean      10148.  normalize_ucdPw\n2 Mas_Vnr_Area mean        101.  normalize_ucdPw\n3 Wood_Deck_SF mean         93.8 normalize_ucdPw\n4 Lot_Area     sd         7880.  normalize_ucdPw\n5 Mas_Vnr_Area sd          179.  normalize_ucdPw\n6 Wood_Deck_SF sd          126.  normalize_ucdPw\n\n\n\n10.2.1 Python Examples"
  },
  {
    "objectID": "numeric-range.html",
    "href": "numeric-range.html",
    "title": "11  Range",
    "section": "",
    "text": "Range scaling, also known as Min-Max scaling, is a method where to make the data fit into a pre-defined interval. Typically the equation used to illustrate this method is show like so\n\\[\nX_{scaled} = \\dfrac{X - \\text{min}(X)}{\\text{max}(X) - \\text{min}(X)}\n\\tag{11.1}\\]\nThis equations only shows what happens if you use the software with defaults. This will scale your variable to be in the range \\([0, 1]\\). It is important to treat this as a learned method, and we will thus calculate \\(\\text{min}(X)\\) and \\(\\text{max}(X)\\) on the training data set, and use those to apply the transformation on new data. One of the benefits of this method is that it transforms any variable into a predefined range of 0 and 1. Unless of cause a new data point is outside the range. Suppose that the original data was on the range \\(\\text{min}(X) = 5\\) and \\(\\text{max}(X) = 100\\), and we encounter a new data point with the value 200. Then according to Equation 11.2 we would get a transformed value of (200-5)/(100-5) = 2.0526 which is very much not between 0 and 1. We thus have a violation of our original goal. One way to get around this is to use clipping. Clipping is a technique when using this method where any values less then \\(\\text{min}(X)\\) will be noted as 0 and any values larger than \\(\\text{max}(X)\\) will be noted as 1. This little tick ensures that any new data points stays within the range. Expending our equation to the following\n\\[\nX_{scaled}=    \n\\begin{cases}        \n0, & X_{new} < \\text{min}(X_{train})\\\\        \n1 & X_{new} > \\text{max}(X_{train})\\\\\n\\dfrac{X_{new} - \\text{min}(X_{train})}{\\text{max}(X_{train}) - \\text{min}(X_{train})}, & \\text{otherwise}\n\\end{cases}\n\\tag{11.2}\\]\nThe equation is a lot bigger, but was it different is that we are specify that the transformation should be done using training data, and by adding a couple of branches to reflect the clipping. In practice it will look something like Figure 11.1 where the original data had a range of \\([10, 32]\\).\nSadly we are not done quite yet. One last thing that Equation 11.2 doesn’t take into account is that sometimes people don’t want to transformation to be into any range, not just \\([0, 1]\\) . This gives us the final equation\n\\[\nX_{scaled}=    \n\\begin{cases}        \nR_{lower}, & X_{new} < \\text{min}(X_{train})\\\\        \nR_{upper} & X_{new} > \\text{max}(X_{train})\\\\\n\\dfrac{X_{new} - \\text{min}(X_{train})}{\\text{max}(X_{train}) - \\text{min}(X_{train})} \\cdot (R_{upper}-R_{lower}) + R_{lower}, & \\text{otherwise}\n\\end{cases}\n\\tag{11.3}\\]\nWhere Equation 11.3 now have \\(R_{lower}\\) to represent a user defined lower bound, and \\(R_{upper}\\) representing the corresponding user defined upper bound. I would recommend that you keep Equation 11.1 in your mind when thinking about this method, but also include a little footnote that it doesn’t include all the little options.\nOne thing you should know is how this transformation is affected by outliers. Clipping essentially ignores the magnitude of how much an outlier is. There is no difference between a new value of 100 and 100000 to a variable that had a range of \\([0, 90]\\) on the training data set. This might be a problem, and it is up to you as the practitioner to decide. One option would be to turn off clipping, but it would violate the assumption that all future transformed observations will be within a specific range.\nBelow is the figure Figure 11.2 is an illustration of the effect by having a single high value. In this case, a single observation with the value `10000` moved the transformed distribution much tighter around zero. And all but removed the variance of the non-outliers."
  },
  {
    "objectID": "numeric-range.html#pros-and-cons",
    "href": "numeric-range.html#pros-and-cons",
    "title": "11  Range",
    "section": "11.1 Pros and Cons",
    "text": "11.1 Pros and Cons\n\n11.1.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale, provided that clipping wasn’t turned on\n\n\n\n11.1.2 Cons\n\nTurning on clipping diminishes the effect of outliers by rounding them up/down\nDoesn’t work with zero variance data as max(x) - min(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn’t preserved"
  },
  {
    "objectID": "numeric-range.html#r-examples",
    "href": "numeric-range.html#r-examples",
    "title": "11  Range",
    "section": "11.2 R Examples",
    "text": "11.2 R Examples\nstep_range() clips. Does allow user to specify range step_minmax() doesn’t clip. doens’t allow user to specify range. A PR is planned to allow users to turn off clipping in step_range()\nWe will be using the ames data set for these examples.\n\nlibrary(recipes)\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <int>        <int>        <dbl>\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# … with 2,920 more rows\n\n\nWe will be using the step_range() step for this\n\nrange_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_range(all_numeric_predictors()) |>\n  prep()\n\nrange_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000   0.142        0.147        0.07  \n 2     105000   0.0482       0.0983       0     \n 3     172000   0.0606       0.276        0.0675\n 4     244000   0.0461       0            0     \n 5     189900   0.0586       0.149        0     \n 6     195500   0.0406       0.253        0.0125\n 7     213500   0.0169       0            0     \n 8     191500   0.0173       0            0     \n 9     236500   0.0191       0.166        0     \n10     189000   0.0290       0.0983       0     \n# … with 2,920 more rows\n\n\nWe can also pull out what the min and max value were for each variable using tidy()\n\nrange_rec |>\n  tidy(1)\n\n# A tibble: 33 × 4\n   terms            min    max id         \n   <chr>          <dbl>  <dbl> <chr>      \n 1 Lot_Frontage       0    313 range_FGmgk\n 2 Lot_Area        1300 215245 range_FGmgk\n 3 Year_Built      1872   2010 range_FGmgk\n 4 Year_Remod_Add  1950   2010 range_FGmgk\n 5 Mas_Vnr_Area       0   1600 range_FGmgk\n 6 BsmtFin_SF_1       0      7 range_FGmgk\n 7 BsmtFin_SF_2       0   1526 range_FGmgk\n 8 Bsmt_Unf_SF        0   2336 range_FGmgk\n 9 Total_Bsmt_SF      0   6110 range_FGmgk\n10 First_Flr_SF     334   5095 range_FGmgk\n# … with 23 more rows\n\n\nusing the min and max argument we can set different ranges\n\nrange_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_range(all_numeric_predictors(), min = -2, max = 2) |>\n  prep()\n\nrange_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000    -1.43       -1.41         -1.72\n 2     105000    -1.81       -1.61         -2   \n 3     172000    -1.76       -0.896        -1.73\n 4     244000    -1.82       -2            -2   \n 5     189900    -1.77       -1.40         -2   \n 6     195500    -1.84       -0.989        -1.95\n 7     213500    -1.93       -2            -2   \n 8     191500    -1.93       -2            -2   \n 9     236500    -1.92       -1.33         -2   \n10     189000    -1.88       -1.61         -2   \n# … with 2,920 more rows\n\n\n\n11.2.1 Python Examples\nMinMaxScaler() doesn’t clip by default. Allows user to specify range."
  },
  {
    "objectID": "numeric-maxabs.html",
    "href": "numeric-maxabs.html",
    "title": "12  Max Abs",
    "section": "",
    "text": "The Max-Abs scaling method works by making sure that the training data lies with the range \\([-1, 1]\\) by applying the following formula\n\\[\nX_{scaled} = \\dfrac{X}{\\text{max}(\\text{abs}(X))}\n\\tag{12.1}\\]\nThis is similar to the scaling we saw in Chapter 10. And we see that the only difference is whether we are aiming for the statistical properly (standard deviation of 1) or a specific decision (dividing by largest quantity seen). This method is a learned transformation. So we use the training data to derive the right value of \\(\\text{max}(\\text{abs}(X))\\) and then this value is used to perform the transformations when applied to new data. For this there is no specific guidance as to which method you want to use and you need to look at your data and see what works best."
  },
  {
    "objectID": "numeric-maxabs.html#pros-and-cons",
    "href": "numeric-maxabs.html#pros-and-cons",
    "title": "12  Max Abs",
    "section": "12.1 Pros and Cons",
    "text": "12.1 Pros and Cons\n\n12.1.1 Pros\n\nFast calculations\nTransformation can easily be reversed, making its interpretations easier on the original scale\nDoesn’t affect sparsity\nCan be used on a zero variance variable. Doesn’t matter such since you likely should get rid of it\n\n\n\n12.1.2 Cons\n\nIs highly affected by outliers"
  },
  {
    "objectID": "numeric-maxabs.html#r-examples",
    "href": "numeric-maxabs.html#r-examples",
    "title": "12  Max Abs",
    "section": "12.2 R Examples",
    "text": "12.2 R Examples\nWe will be using the ames data set for these examples.\n\n\n\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <int>        <int>        <dbl>\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# … with 2,920 more rows\n\n\nWe will be using the step_maxabs() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_maxabs(all_numeric_predictors()) |>\n  prep()\n\nmaxabs_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000   0.148        0.147        0.07  \n 2     105000   0.0540       0.0983       0     \n 3     172000   0.0663       0.276        0.0675\n 4     244000   0.0518       0            0     \n 5     189900   0.0643       0.149        0     \n 6     195500   0.0464       0.253        0.0125\n 7     213500   0.0229       0            0     \n 8     191500   0.0233       0            0     \n 9     236500   0.0250       0.166        0     \n10     189000   0.0348       0.0983       0     \n# … with 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |>\n  tidy(1)\n\n# A tibble: 33 × 4\n   terms          statistic  value id          \n   <chr>          <chr>      <dbl> <chr>       \n 1 Lot_Frontage   max          313 maxabs_Bp5vK\n 2 Lot_Area       max       215245 maxabs_Bp5vK\n 3 Year_Built     max         2010 maxabs_Bp5vK\n 4 Year_Remod_Add max         2010 maxabs_Bp5vK\n 5 Mas_Vnr_Area   max         1600 maxabs_Bp5vK\n 6 BsmtFin_SF_1   max            7 maxabs_Bp5vK\n 7 BsmtFin_SF_2   max         1526 maxabs_Bp5vK\n 8 Bsmt_Unf_SF    max         2336 maxabs_Bp5vK\n 9 Total_Bsmt_SF  max         6110 maxabs_Bp5vK\n10 First_Flr_SF   max         5095 maxabs_Bp5vK\n# … with 23 more rows\n\n\n\n12.2.1 Python Examples"
  },
  {
    "objectID": "numeric-robust.html",
    "href": "numeric-robust.html",
    "title": "13  Robust Scaling",
    "section": "",
    "text": "Range scaling, is a scaling method that is typically done be removing the median and dividing by the interquartile range. As illustrated by Equation 13.1\n\\[X_{scaled} = \\dfrac{X - \\text{median}(X)}{\\text{Q3}(X) - \\text{Q1}(X)} \\tag{13.1}\\]\nThis is the most common formulation of this method. This method is a learned transformation. So we use the training data to derive the right values of \\(\\text{Q3}(X)\\), \\(\\text{Q1}(X)\\), and \\(\\text{median}(X)\\) and then these values are used to perform the transformations when applied to new data. You are not bound to use Q1 and Q3, any quantiles can be used. Most software implementations allow you to modify the ranges. It is typically recommended that you pick a symmetric range like \\([0.1, 0.9]\\) or \\([0.3, 0.7]\\) unless you have a good reason why observations high or low should be excluded.\nThis method is normally showcased as a way to scale variables with outliers in them. That is true, in so far that you don’t take the outer quantiles into considerations. The default range means that only 50% of the observations are used to calculate the scaling statistics. This is fine if you want to ignore the outliers, however it is conventionally not a good idea to outright ignore outliers, so you might want to take a look at Section 4.4 before you throw away the information that is present in the outliers."
  },
  {
    "objectID": "numeric-robust.html#pros-and-cons",
    "href": "numeric-robust.html#pros-and-cons",
    "title": "13  Robust Scaling",
    "section": "13.1 Pros and Cons",
    "text": "13.1 Pros and Cons\n\n13.1.1 Pros\n\nIsn’t affected by outliers\nTransformation can easily be reversed, making its interpretations easier on the original scale\n\n\n\n13.1.2 Cons\n\nCompletely ignores part of the data outside the quantile ranges\nDoesn’t work with near zero variance data as Q1(x) - Q3(x) = 0, yielding a division by zero\nCannot be used with sparse data as it isn’t preserved"
  },
  {
    "objectID": "numeric-robust.html#r-examples",
    "href": "numeric-robust.html#r-examples",
    "title": "13  Robust Scaling",
    "section": "13.2 R Examples",
    "text": "13.2 R Examples\nWe will be using the ames data set for these examples.\n\n\n\n\n# remotes::install_github(\"emilhvitfeldt/extrasteps\")\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(extrasteps)\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <int>        <int>        <dbl>\n 1     215000    31770          210          112\n 2     105000    11622          140            0\n 3     172000    14267          393          108\n 4     244000    11160            0            0\n 5     189900    13830          212            0\n 6     195500     9978          360           20\n 7     213500     4920            0            0\n 8     191500     5005            0            0\n 9     236500     5389          237            0\n10     189000     7500          140            0\n# … with 2,920 more rows\n\n\nWe will be using the step_robust() step for this, and it can be found in the extrasteps extension package.\n\nmaxabs_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_robust(all_numeric_predictors()) |>\n  prep()\n\nmaxabs_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000    5.43         1.25         0.688\n 2     105000    0.531        0.833        0    \n 3     172000    1.17         2.34         0.664\n 4     244000    0.419        0            0    \n 5     189900    1.07         1.26         0    \n 6     195500    0.132        2.14         0.123\n 7     213500   -1.10         0            0    \n 8     191500   -1.08         0            0    \n 9     236500   -0.984        1.41         0    \n10     189000   -0.471        0.833        0    \n# … with 2,920 more rows\n\n\nWe can also pull out what the max values were for each variable using tidy()\n\nmaxabs_rec |>\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms          statistic  value id          \n   <chr>          <chr>      <dbl> <chr>       \n 1 Lot_Frontage   lower        43  robust_Bp5vK\n 2 Lot_Frontage   median       63  robust_Bp5vK\n 3 Lot_Frontage   higher       78  robust_Bp5vK\n 4 Lot_Area       lower      7440. robust_Bp5vK\n 5 Lot_Area       median     9436. robust_Bp5vK\n 6 Lot_Area       higher    11555. robust_Bp5vK\n 7 Year_Built     lower      1954  robust_Bp5vK\n 8 Year_Built     median     1973  robust_Bp5vK\n 9 Year_Built     higher     2001  robust_Bp5vK\n10 Year_Remod_Add lower      1965  robust_Bp5vK\n# … with 89 more rows\n\n\nWe can also can\n\nmaxabs_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_robust(all_numeric_predictors(), range = c(0.1, 0.9)) |>\n  prep()\n\nmaxabs_rec |>\n  bake(new_data = NULL, Sale_Price, Lot_Area, Wood_Deck_SF, Mas_Vnr_Area)\n\n# A tibble: 2,930 × 4\n   Sale_Price Lot_Area Wood_Deck_SF Mas_Vnr_Area\n        <int>    <dbl>        <dbl>        <dbl>\n 1     215000   2.35          0.820       0.350 \n 2     105000   0.230         0.547       0     \n 3     172000   0.509         1.53        0.337 \n 4     244000   0.181         0           0     \n 5     189900   0.463         0.828       0     \n 6     195500   0.0570        1.41        0.0625\n 7     213500  -0.475         0           0     \n 8     191500  -0.467         0           0     \n 9     236500  -0.426         0.925       0     \n10     189000  -0.204         0.547       0     \n# … with 2,920 more rows\n\n\nwhen we pull out the ranges, we see that they are wider\n\nmaxabs_rec |>\n  tidy(1)\n\n# A tibble: 99 × 4\n   terms          statistic  value id          \n   <chr>          <chr>      <dbl> <chr>       \n 1 Lot_Frontage   lower         0  robust_RUieL\n 2 Lot_Frontage   median       63  robust_RUieL\n 3 Lot_Frontage   higher       91  robust_RUieL\n 4 Lot_Area       lower      4800  robust_RUieL\n 5 Lot_Area       median     9436. robust_RUieL\n 6 Lot_Area       higher    14299. robust_RUieL\n 7 Year_Built     lower      1925. robust_RUieL\n 8 Year_Built     median     1973  robust_RUieL\n 9 Year_Built     higher     2006  robust_RUieL\n10 Year_Remod_Add lower      1950  robust_RUieL\n# … with 89 more rows\n\n\n\n13.2.1 Python Examples"
  },
  {
    "objectID": "numeric-splines.html",
    "href": "numeric-splines.html",
    "title": "14  Splines",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "numeric-polynomial.html",
    "href": "numeric-polynomial.html",
    "title": "15  Polynomial",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "categorical.html",
    "href": "categorical.html",
    "title": "16  Overview",
    "section": "",
    "text": "One of the most common types of data you can encounter is categorical variables. These variables take non-numeric values and are things like names, animal, housetype zipcode, mood, and so on. We call these qualitative variables and you will have to find a way to deal with them as they are plentiful in most data sets you will encounter. You will, however, unlike numerical variables in Chapter 4, have to transform these variables into numeric variables as most models only work with numeric variables. The main exception to this is tree-based models such as decision trees, random forests, and boosted trees. This is of cause a theoretical fact, as some implementations of tree-based models don’t support categorical variables.\nThe above description is a small but useful simplification. A categorical variable can take a known or an unknown number of unique values. day_of_the_week and zipcode are examples of variables with a fixed known number of values. Even if our data only contains Sundays, Mondays, and Thursdays we know that there are 7 different possible options. On the other hand, there are plenty of categorical variables where the levels are realistically unknown, such as company_name, street_name, and food_item. This distinction matters as it can be used to inform the pre-processing that will be done.\nAnother nuance is some categorical variables have an inherent order to them. So the variables size with the values \"small\", \"medium\", and \"large\", they clearly have a ordering to then \"small\" < \"medium\" < \"large\". This is unlike the theoretical car_color with the values “blue”,“red”, and“black”`, which doesn’t have a natural ordering. Depending on whether a variable has an ordering, we can use that information. This is something that doesn’t have to be dealt with, but the added information can be useful.\nLastly, we have the encoding these categorical variables have. The same variable household_pet can be encoded as [\"cat\", \"cat\", \"dog\", \"cat\", \"hamster\"] or as [1, 1, 2, 1, 3]. The latter (hopefully) accompanied with a data dictionary saying [1 = \"cat\", 2 = \"dog\", 3 = \"hamster\"]. These variables contain the exact same information, but the encoding is vastly different, and if you are not careful to treat household_pet as a categorical variable the model with believing that \"hamster\" - \"cat\" = \"dog\".\nTODO find an example of the above data encoding in the wild\nTODO add a section about data formats that preserve levels\nThe chapters in this section can be put into 2 categories:"
  },
  {
    "objectID": "categorical.html#categorical-to-categorical",
    "href": "categorical.html#categorical-to-categorical",
    "title": "16  Overview",
    "section": "16.1 Categorical to Categorical",
    "text": "16.1 Categorical to Categorical\nThese methods take a categorical variable and improve them. Whether it means cleaning levels, collapsing levels, or making sure it handles new levels correctly. These Tasks as not always needed depending on the method you are using but they are generally helpful to apply. One method that would have been located here if it wasn’t for the fact that it has a whole section by itself is dealing with missing values as seen in Chapter 47."
  },
  {
    "objectID": "categorical.html#categorical-to-numerical",
    "href": "categorical.html#categorical-to-numerical",
    "title": "16  Overview",
    "section": "16.2 Categorical to Numerical",
    "text": "16.2 Categorical to Numerical\nThe vast majority of the chapters in this section concern methods that take a categorical variable and produce one or more numerical variables suitable for modeling. There are quite a lot of different methods, all have upsides and downsides and they will all be explored in the remaining chapters."
  },
  {
    "objectID": "categorical-cleaning.html",
    "href": "categorical-cleaning.html",
    "title": "17  Cleaning",
    "section": "",
    "text": "For cleaning categorical data, there is a whole host of problems you can have in your data. We won’t be able to describe how to deal with every single type of problem. Instead, we will go over a class of common problems. In an ideal world, you wouldn’t have these problems, but that isn’t one we are living in right now. Mistakes are happening for a multitude of reasons and it is your job to deal with it.\nTODO: Figure out how to properly reference https://twitter.com/camfassett/status/1575578362927452160\nLook at the following vector of spellings of St Albans, the city in England.\nThey all refer to the same city name, they don’t just agree on the spelling. If we didn’t perform any cleaning, most of the following methods would assume that these spellings are completely different works and should be treated as such. Depending on the method this would vastly change the output and dilute any information you may have in your data.\nThis will of course depend on whether these spelling differences are of importance in your model. Only you, the practitioner, will know the answer to that question. This is where you need to be in touch with the people generating your data. In the above situation, one could imagine that the data comes from 2 sources; one using a drop-down menu to select the city, and one where the city is being typed in manually on a tablet. There will be big differences. In a case such as that, we would want all these spellings to be fixed up.\nDomain knowledge is important. In the case of spell-checking, we need to be careful not to over-correct the data by collapsing two different items together by accident.\nBack to the example at hand. The first thing we notice is that there is a difference in capitalization. In this case, since we presumably are working with a variable of city names, capitalization shouldn’t matter. A common trick is to either turn everything upper-case or lower-case. I prefer to lower-casing as I find the results easier to read.\nBy turning everything into lower-case, we were able to remove 2 of the errors. Next, we see that some of these spellings include periods. For this example, I’m going to make the decision that they are safe to remove as well.\nThis removes yet another of our problems. This next problem is absolutely horrible and is so easy to overlook. Notice how the second spelling has a double space between st and alban? At a glance that can be hard to see. Likewise, the last spelling has a trailing space. Let us fix this as well.\nNow we are left with two values, which we manually would have to deal with. But even if we had to manually write out the mapping, it is a lot easier to write it for 2 different spellings than 7.\nAnother problem that you may or may not run into, depends on the resilience of your modeling package. Some implementations are rather fragile when it comes to the column names of your data, Non-ASCII characters, punctuation and even spaces can cause errors. At this point in our journey, we are almost there, and we can replace the spaces with underscores to be left with st_albans.\nOne problem we didn’t see in the above example is what happens you are working with accented characters. Consider the German word “schön.” The o with an umlaut (two dots over it) is a fairly simple character, but it can be represented in a couple of different ways. We can either use a single character \\U00f6 to represent the letter with an umlaut. Alternatively, we can use two characters, one for the o and one character to denote the presence of two dots over the previous character \\U0308. As you can imagine this can happen with many words if you have free-form data from a wide array of backgrounds. The method needed to fix this problem is called Unicode Normalization.\nTODO Pull nuggets from https://twitter.com/Emil_Hvitfeldt/status/1583466133977382912\nThis chapter was not meant to scare you, what I hope you get away from this chapter is that it is very important that you look at your data carefully, especially categorical variables that can be varied in so many unexpected ways. Nevertheless, using the above-described techniques, combined with some subject matter expertise should get you quite a long way."
  },
  {
    "objectID": "categorical-cleaning.html#r-examples",
    "href": "categorical-cleaning.html#r-examples",
    "title": "17  Cleaning",
    "section": "17.1 R examples",
    "text": "17.1 R examples\nTODO find good data set for these examples\nUse janitor\ntextrecipes::step_clean_levels()"
  },
  {
    "objectID": "categorical-cleaning.html#python-examples",
    "href": "categorical-cleaning.html#python-examples",
    "title": "17  Cleaning",
    "section": "17.2 Python Examples",
    "text": "17.2 Python Examples"
  },
  {
    "objectID": "categorical-unseen.html",
    "href": "categorical-unseen.html",
    "title": "18  Unseen Levels",
    "section": "",
    "text": "When you are dealing with categorical variables, it is understood that they can take many values. And we have various methods about how to deal with these categorical values, regardless of what values they take. One problem that eventually will happen for you is that you try to apply a trained preprocessor on data that has levels in your categorical variable that you haven’t seen before. This can happen when you are fitting your model using resampled data, when you are applying your model on the testing data set or, if you are unlucky, at some future time in production.\nTODO add diagram here\nThe reason why you need to think about this problem is that some methods and/or models will complain and even error if you are providing unseen levels. Some implementation will allow you deal with this at the method level. Other methods such as Chapter 30 doesn’t care at all that you have unseen levels in your data.\nOne surefire way to deal with this issue is to add a step in your data preprocessing pipeline that will turn any unseen levels into \"unseen\". What this method does in practice, is that it looks at your categorical variables during training, taking note of all the levels it sees and saves them. Then any time the preprocessing is applied to new data it will look at the levels again, and if it sees a level it haven’t seen, label it \"unseen\" (or any other meaningful label that doesn’t conflict with the data). This way, you have that any future levels.\nTODO add diagram here"
  },
  {
    "objectID": "categorical-unseen.html#r-examples",
    "href": "categorical-unseen.html#r-examples",
    "title": "18  Unseen Levels",
    "section": "18.1 R Examples",
    "text": "18.1 R Examples\nWe will be using the nycflights13 data set. We are downsampling just a bit to only work on the first day and doing a test-train split.\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(rsample)\nlibrary(nycflights13)\n\nflights <- flights |>\n  filter(year == 2013, month == 1, day == 1)\n\nset.seed(13630)\nflights_split <- initial_split(flights)\nflights_train <- training(flights_split)\nflights_test <- testing(flights_split)\n\nNow we are doing the cardinal sin by looking at the testing data. But in this case it is okay because we are doing it for educational purposed.\n\nflights_train |> pull(carrier) |> unique() |> sort()\n\n [1] \"9E\" \"AA\" \"B6\" \"DL\" \"EV\" \"F9\" \"FL\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\nflights_test |> pull(carrier) |> unique() |> sort()\n\n [1] \"9E\" \"AA\" \"AS\" \"B6\" \"DL\" \"EV\" \"FL\" \"HA\" \"MQ\" \"UA\" \"US\" \"VX\" \"WN\"\n\n\nNotice that the testing data includes the carrier \"AS\" and \"HA\" but the training data doesn’t know that. Let us see what would happen if we were to calculate dummy variables without doing any adjusting.\n\ndummy_spec <- recipe(arr_delay ~ carrier, data = flights_train) |>\n  step_dummy(carrier)\n\ndummy_spec_prepped <- prep(dummy_spec)\n\nbake(dummy_spec_prepped, new_data = flights_test)\n\nWarning: There are new levels in a factor: AS, HA\n\n\n# A tibble: 211 × 12\n   arr_delay carrier_AA carrie…¹ carri…² carri…³ carri…⁴ carri…⁵ carri…⁶ carri…⁷\n       <dbl>      <int>    <int>   <int>   <int>   <int>   <int>   <int>   <int>\n 1        12          0        0       0       0       0       0       0       1\n 2         8          1        0       0       0       0       0       0       0\n 3       -14          0        0       0       0       0       0       0       1\n 4        -6          0        1       0       0       0       0       0       0\n 5        -3          1        0       0       0       0       0       0       0\n 6       -33          0        0       1       0       0       0       0       0\n 7        -7          1        0       0       0       0       0       0       0\n 8         5          0        1       0       0       0       0       0       0\n 9        31          1        0       0       0       0       0       0       0\n10       -10         NA       NA      NA      NA      NA      NA      NA      NA\n# … with 201 more rows, 3 more variables: carrier_US <int>, carrier_VX <int>,\n#   carrier_WN <int>, and abbreviated variable names ¹​carrier_B6, ²​carrier_DL,\n#   ³​carrier_EV, ⁴​carrier_F9, ⁵​carrier_FL, ⁶​carrier_MQ, ⁷​carrier_UA\n\n\nWe get a warning, and if you look at the rows that were affected we see that it produces NAs. Let us now use the function step_novel() that implements the above described method.\n\nnovel_spec <- recipe(arr_delay ~ carrier, data = flights_train) |>\n  step_novel(carrier) |>\n  step_dummy(carrier) \n\nnovel_spec_prepped <- prep(novel_spec)\n\nbake(novel_spec_prepped, new_data = flights_test)\n\n# A tibble: 211 × 13\n   arr_delay carrier_AA carrie…¹ carri…² carri…³ carri…⁴ carri…⁵ carri…⁶ carri…⁷\n       <dbl>      <int>    <int>   <int>   <int>   <int>   <int>   <int>   <int>\n 1        12          0        0       0       0       0       0       0       1\n 2         8          1        0       0       0       0       0       0       0\n 3       -14          0        0       0       0       0       0       0       1\n 4        -6          0        1       0       0       0       0       0       0\n 5        -3          1        0       0       0       0       0       0       0\n 6       -33          0        0       1       0       0       0       0       0\n 7        -7          1        0       0       0       0       0       0       0\n 8         5          0        1       0       0       0       0       0       0\n 9        31          1        0       0       0       0       0       0       0\n10       -10          0        0       0       0       0       0       0       0\n# … with 201 more rows, 4 more variables: carrier_US <int>, carrier_VX <int>,\n#   carrier_WN <int>, carrier_new <int>, and abbreviated variable names\n#   ¹​carrier_B6, ²​carrier_DL, ³​carrier_EV, ⁴​carrier_F9, ⁵​carrier_FL,\n#   ⁶​carrier_MQ, ⁷​carrier_UA\n\n\nAnd we see that we get no error or anything."
  },
  {
    "objectID": "categorical-unseen.html#python-examples",
    "href": "categorical-unseen.html#python-examples",
    "title": "18  Unseen Levels",
    "section": "18.2 Python Examples",
    "text": "18.2 Python Examples\nTODO"
  },
  {
    "objectID": "categorical-dummy.html",
    "href": "categorical-dummy.html",
    "title": "19  Dummy Encoding",
    "section": "",
    "text": "We have some categorical variable and we want to turn it into numerical values, one of the most common ways of going about it it so create dummy variables. Dummy variables are variables that only takes the values 0 and 1 to indicate the absence or presence of the levels in a categorical variable. This is nicely shown with an example.\nConsider this short categorical variable of animals, we observe there are 3 unique values “cat”, “dog”, and “horse”.\nWhich just this knowledge we can create the corresponding dummy variables. There should be 3 columns one for each of the levels\nFrom this we have a couple of observations. Firstly the length of each of these variables are equal to the length of the original categorical variable. The number of columns corresponds to the number of levels. And lastly the sum of all the values on each row equals 1 since all the rows contains one 1 and the remaining 0s. This means that for even small number of levels, you get sparse data. Sparse data is data where there is a lot of zeroes, meaning that it it would take less space to store where the non-zero values are instead of all the values. You can read more about how and when to care about sparse data in Chapter 78. What this means for dummy variable creating, that depending on whether your software can handle sparse data, you might need to limit the number of levels in your categorical variables. One way to do this would be to collapse levels together, which you can read about in Chapter 43.\nDummy variable creation is a trained method. This means that doing the training step, the levels of each categorical variables are saved, and then these and only these values are used for dummy variable creation. If we assumed that the above example data were used to train the preprocessor, and we passed in the values [\"dog\", \"cat\", \"cat\", \"dog\"] during future applications, we would expect the following dummy variables\nit is important that the horse variable is here too, even if it is empty as the subsequent preprocessing steps and model expects the horse variable to be present. Likewise you can run into problems if the value \"duck\" was used as the preprosessor wouldn’t know what to do. These cases are talked about in Chapter 18."
  },
  {
    "objectID": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "href": "categorical-dummy.html#dummy-or-one-hot-encoding",
    "title": "19  Dummy Encoding",
    "section": "19.1 Dummy or one-hot encoding",
    "text": "19.1 Dummy or one-hot encoding\nTODO add diagram\nThe terms dummy encoding and one-hot encoding gets thrown around interchangeably, but they do have different and distinct meanings. One-hot encoding is when you return k variables when you have k different levels. Like we have shown above\n\n\n     cat dog horse\n[1,]   0   1     0\n[2,]   1   0     0\n[3,]   0   0     1\n[4,]   0   1     0\n[5,]   1   0     0\n\n\nDummy encoding on the other hand returns k-1 variables, where the excluded one typically is the first one.\n\n\n     dog horse\n[1,]   1     0\n[2,]   0     0\n[3,]   0     1\n[4,]   1     0\n[5,]   0     0\n\n\nThese two encodings store the exact same information, even though the dummy encoding has 1 less column. Because we are able to deduce which observations are cat by find the rows with all zeros. The main reason why one would use dummy variables is because of what some people call the dummy variable trap. When you use one-hot encoding, you are increasing the likely that you run into collinearity problem. With the above example, if you included an intercept in your model you have that intercept = cat + dog + horse which gives perfect collinearity and would cause some models to error as they aren’t able to handle that.\n\n\n\n\n\n\nNote\n\n\n\nAn intercept is a variable that takes the value 1 for all entries.\n\n\nEven if you don’t include an intercept you could still run into collinearity. Imagine that in addition to the animal variable also creates a one-hot encoding of the home variable taking the two values \"house\" and \"apartment\", you would get the following indicator variables\n\n\n     cat dog horse house apartment\n[1,]   0   1     0     0         1\n[2,]   1   0     0     1         0\n[3,]   0   0     1     0         1\n[4,]   0   1     0     0         1\n[5,]   1   0     0     1         0\n\n\nAnd in this case we have that house = cat + dog + horse - apartment which again is an example of perfect collinearity. Unless you have a reason to otherwise I would suggest that you use dummy encoding in your models. Additionally this leads to slightly smaller models as they each categorical variable produces 1 less variable. It is worth noting that the choice between dummy encoding and one-hot encoding does matter for some models such as decision trees. Depending on what types of rules they are able to use. Being able to write animal == \"cat\" is easier then saying animal != \"dog\" & animal != \"horse\". This is unlikely to be an issue as many tree based models can work with categorical variables directly without the need for encoding."
  },
  {
    "objectID": "categorical-dummy.html#ordered-factors",
    "href": "categorical-dummy.html#ordered-factors",
    "title": "19  Dummy Encoding",
    "section": "19.2 Ordered factors",
    "text": "19.2 Ordered factors\nTODO"
  },
  {
    "objectID": "categorical-dummy.html#contrasts",
    "href": "categorical-dummy.html#contrasts",
    "title": "19  Dummy Encoding",
    "section": "19.3 Contrasts",
    "text": "19.3 Contrasts\nTODO"
  },
  {
    "objectID": "categorical-dummy.html#pros-and-cons",
    "href": "categorical-dummy.html#pros-and-cons",
    "title": "19  Dummy Encoding",
    "section": "19.4 Pros and Cons",
    "text": "19.4 Pros and Cons\n\n19.4.1 Pros\n\nVersatile and commonly used\nEasy interpretation\nWill rarely leads to a decrease in performance\n\n\n\n19.4.2 Cons\n\nDoes require fairly clean categorical levels\nCan be quite memory intensive if you have many levels in your categorical variables and you are unable to use sparse representation\nProvides a complete, but not neasesarily compact set of variables"
  },
  {
    "objectID": "categorical-dummy.html#r-examples",
    "href": "categorical-dummy.html#r-examples",
    "title": "19  Dummy Encoding",
    "section": "19.5 R Examples",
    "text": "19.5 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allow us to perform dummy encoding and one-hot encoding.\n\n\n\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        <int> <fct>                               <fct>                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# … with 2,920 more rows\n\n\nWe can take a quick look at the the possible values MS_SubClass takes\n\names |>\n  count(MS_SubClass, sort = TRUE)\n\n# A tibble: 16 × 2\n   MS_SubClass                                   n\n   <fct>                                     <int>\n 1 One_Story_1946_and_Newer_All_Styles        1079\n 2 Two_Story_1946_and_Newer                    575\n 3 One_and_Half_Story_Finished_All_Ages        287\n 4 One_Story_PUD_1946_and_Newer                192\n 5 One_Story_1945_and_Older                    139\n 6 Two_Story_PUD_1946_and_Newer                129\n 7 Two_Story_1945_and_Older                    128\n 8 Split_or_Multilevel                         118\n 9 Duplex_All_Styles_and_Ages                  109\n10 Two_Family_conversion_All_Styles_and_Ages    61\n11 Split_Foyer                                  48\n12 Two_and_Half_Story_All_Ages                  23\n13 One_and_Half_Story_Unfinished_All_Ages       18\n14 PUD_Multilevel_Split_Level_Foyer             17\n15 One_Story_with_Finished_Attic_All_Ages        6\n16 One_and_Half_Story_PUD_All_Ages               1\n\n\nAnd since MS_SubClass is a factor, we can verity that they match and that all the levels are observed\n\names |> pull(MS_SubClass) |> levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_dummy() step for this, which defaults to creating dummy variables\n\ndummy_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_dummy(all_nominal_predictors()) |>\n  prep()\n\ndummy_rec |>\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |>\n  glimpse()\n\nRows: 2,930\nColumns: 21\n$ MS_SubClass_One_Story_1945_and_Older                  <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_1946_and_Newer                  <int> 0, 0, 0, 0, 1, 1…\n$ MS_SubClass_Two_Story_1945_and_Older                  <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_or_Multilevel                       <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_Foyer                               <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Residential_High_Density                    <int> 0, 1, 0, 0, 0, 0…\n$ MS_Zoning_Residential_Low_Density                     <int> 1, 0, 1, 1, 1, 1…\n$ MS_Zoning_Residential_Medium_Density                  <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_A_agr                                       <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_C_all                                       <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_I_all                                       <int> 0, 0, 0, 0, 0, 0…\n\n\nWe can pull the factor levels for each variable by using tidy(). If a character vector was present in the data set, it would reord the observed variables.\n\ndummy_rec |>\n  tidy(1)\n\n# A tibble: 243 × 3\n   terms       columns                                id         \n   <chr>       <chr>                                  <chr>      \n 1 MS_SubClass One_Story_1945_and_Older               dummy_Bp5vK\n 2 MS_SubClass One_Story_with_Finished_Attic_All_Ages dummy_Bp5vK\n 3 MS_SubClass One_and_Half_Story_Unfinished_All_Ages dummy_Bp5vK\n 4 MS_SubClass One_and_Half_Story_Finished_All_Ages   dummy_Bp5vK\n 5 MS_SubClass Two_Story_1946_and_Newer               dummy_Bp5vK\n 6 MS_SubClass Two_Story_1945_and_Older               dummy_Bp5vK\n 7 MS_SubClass Two_and_Half_Story_All_Ages            dummy_Bp5vK\n 8 MS_SubClass Split_or_Multilevel                    dummy_Bp5vK\n 9 MS_SubClass Split_Foyer                            dummy_Bp5vK\n10 MS_SubClass Duplex_All_Styles_and_Ages             dummy_Bp5vK\n# … with 233 more rows\n\n\nsetting one_hot = TRUE gives us the complete one-hot encoding results.\n\nonehot_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>\n  prep()\n\nonehot_rec |>\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\")) |>\n  glimpse()\n\nRows: 2,930\nColumns: 23\n$ MS_SubClass_One_Story_1946_and_Newer_All_Styles       <int> 1, 1, 1, 1, 0, 0…\n$ MS_SubClass_One_Story_1945_and_Older                  <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_with_Finished_Attic_All_Ages    <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages    <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_Finished_All_Ages      <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_1946_and_Newer                  <int> 0, 0, 0, 0, 1, 1…\n$ MS_SubClass_Two_Story_1945_and_Older                  <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_and_Half_Story_All_Ages               <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_or_Multilevel                       <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Split_Foyer                               <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Duplex_All_Styles_and_Ages                <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_Story_PUD_1946_and_Newer              <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_One_and_Half_Story_PUD_All_Ages           <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Story_PUD_1946_and_Newer              <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_PUD_Multilevel_Split_Level_Foyer          <int> 0, 0, 0, 0, 0, 0…\n$ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Floating_Village_Residential                <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_Residential_High_Density                    <int> 0, 1, 0, 0, 0, 0…\n$ MS_Zoning_Residential_Low_Density                     <int> 1, 0, 1, 1, 1, 1…\n$ MS_Zoning_Residential_Medium_Density                  <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_A_agr                                       <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_C_all                                       <int> 0, 0, 0, 0, 0, 0…\n$ MS_Zoning_I_all                                       <int> 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "categorical-dummy.html#python-examples",
    "href": "categorical-dummy.html#python-examples",
    "title": "19  Dummy Encoding",
    "section": "19.6 Python Examples",
    "text": "19.6 Python Examples\nTODO"
  },
  {
    "objectID": "categorical-label.html",
    "href": "categorical-label.html",
    "title": "20  Label Encoding",
    "section": "",
    "text": "Label encoding (also called integer encoding) is a method that map the categorical levels into the integers 1 through n where n is the number of levels.\nThis method is a trained method since the preprocessor need to keep a record of the possible values and their corresponding integer value. Unseen levels can be encoding to outside the range to be either 0 or n + 1, allowing unseen levels to be handled with minimal extra work.\nTODO add diagram\nThis method is often not ideal as the ordering of the levels will matter a lot for the performance of the model that needs to make sense of the generated numeric variables. For a variable with the levels “Studio”, “Apartment”, “Loft”, “Duplex”. This variable contains 4! = 4 * 3 * 2 * 1 = 24 different orderings. And since the number of permutations are calculated calculated with factorials, this number goes up fast. With just 10 levels we are looking at 3,628,800 different orderings. Even if some orders are better than others, It would be a very slow task to iterate through to find which ones are good. If you have prior information about the levels, then you should use Chapter 21.\nIf you are working an implementation that works with factors, then they will be used. Otherwise the ordering most likely will be alphabetical or in order of occurrence. You should check the documentation of your implementation to figure out which.\nThe performance of this method will depend a lot on the model. If you are working with a linear model, then you most likely are out of luck as it wouldn’t be able to use a variable where the value 2, 6, and 10 are provide evidence one way, and the rest are provide evidence the other way. Three-based models will be able to do better, but would do even better if label encoding was applied to begin with."
  },
  {
    "objectID": "categorical-label.html#pros-and-cons",
    "href": "categorical-label.html#pros-and-cons",
    "title": "20  Label Encoding",
    "section": "20.1 Pros and Cons",
    "text": "20.1 Pros and Cons\n\n20.1.1 Pros\n\nOnly produces a single numeric variable for each categorical variables\nHas a way to handle unseen levels, although poorly\n\n\n\n20.1.2 Cons\n\nOrdering of labels matter a lot!\nWill very often give inferior performance compared to other methods."
  },
  {
    "objectID": "categorical-label.html#r-examples",
    "href": "categorical-label.html#r-examples",
    "title": "20  Label Encoding",
    "section": "20.2 R Examples",
    "text": "20.2 R Examples\nWe will be using the ames data set for these examples. The step_dummy() function allow us to perform dummy encoding and one-hot encoding.\n\n\n\n\nlibrary(recipes)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nAttaching package: 'recipes'\n\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(modeldata)\ndata(\"ames\")\n\names |>\n  select(Sale_Price, MS_SubClass, MS_Zoning)\n\n# A tibble: 2,930 × 3\n   Sale_Price MS_SubClass                         MS_Zoning               \n        <int> <fct>                               <fct>                   \n 1     215000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 2     105000 One_Story_1946_and_Newer_All_Styles Residential_High_Density\n 3     172000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 4     244000 One_Story_1946_and_Newer_All_Styles Residential_Low_Density \n 5     189900 Two_Story_1946_and_Newer            Residential_Low_Density \n 6     195500 Two_Story_1946_and_Newer            Residential_Low_Density \n 7     213500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 8     191500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n 9     236500 One_Story_PUD_1946_and_Newer        Residential_Low_Density \n10     189000 Two_Story_1946_and_Newer            Residential_Low_Density \n# … with 2,920 more rows\n\n\nLooking at the levels of MS_SubClass we see that levels are set in a specific way. It isn’t alphabetical, but there isn’t one clear ordering. No clarification of the ordering can be doing in the data documentation http://jse.amstat.org/v19n3/decock/DataDocumentation.txt.\n\names |> pull(MS_SubClass) |> levels()\n\n [1] \"One_Story_1946_and_Newer_All_Styles\"      \n [2] \"One_Story_1945_and_Older\"                 \n [3] \"One_Story_with_Finished_Attic_All_Ages\"   \n [4] \"One_and_Half_Story_Unfinished_All_Ages\"   \n [5] \"One_and_Half_Story_Finished_All_Ages\"     \n [6] \"Two_Story_1946_and_Newer\"                 \n [7] \"Two_Story_1945_and_Older\"                 \n [8] \"Two_and_Half_Story_All_Ages\"              \n [9] \"Split_or_Multilevel\"                      \n[10] \"Split_Foyer\"                              \n[11] \"Duplex_All_Styles_and_Ages\"               \n[12] \"One_Story_PUD_1946_and_Newer\"             \n[13] \"One_and_Half_Story_PUD_All_Ages\"          \n[14] \"Two_Story_PUD_1946_and_Newer\"             \n[15] \"PUD_Multilevel_Split_Level_Foyer\"         \n[16] \"Two_Family_conversion_All_Styles_and_Ages\"\n\n\nWe will be using the step_integer() step for this, which defaults to 1-based indexing\n\ndummy_rec <- recipe(Sale_Price ~ ., data = ames) |>\n  step_integer(all_nominal_predictors()) |>\n  prep()\n\ndummy_rec |>\n  bake(new_data = NULL, starts_with(\"MS_SubClass\"), starts_with(\"MS_Zoning\"))\n\n# A tibble: 2,930 × 2\n   MS_SubClass MS_Zoning\n         <int>     <int>\n 1           1         3\n 2           1         2\n 3           1         3\n 4           1         3\n 5           6         3\n 6           6         3\n 7          12         3\n 8          12         3\n 9          12         3\n10           6         3\n# … with 2,920 more rows"
  },
  {
    "objectID": "categorical-label.html#python-examples",
    "href": "categorical-label.html#python-examples",
    "title": "20  Label Encoding",
    "section": "20.3 Python Examples",
    "text": "20.3 Python Examples"
  },
  {
    "objectID": "categorical-ordinal.html",
    "href": "categorical-ordinal.html",
    "title": "21  Ordinal Encoding",
    "section": "",
    "text": "This method is similar to Chapter 20, expect that we manually specify the mapping. Thus we can have (cold = 1, warm = 5, hot = 20)."
  },
  {
    "objectID": "categorical-ordinal.html#pros-and-cons",
    "href": "categorical-ordinal.html#pros-and-cons",
    "title": "21  Ordinal Encoding",
    "section": "21.1 Pros and Cons",
    "text": "21.1 Pros and Cons\n\n21.1.1 Pros\n\n\n21.1.2 Cons"
  },
  {
    "objectID": "categorical-ordinal.html#r-examples",
    "href": "categorical-ordinal.html#r-examples",
    "title": "21  Ordinal Encoding",
    "section": "21.2 R Examples",
    "text": "21.2 R Examples"
  },
  {
    "objectID": "categorical-ordinal.html#python-examples",
    "href": "categorical-ordinal.html#python-examples",
    "title": "21  Ordinal Encoding",
    "section": "21.3 Python Examples",
    "text": "21.3 Python Examples"
  },
  {
    "objectID": "categorical-binary.html#r-examples",
    "href": "categorical-binary.html#r-examples",
    "title": "22  Binary Encoding",
    "section": "22.2 R Examples",
    "text": "22.2 R Examples"
  },
  {
    "objectID": "categorical-binary.html#python-examples",
    "href": "categorical-binary.html#python-examples",
    "title": "22  Binary Encoding",
    "section": "22.3 Python Examples",
    "text": "22.3 Python Examples"
  },
  {
    "objectID": "categorical-frequency.html#r-examples",
    "href": "categorical-frequency.html#r-examples",
    "title": "23  Frequency Encoding",
    "section": "23.2 R Examples",
    "text": "23.2 R Examples"
  },
  {
    "objectID": "categorical-frequency.html#python-examples",
    "href": "categorical-frequency.html#python-examples",
    "title": "23  Frequency Encoding",
    "section": "23.3 Python Examples",
    "text": "23.3 Python Examples"
  },
  {
    "objectID": "categorical-helmert.html#r-examples",
    "href": "categorical-helmert.html#r-examples",
    "title": "24  Helmert Encoding",
    "section": "24.2 R Examples",
    "text": "24.2 R Examples"
  },
  {
    "objectID": "categorical-helmert.html#python-examples",
    "href": "categorical-helmert.html#python-examples",
    "title": "24  Helmert Encoding",
    "section": "24.3 Python Examples",
    "text": "24.3 Python Examples"
  },
  {
    "objectID": "categorical-difference.html#r-examples",
    "href": "categorical-difference.html#r-examples",
    "title": "25  Difference Encoding",
    "section": "25.2 R Examples",
    "text": "25.2 R Examples"
  },
  {
    "objectID": "categorical-difference.html#python-examples",
    "href": "categorical-difference.html#python-examples",
    "title": "25  Difference Encoding",
    "section": "25.3 Python Examples",
    "text": "25.3 Python Examples"
  },
  {
    "objectID": "categorical-target.html#r-examples",
    "href": "categorical-target.html#r-examples",
    "title": "26  Target Encoding",
    "section": "26.2 R Examples",
    "text": "26.2 R Examples"
  },
  {
    "objectID": "categorical-target.html#python-examples",
    "href": "categorical-target.html#python-examples",
    "title": "26  Target Encoding",
    "section": "26.3 Python Examples",
    "text": "26.3 Python Examples"
  },
  {
    "objectID": "categorical-impact.html#r-examples",
    "href": "categorical-impact.html#r-examples",
    "title": "27  Impact Encoding",
    "section": "27.2 R Examples",
    "text": "27.2 R Examples"
  },
  {
    "objectID": "categorical-impact.html#python-examples",
    "href": "categorical-impact.html#python-examples",
    "title": "27  Impact Encoding",
    "section": "27.3 Python Examples",
    "text": "27.3 Python Examples"
  },
  {
    "objectID": "categorical-polynomial.html#r-examples",
    "href": "categorical-polynomial.html#r-examples",
    "title": "28  Polynomial Encoding",
    "section": "28.2 R Examples",
    "text": "28.2 R Examples"
  },
  {
    "objectID": "categorical-polynomial.html#python-examples",
    "href": "categorical-polynomial.html#python-examples",
    "title": "28  Polynomial Encoding",
    "section": "28.3 Python Examples",
    "text": "28.3 Python Examples"
  },
  {
    "objectID": "categorical-sum.html#r-examples",
    "href": "categorical-sum.html#r-examples",
    "title": "29  Sum Encoding",
    "section": "29.2 R Examples",
    "text": "29.2 R Examples"
  },
  {
    "objectID": "categorical-sum.html#python-examples",
    "href": "categorical-sum.html#python-examples",
    "title": "29  Sum Encoding",
    "section": "29.3 Python Examples",
    "text": "29.3 Python Examples"
  },
  {
    "objectID": "categorical-hashing.html#r-examples",
    "href": "categorical-hashing.html#r-examples",
    "title": "30  Hashing Encoding",
    "section": "30.2 R Examples",
    "text": "30.2 R Examples"
  },
  {
    "objectID": "categorical-hashing.html#python-examples",
    "href": "categorical-hashing.html#python-examples",
    "title": "30  Hashing Encoding",
    "section": "30.3 Python Examples",
    "text": "30.3 Python Examples"
  },
  {
    "objectID": "categorical-leaveoneout.html#r-examples",
    "href": "categorical-leaveoneout.html#r-examples",
    "title": "31  Leave One Out Encoding",
    "section": "31.2 R Examples",
    "text": "31.2 R Examples"
  },
  {
    "objectID": "categorical-leaveoneout.html#python-examples",
    "href": "categorical-leaveoneout.html#python-examples",
    "title": "31  Leave One Out Encoding",
    "section": "31.3 Python Examples",
    "text": "31.3 Python Examples"
  },
  {
    "objectID": "categorical-leaf.html#r-examples",
    "href": "categorical-leaf.html#r-examples",
    "title": "32  Leaf Encoding",
    "section": "32.2 R Examples",
    "text": "32.2 R Examples"
  },
  {
    "objectID": "categorical-leaf.html#python-examples",
    "href": "categorical-leaf.html#python-examples",
    "title": "32  Leaf Encoding",
    "section": "32.3 Python Examples",
    "text": "32.3 Python Examples"
  },
  {
    "objectID": "categorical-glmm.html#r-examples",
    "href": "categorical-glmm.html#r-examples",
    "title": "33  GLMM Encoding",
    "section": "33.2 R Examples",
    "text": "33.2 R Examples"
  },
  {
    "objectID": "categorical-glmm.html#python-examples",
    "href": "categorical-glmm.html#python-examples",
    "title": "33  GLMM Encoding",
    "section": "33.3 Python Examples",
    "text": "33.3 Python Examples"
  },
  {
    "objectID": "categorical-randomforest.html#r-examples",
    "href": "categorical-randomforest.html#r-examples",
    "title": "34  Random Forest Encoding",
    "section": "34.2 R Examples",
    "text": "34.2 R Examples"
  },
  {
    "objectID": "categorical-randomforest.html#python-examples",
    "href": "categorical-randomforest.html#python-examples",
    "title": "34  Random Forest Encoding",
    "section": "34.3 Python Examples",
    "text": "34.3 Python Examples"
  },
  {
    "objectID": "categorical-catboost.html#r-examples",
    "href": "categorical-catboost.html#r-examples",
    "title": "35  Catboost Encoding",
    "section": "35.2 R Examples",
    "text": "35.2 R Examples"
  },
  {
    "objectID": "categorical-catboost.html#python-examples",
    "href": "categorical-catboost.html#python-examples",
    "title": "35  Catboost Encoding",
    "section": "35.3 Python Examples",
    "text": "35.3 Python Examples"
  },
  {
    "objectID": "categorical-woe.html#r-examples",
    "href": "categorical-woe.html#r-examples",
    "title": "36  Weight of Evidence Encoding",
    "section": "36.2 R Examples",
    "text": "36.2 R Examples"
  },
  {
    "objectID": "categorical-woe.html#python-examples",
    "href": "categorical-woe.html#python-examples",
    "title": "36  Weight of Evidence Encoding",
    "section": "36.3 Python Examples",
    "text": "36.3 Python Examples"
  },
  {
    "objectID": "categorical-jamesstein.html#r-examples",
    "href": "categorical-jamesstein.html#r-examples",
    "title": "37  James-Stein Encoding",
    "section": "37.2 R Examples",
    "text": "37.2 R Examples"
  },
  {
    "objectID": "categorical-jamesstein.html#python-examples",
    "href": "categorical-jamesstein.html#python-examples",
    "title": "37  James-Stein Encoding",
    "section": "37.3 Python Examples",
    "text": "37.3 Python Examples"
  },
  {
    "objectID": "categorical-mestimator.html#r-examples",
    "href": "categorical-mestimator.html#r-examples",
    "title": "38  M-Estimator Encoding",
    "section": "38.2 R Examples",
    "text": "38.2 R Examples"
  },
  {
    "objectID": "categorical-mestimator.html#python-examples",
    "href": "categorical-mestimator.html#python-examples",
    "title": "38  M-Estimator Encoding",
    "section": "38.3 Python Examples",
    "text": "38.3 Python Examples"
  },
  {
    "objectID": "categorical-thermometer.html#r-examples",
    "href": "categorical-thermometer.html#r-examples",
    "title": "39  Thermometer Encoding",
    "section": "39.2 R Examples",
    "text": "39.2 R Examples"
  },
  {
    "objectID": "categorical-thermometer.html#python-examples",
    "href": "categorical-thermometer.html#python-examples",
    "title": "39  Thermometer Encoding",
    "section": "39.3 Python Examples",
    "text": "39.3 Python Examples"
  },
  {
    "objectID": "categorical-quantile.html#r-examples",
    "href": "categorical-quantile.html#r-examples",
    "title": "40  Quantile Encoding",
    "section": "40.2 R Examples",
    "text": "40.2 R Examples"
  },
  {
    "objectID": "categorical-quantile.html#python-examples",
    "href": "categorical-quantile.html#python-examples",
    "title": "40  Quantile Encoding",
    "section": "40.3 Python Examples",
    "text": "40.3 Python Examples"
  },
  {
    "objectID": "categorical-summary.html#r-examples",
    "href": "categorical-summary.html#r-examples",
    "title": "41  Summary Encoding",
    "section": "41.2 R Examples",
    "text": "41.2 R Examples"
  },
  {
    "objectID": "categorical-summary.html#python-examples",
    "href": "categorical-summary.html#python-examples",
    "title": "41  Summary Encoding",
    "section": "41.3 Python Examples",
    "text": "41.3 Python Examples"
  },
  {
    "objectID": "categorical-multi-dummy.html#r-examples",
    "href": "categorical-multi-dummy.html#r-examples",
    "title": "42  Multi-Dummy encoding",
    "section": "42.2 R Examples",
    "text": "42.2 R Examples"
  },
  {
    "objectID": "categorical-multi-dummy.html#python-examples",
    "href": "categorical-multi-dummy.html#python-examples",
    "title": "42  Multi-Dummy encoding",
    "section": "42.3 Python Examples",
    "text": "42.3 Python Examples"
  },
  {
    "objectID": "categorical-collapse.html#r-examples",
    "href": "categorical-collapse.html#r-examples",
    "title": "43  Collapsing Categories",
    "section": "43.2 R Examples",
    "text": "43.2 R Examples"
  },
  {
    "objectID": "categorical-collapse.html#python-examples",
    "href": "categorical-collapse.html#python-examples",
    "title": "43  Collapsing Categories",
    "section": "43.3 Python Examples",
    "text": "43.3 Python Examples"
  },
  {
    "objectID": "categorical-combination.html#r-examples",
    "href": "categorical-combination.html#r-examples",
    "title": "44  Combination",
    "section": "44.2 R Examples",
    "text": "44.2 R Examples"
  },
  {
    "objectID": "categorical-combination.html#python-examples",
    "href": "categorical-combination.html#python-examples",
    "title": "44  Combination",
    "section": "44.3 Python Examples",
    "text": "44.3 Python Examples"
  },
  {
    "objectID": "imbalenced.html",
    "href": "imbalenced.html",
    "title": "45  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "imbalenced-smote.html",
    "href": "imbalenced-smote.html",
    "title": "46  SMOTE",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "missing.html",
    "href": "missing.html",
    "title": "47  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "missing-imputation.html",
    "href": "missing-imputation.html",
    "title": "48  Imputation",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many.html",
    "href": "too-many.html",
    "title": "49  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "too-many-pca.html",
    "href": "too-many-pca.html",
    "title": "50  Principal Component Analysis",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "outliers.html",
    "href": "outliers.html",
    "title": "51  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "outliers-remove.html",
    "href": "outliers-remove.html",
    "title": "52  Removal",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "datetime.html",
    "href": "datetime.html",
    "title": "53  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "datetime-holiday.html",
    "href": "datetime-holiday.html",
    "title": "54  Holidays",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "text.html",
    "href": "text.html",
    "title": "55  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "text-tfidf.html",
    "href": "text-tfidf.html",
    "title": "56  TF-IDF",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "image.html",
    "href": "image.html",
    "title": "57  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "image-stats.html",
    "href": "image-stats.html",
    "title": "58  Visual Features",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "circular.html",
    "href": "circular.html",
    "title": "59  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "circular-trig.html",
    "href": "circular-trig.html",
    "title": "60  Trigonometric",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "time-series.html",
    "href": "time-series.html",
    "title": "61  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "time-series-fourier.html",
    "href": "time-series-fourier.html",
    "title": "62  Fourier Features",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "correlated.html",
    "href": "correlated.html",
    "title": "63  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "correlated-filter.html",
    "href": "correlated-filter.html",
    "title": "64  High Correlation Filter",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "miscellaneous.html",
    "href": "miscellaneous.html",
    "title": "65  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "miscellaneous-zipcodes.html",
    "href": "miscellaneous-zipcodes.html",
    "title": "66  Zip Codes",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "video.html",
    "href": "video.html",
    "title": "67  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "video-tmp.html",
    "href": "video-tmp.html",
    "title": "68  Temporary",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "sound.html",
    "href": "sound.html",
    "title": "69  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "sound-tmp.html",
    "href": "sound-tmp.html",
    "title": "70  Temporary",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "spatial.html",
    "href": "spatial.html",
    "title": "71  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "spatial-gps.html",
    "href": "spatial-gps.html",
    "title": "72  GPS",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "geospatial.html",
    "href": "geospatial.html",
    "title": "73  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "geospatial-shapefiles.html",
    "href": "geospatial-shapefiles.html",
    "title": "74  Shapefiles",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "relational.html",
    "href": "relational.html",
    "title": "75  Overview",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "relational-tmp.html",
    "href": "relational-tmp.html",
    "title": "76  Temporary",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "order.html",
    "href": "order.html",
    "title": "77  Order of transformations",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "sparse.html",
    "href": "sparse.html",
    "title": "78  What should you do if you have sparse data?",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "79  How Different Models Deal With Input",
    "section": "",
    "text": "WIP"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "80  Summary",
    "section": "",
    "text": "WIP"
  }
]
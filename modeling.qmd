# Where does feature engineering fit into the modeling workflow? {#sec-modeling .unnumbered}

When we talk about the modeling workflow, it starts at the data source and ends with a fitted model. The fitted model in this instance should be created such that it can be used for the downstream task, be it inference or prediction. We want to make sure that the feature engineering methods we are applying are done correctly to avoid problems with the modeling. Things we especially want to avoid are **data leakage**, **overfitting**, and high computational cost.

::: {.callout-caution}
# TODO

Add diagram of modeling workflow from data source to model
:::

When applying feature engineering methods, we need to think about **trained** and **untrained** methods. Trained methods will perform a calculation doing the training of the method, and then using the extracted values to perform the transformation again. We see this in @sec-numeric-normalization, where we do centering. To do centering we subtract the mean value of the variable, calculated based on the training data set. Since this value needs to be calculated, it becomes a trained method. Examples of untrained methods are logarithmic transformation as seen in @sec-numeric-logarithms and datetime value extraction as seen in @sec-datetime-extraction. These methods are static in the sense the way they are performed doesn't need any parameters.

In practice, this means that untrained methods can be applied before the data-splitting procedure, as it would give the same results regardless of when it was done. Trained methods have to be performed after the data-splitting to ensure you don't have data leakage. The wrinkle to this is that untrained methods applied to variables that have already been transformed by a trained method will have to also be done after the data-splitting.

::: {.callout-caution}
# TODO

add a diagram for untrained/trained rule
:::

Some untrained methods have a high computational cost, such as BERT from @sec-text-bert. A general advice that errs on the side of safety is to do as much as you can after the data-splitting if you are unsure.

### Why do we use thresholds?

Oftentimes, when we use a method that selects something with a quantity, we end up doing it with a threshold instead of counting directly. The answer to this is purely practical, as it leaves less ambiguity. When selecting these features to keep in a feature selection routine @sec-too-many is a good example. It is easier to write the code that selects every feature that has more than X amount of variability. On the other hand, if we said "Give me the 25 most useful features", we might have 4 variables tied for 25th place. Now we have another problem. Does it keep all of them in, leaving 27 variables? If we do that then we violated our request of 25 variables. What if we select the first? then we arbitrarily give a bias towards variables early in the data set. What if we randomly select among the ties? then we introduce randomness into the method.

It is for the above reasons that many methods in feature engineering and machine learning use thresholds instead of precise numbers.

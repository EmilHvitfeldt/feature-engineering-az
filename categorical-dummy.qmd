---
pagetitle: "Feature Engineering A-Z | Dummy Encoding"
---

# Dummy Encoding {#sec-categorical-dummy}

::: {style="visibility: hidden; height: 0px;"}
## Dummy Encoding
:::

We have some categorical variables and we want to turn them into numerical values, one of the most common ways of going about it is to create **dummy variables**.
Dummy variables are variables that only take the values 0 and 1 to indicate the absence or presence of the levels in a categorical variable.
This is nicely shown with an example.

Considering this short categorical variable of animals, we observe there are 3 unique values "cat", "dog", and "horse".

```{r}
#| echo: false
c("dog", "cat", "horse", "dog", "cat")
```

With just this knowledge we can create the corresponding dummy variables.
There should be 3 columns one for each of the levels

```{r}
#| echo: false
dummy <- matrix(0L, nrow = 5, ncol = 3)
colnames(dummy) <- c("cat", "dog", "horse")
dummy[1, 2] <- 1L
dummy[2, 1] <- 1L
dummy[3, 3] <- 1L
dummy[4, 2] <- 1L
dummy[5, 1] <- 1L
dummy
```

From this, we have a couple of observations.
Firstly the length of each of these variables is equal to the length of the original categorical variable.
The number of columns corresponds to the number of levels.
Lastly, the sum of all the values on each row equals 1 since all the rows contain one 1 and the remaining 0s.
This means that for even a small number of levels, you get **sparse data**.
Sparse data is data where there are a lot of zeroes, meaning that it would take less space to store where the non-zero values are instead of all the values.
You can read more about how and when to care about sparse data in @sec-sparse.
What this means for dummy variable creation, is that depending on whether your software can handle sparse data, you might need to limit the number of levels in your categorical variables.
One way to do this would be to collapse levels together, which you can read about in @sec-categorical-collapse.

Dummy variable creation is a *trained* method.
This means that during the training step, the levels of each categorical variable are saved, and then these and only these values are used for dummy variable creation.
If we assumed that the above example data were used to train the preprocessor, and we passed in the values `["dog", "cat", "cat", "dog"]` during future applications, we would expect the following dummy variables

```{r}
#| echo: false
dummy <- matrix(0L, nrow = 4, ncol = 3)
colnames(dummy) <- c("cat", "dog", "horse")
dummy[1, 2] <- 1L
dummy[2, 1] <- 1L
dummy[3, 1] <- 1L
dummy[4, 2] <- 1L
dummy
```

the `horse` variable must be here too, even if it is empty as the subsequent preprocessing steps and model expect the horse variable to be present.
Likewise, you can run into problems if the value `"duck"` was used as the preprocessor wouldn't know what to do.
These cases are talked about in @sec-categorical-unseen.

## Dummy or one-hot encoding

::: callout-caution
# TODO

add diagram
:::

The terms **dummy encoding** and **one-hot encoding** get thrown around interchangeably, but they do have different and distinct meanings.
One-hot encoding is when you return `k` variables when you have `k` different levels.
Like we have shown above

```{r}
#| echo: false
dummy <- matrix(0L, nrow = 5, ncol = 3)
colnames(dummy) <- c("cat", "dog", "horse")
dummy[1, 2] <- 1L
dummy[2, 1] <- 1L
dummy[3, 3] <- 1L
dummy[4, 2] <- 1L
dummy[5, 1] <- 1L
dummy
```

Dummy encoding on the other hand returns `k-1` variables, where the excluded one typically is the first one.

```{r}
#| echo: false
dummy <- matrix(0L, nrow = 5, ncol = 2)
colnames(dummy) <- c("dog", "horse")
dummy[1, 1] <- 1L
dummy[3, 2] <- 1L
dummy[4, 1] <- 1L
dummy
```

These two encodings store the same information, even though the dummy encoding has 1 less column.
Because we can deduce which observations are `cat` by finding the rows with all zeros.
The main reason why one would use dummy variables is because of what some people call [the dummy variable trap](https://www.algosome.com/articles/dummy-variable-trap-regression.html).
When you use one-hot encoding, you are increasing the likelihood that you run into a collinearity problem.
With the above example, if you included an intercept in your model you have that `intercept = cat + dog + horse` which gives perfect collinearity and would cause some models to error as they aren't able to handle that.

::: callout-note
An **intercept** is a variable that takes the value 1 for all entries.
:::

Even if you don't include an intercept you could still run into collinearity.
Imagine that in addition to the `animal` variable also creates a one-hot encoding of the `home` variable taking the two values `"house"` and `"apartment"`, you would get the following indicator variables

```{r}
#| echo: false
animal <- matrix(0L, nrow = 5, ncol = 3)
colnames(animal) <- c("cat", "dog", "horse")
animal[1, 2] <- 1L
animal[2, 1] <- 1L
animal[3, 3] <- 1L
animal[4, 2] <- 1L
animal[5, 1] <- 1L

home <- matrix(0L, nrow = 5, ncol = 2)
colnames(home) <- c("house", "apartment")
home[1, 2] <- 1L
home[2, 1] <- 1L
home[3, 2] <- 1L
home[4, 2] <- 1L
home[5, 1] <- 1L

cbind(animal, home)
```

And in this case, we have that `house = cat + dog + horse - apartment` which again is an example of perfect collinearity.
Unless you have a reason to do otherwise I would suggest that you use dummy encoding in your models.
Additionally, this leads to slightly smaller models as each categorical variable produces 1 less variable.
It is worth noting that the choice between dummy encoding and one-hot encoding does matter for some models such as decision trees.
Depending on what types of rules they can use.
Being able to write `animal == "cat"` is easier then saying `animal != "dog" & animal != "horse"`.
This is unlikely to be an issue as many tree-based models can work with categorical variables directly without the need for encoding.

## Ordered factors

::: callout-caution
# TODO

finish section
:::

## Contrasts

::: callout-caution
# TODO

finish section
:::

## Pros and Cons

### Pros

-   Versatile and commonly used
-   Easy interpretation
-   Will rarely lead to a decrease in performance

### Cons

-   Does require fairly clean categorical levels
-   Can be quite memory intensive if you have many levels in your categorical variables and you are unable to use sparse representation
-   Provides a complete, but not necessarily compact set of variables

## R Examples

We will be using the `ames` data set for these examples.
The `step_dummy()` function allows us to perform dummy encoding and one-hot encoding.

```{r}
#| echo: false
set.seed(1234)
# To avoid changing recipe ID columns
```

```{r}
#| message: false
library(recipes)
library(modeldata)
data("ames")

ames |>
  select(Sale_Price, MS_SubClass, MS_Zoning)
```

We can take a quick look at the possible values `MS_SubClass` takes

```{r}
ames |>
  count(MS_SubClass, sort = TRUE)
```

And since `MS_SubClass` is a factor, we can verify that they match and that all the levels are observed

```{r}
ames |> pull(MS_SubClass) |> levels()
```

We will be using the `step_dummy()` step for this, which defaults to creating dummy variables

```{r}
dummy_rec <- recipe(Sale_Price ~ ., data = ames) |>
  step_dummy(all_nominal_predictors()) |>
  prep()

dummy_rec |>
  bake(new_data = NULL, starts_with("MS_SubClass"), starts_with("MS_Zoning")) |>
  glimpse()
```

We can pull the factor levels for each variable by using `tidy()`.
If a character vector was present in the data set, it would record the observed variables.

```{r}
dummy_rec |>
  tidy(1)
```

setting `one_hot = TRUE` gives us the complete one-hot encoding results.

```{r}
onehot_rec <- recipe(Sale_Price ~ ., data = ames) |>
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  prep()

onehot_rec |>
  bake(new_data = NULL, starts_with("MS_SubClass"), starts_with("MS_Zoning")) |>
  glimpse()
```

## Python Examples

```{python}
#| echo: false
import pandas as pd
from sklearn import set_config

set_config(transform_output="pandas")
pd.set_option('display.precision', 3)
```

We are using the `ames` data set for examples.
{sklearn} provided the `OneHotEncoder()` method we can use.
Below we see how it can be used with the `MS_Zoning` columns.

::: callout-note
We are setting `sparse_output=False` in this example because we are having `transform()` return pandas data frames for better printing.
:::

```{python}
from feazdata import ames
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

ct = ColumnTransformer(
    [('onehot', OneHotEncoder(sparse_output=False), ['MS_Zoning'])], 
    remainder="passthrough")

ct.fit(ames)
ct.transform(ames).filter(regex=("MS_Zoning.*"))
```

By default `OneHotEncoder()` performs one-hot encoding, we can change this to dummy encoding by setting `drop='first'`.

```{python}
ct = ColumnTransformer(
    [('dummy', OneHotEncoder(sparse_output=False, drop='first'), ['MS_Zoning'])], 
    remainder="passthrough")

ct.fit(ames)
ct.transform(ames).filter(regex=("MS_Zoning.*"))
```
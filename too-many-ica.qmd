---
pagetitle: "Feature Engineering A-Z | Independent Component Analysis"
---

# Independent Component Analysis {#sec-too-many-ica}

::: {style="visibility: hidden; height: 0px;"}
## Independent Component Analysis
:::

Independent Component Analysis (ICA) is a method quite similar to [Principal Component Analysis](too-many-pca.qmd).
PCA aims to create a transformation that maximizes the variance of the resulting variables, 
while making them uncorrelated.
ICA, on the other hand, aims to create variables that are statistically independent.
Note that the ICA components are not assumed to be uncorrelated or orthogonal.

This allows ICA to pull out stronger signals in your data.
It also doesn't assume that the data is Gaussian.

One way to think about the difference between PCA and ICA,
PCA can be used more effectively as a data compression technique,
On the other hand, ICA helps uncover and separate the structure in the data itself.

The notion that ICA is a dimensionality reduction method is because the implementation of fastICA, which is commonly used, works incrementally.

ICA, much like PCA, requires that your data be normalized before it is applied.

Below is an example of the principle components in actions.
We took the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) and performed ICA on the pixel values as predictors.
First We apply it to the entire data set.

```{r}
#| label: ica-mnist
#| echo: false
#| fig-cap: |
#|   ICA applied to all of MNIST
#| fig-alt: |
#|   Faceted tile chart. Each chart corresponds to a different
#|   component. Early components show some shapes that might suggest numbers
#|   such as 6 or 7, later ones show less and less identifiable patterns.
library(keras)

mnist <- keras::dataset_mnist()

set.seed(1234)
mnist$train$x |> dim()

out <- list()

for (x in 1:28) {
  for (y in 1:28) {
    out[[paste0(x, ",", y)]] <- mnist$train$x[, y, x]
  }
}

library(tidymodels)

mnist_tbl <- bind_cols(out)

rec <- recipe(~., data = mnist_tbl[1:1000, ]) |>
  step_ica(all_predictors()) |>
  prep()

rec |>
  tidy(1) |>
  filter(component %in% paste0("IC", 1:4)) |>
  separate(terms, c("x", "y"), ",") |>
  mutate(x = factor(x, 1:28)) |>
  mutate(y = factor(y, 28:1)) |>
  ggplot(aes(x, y, fill = value)) +
  geom_raster() +
  facet_wrap(~component) +
  scale_fill_gradient2() +
  theme(axis.ticks = element_blank(), axis.text = element_blank()) +
  labs(x = NULL, y = NULL, fill = NULL)
```

We clearly see some effects here.
Remember that it isn't important whether something is positive or negative,
just that something is different than something else.
there isn't super strong signals,
but it appears that we are capturing 7-ness in the first IC and 6-ness in the third IC.
We notice that each IC appears 

## Pros and Cons

### Pros

- Can identify stronger signals

### Cons

- Sensitive to noise and outliers
- Computationally intensive

## R Examples

```{r}
#| label: ames
#| echo: false
#| message: false
library(tidymodels)
data("ames")
```

We will be using the `ames` data set for these examples.

```{r}
#| label: show-data
library(recipes)
library(modeldata)

ames_num <- ames |>
  select(where(is.numeric))
```

{recipes} provides `step_ica()`, which is the standard way to perform ICA.

```{r}
#| label: step_ica
pca_rec <- recipe(~ ., data = ames_num) |>
  step_normalize(all_numeric_predictors()) |>
  step_ica(all_numeric_predictors())

pca_rec |>
  prep() |>
  bake(new_data = NULL) |>
  glimpse()
```

## Python Examples

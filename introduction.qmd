# Introduction {.unnumbered}

It is commonly said that feature engineering, much like machine learning, is an art rather than a science. This wants to reemphasize this point. But just because it is an art doesn't mean we can't thoroughly explain the tools and techniques. This is the main goal of this book. Giving you the knowledge of the different techniques you are likely to see and work with, and enough of a base, that any future methods won't be too intimidating.

## Where does feature engineering fit into the modeling workflow? {#sec-modeling .unnumbered}

When we talk about the modeling workflow, it starts at the data source and ends with a fitted model. The fitted model in this instance should be created such that it can be used for the downstream task, be it inference or prediction. We want to make sure that the feature engineering methods we are applying are done correctly to avoid problems with the modeling. Things we especially want to avoid are **data leakage**, **overfitting**, and high computational cost.

::: {.callout-caution}
# TODO

Add diagram of modeling workflow from data source to model
:::

When applying feature engineering methods, we need to think about **trained** and **untrained** methods. Trained methods will perform a calculation doing the training of the method, and then using the extracted values to perform the transformation again. We see this in @sec-numeric-normalization, where we do centering. To do centering we subtract the mean value of the variable, calculated based on the training data set. Since this value needs to be calculated, it becomes a trained method. Examples of untrained methods are logarithmic transformation as seen in @sec-numeric-logarithms and datetime value extraction as seen in @sec-datetime-extraction. These methods are static in the sense the way they are performed doesn't need any parameters.

In practice, this means that untrained methods can be applied before the data-splitting procedure, as it would give the same results regardless of when it was done. Trained methods have to be performed after the data-splitting to ensure you don't have data leakage. The wrinkle to this is that untrained methods applied to variables that have already been transformed by a trained method will have to also be done after the data-splitting.

::: {.callout-caution}
# TODO

add a diagram for untrained/trained rule
:::

Some untrained methods have a high computational cost, such as BERT from @sec-text-bert. A general advice that errs on the side of safety is to do as much as you can after the data-splitting if you are unsure.

### Why do we use thresholds?

Oftentimes, when we use a method that selects something with a quantity, we end up doing it with a threshold instead of counting directly. The answer to this is purely practical, as it leaves less ambiguity. When selecting these features to keep in a feature selection routine @sec-too-many is a good example. It is easier to write the code that selects every feature that has more than X amount of variability. On the other hand, if we said "Give me the 25 most useful features", we might have 4 variables tied for 25th place. Now we have another problem. Does it keep all of them in, leaving 27 variables? If we do that then we violated our request of 25 variables. What if we select the first? then we arbitrarily give a bias towards variables early in the data set. What if we randomly select among the ties? then we introduce randomness into the method.

It is for the above reasons that many methods in feature engineering and machine learning use thresholds instead of precise numbers.

## How to Deal With ... {.unnumbered}

This book is structured according to the types of data and problems you will encounter. Each section specifies a type of data or problem, and each chapter details a method or group of methods that can be useful in dealing with that type. So for example @sec-numeric contains methods that deal with numeric variables such as @sec-numeric-logarithms and @sec-numeric-maxabs, and @sec-categorical contains methods that deal with categorical variables such as @sec-categorical-dummy and @sec-categorical-hashing. There should be sections and chapters for most methods you will find in practice that aren't too domain-specific.

It is because of this structure that this book is most suited as reference material, each time you encounter some data you are unsure how to deal with, you find the corresponding section and study the methods listed to see which would be best for your use case. This isn't to say that you can't read this book from end to end. The sections have been ordered roughly such that earlier chapters are broadly useful and later chapters touch on less used data types and problems.

## Terminology {#sec-terminology}

Below are some terms we use throughout the book, that we want to make sure are clear. Some of these might differ from other books, and that is fine. This is why we have this section. Some of the methods described in this book are known under multiple names. When that is the case, it will be listed at the beginning of the chapter. The index will likewise point you to the right chapter regardless of which name you use.

### Observations

This book will mostly be working with rectangular data. In this context, each *observation* is defined as a row, with the columns holding the specific characteristics for each observation.

The observational unit can change depending on the data. If we were looking at a data set of restaurant health code inspections, you are likely to see the data with one row per inspection. However, depending on your problem statement or hypothesis, you might want to think of each restaurant as an observation. If you are thinking from a planning perspective you could think of each day/week as an observation.

Reading this book will not tell you how to think of your data as we don't know what you are trying to do. Once you have your data in the right format and order, we can show you what is possible.

### Learned

Some methods require information to be transformed, that we are not able to supply beforehand. In the case of centering of numeric variables described in @sec-numeric-normalization. To be able to do this transformation, you need to know the mean value of the training data set. This means is the sufficient information needed to perform the calculations and is the reason why the method is a *learned* method.

On the other hand, taking the square root of a variable as described in @sec-numeric-sqrt isn't a learned method as there isn't any sufficient information needed. The method can be applied right away.

### Supervised / Unsupervised

Some methods use the outcome to guide the calculations. If the outcome is used, the method is said to be *supervised*. Most methods are unsupervised.

### Levels

Variables that contain non-numeric information are typically called qualitative or categorical variables. This can be things such as eye color, street names, names, grades, car models and subscription types. Where there is a finite known set of values a categorical variable can take, we call these values the *levels* of that variable. So the levels of the variables containing weekdays are "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", and "Sunday". But the names of our subscribers don't have levels as we don't know all of them.

We will sometimes bend this definition, as it is sometimes useful to pretend that a variable has a finite known set of values, even if it doesn't.

### Linear models

We talk about *linear models* as models that are specified as a linear combination of features. These models tend to be simple, and fast to use, but having the limitation of "linear combination of features" means that struggle if the data has non-linear effects.

### Embedding

The word **embedding** is thrown around a lot in machine learning and artificial intelligence. but in essence, it is a simple concept. An embedding is what happens when each observation is transformed into a numerical representation. We see this often in text embeddings, where a free-from-text field is turned into a fixed-length numerical vector.

Something being an embedding doesn't mean that it is useful. But with care, and signal, useful representations of the data can be created. The reason why we have embeddings in the first place is that most machine learning models require numerical features for the models to work.

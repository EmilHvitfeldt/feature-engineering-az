# Collapsing Categories {#sec-categorical-collapse}

There are times, especially when you have a lot of levels in a categorical variable, that it will be beneficial for you to combine some of them. This practice is in some ways similar to what we saw in @sec-categorical-cleaning, but here we are doing it for performance reasons.

Essentially what we are working on, is trying to combine many levels to get higher performance and interpretability

this has two prongs

- there aren't enough observations here, let us combine them
- these levels are similar
    - expert knowledge
    - inferred from data

The first issue can be quite a common one. See the below distribution as an example

```{r}
library(dplyr)
library(ggplot2)
library(modeldata)

ames |>
  count(Exterior_2nd) |>
  mutate(Exterior_2nd = forcats::fct_reorder(Exterior_2nd, n)) |>
  ggplot(aes(n, Exterior_2nd)) +
  geom_col() +
  theme_minimal() +
  labs(
    title = "Exterior covering on house",
    x = NULL,
    y = NULL
  )
```

The proportion of how often each level appears is quite stark, to the point where 4 of them happen less than 10 times, which is not a lot considering the most frequent level occurs over 1000 times.

For some methods such as @sec-categorical-dummy, having these infrequent levels would not do us much good, and may even make things worse. Having a level be so infrequent increases its likelihood of being uninformative. This is where collapsing can come into play. The method takes the most infrequent levels and combines them into one, typically called `"other"`.

```{r}
ames |>
  mutate(Exterior_2nd = forcats::fct_lump_prop(Exterior_2nd, 0.025)) |>
  count(Exterior_2nd) |>
  mutate(Exterior_2nd = forcats::fct_reorder(Exterior_2nd, n)) |>
  ggplot(aes(n, Exterior_2nd)) +
  geom_col() +
  theme_minimal() +
  labs(
    title = "Exterior covering on house",
    x = NULL,
    y = NULL
  )
```

Above we see how that is done. We took all the levels that appeared less than `2.5%` of the time and combined them into a new level called `"other"`. This value threshold will off cause depend on many things and is a good candidate for tuning. And we don't have to do it as a percentage, we might as well do it based on counts. Collapsing anything with less than 10 occurrences.

This method can give pretty good results. But is by nature very crude. We are more than likely to combine levels that have nothing to do with each other than their low frequency. This will sometimes be inefficient, and while it has straightforward explainability due to its simple nature it can be hard to argue for its approach to shareholders.

This is where the other type of collapsing comes in. These methods use a little more information about the data, in the hopes that the collapsing will be more sensible. We will describe two of them here. First is the model-based approach. You can image that we fit a small model, such as a decision tree on the categorical variable, using a sensible outcome. This outcome could the the real outcome of out larger modeling problem. then we let the decision tree run, and the way the tree splits the data is how we combine the levels.

This is done below as an example:

```{r}
library(embed)

res <- recipe(Sale_Price ~ Exterior_2nd, data = ames) |>
  step_collapse_cart(Exterior_2nd, outcome = vars(Sale_Price)) |>
  prep() |>
  tidy(1) 

split(res$old, res$new)
```

And it isn't super hard to see that these make sense. "Asbestos Shingles" and "Asphalt Shingles" got paired, as did "Metal Siding" and "Wood Siding", and "Hard Board" and "Plywood". 




TODO: talk about string distance other collapse_steps

TODO: talk about dirtycat

## Pros and Cons

### Pros

### Cons

## R Examples

## Python Examples

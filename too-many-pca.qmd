---
pagetitle: "Feature Engineering A-Z | Principal Component Analysis"
---

# Principal Component Analysis {#sec-too-many-pca}

::: {style="visibility: hidden; height: 0px;"}
## Principal Component Analysis
:::

The first dimensionality reduction method most people are introduced to is Principal Component Analysis (PCA).
There is a good reason for this,
it is a dependable method that has stood the test of time.
This chapter will cover what PCA is in its main form,
with its varients described in next chapter on [PCA variants](too-many-pca-variants).

The reason why we want to apply PCA in the first place,
is because we expect that the information in the predictors could be more efficiently stored.
Or in other words, 
because there is redundency in the data.
This can also be seen in part using a correlation plot.
Using the numeric variables of the `ames` data set, we can plot the correlation of it.

```{r}
#| label: ames-correlation-plot
#| echo: false
#| message: false
#| fig-cap: |
#|   Many of the variables are positively correlated with each other, which makes
#|   sense since bigger houses leads to bigger square footage and room counts.
#| fig-alt: |
#|   Correlation chart. Using a grid based visualization, color is used to
#|   indicate strength of correlations with color intensity and direction 
#|   two different colors, one for positive correlation and one for negative.
#|   the chart can visually be split up into 2 sections. One section has columns
#|   that are significantly positive correlated, these have something to do
#|   with square footage or room counts. The other section of variables doens't
#|   have any noteworthy correlations.
library(tidymodels)
ames |>
  select(where(is.numeric)) |>
  corrr::correlate() |>
  autoplot(method = "HC")
```

What we see is that many of the variables are corelated with eachother. 
This is expected since larger houses is more likely to have more bed rooms, larger basements, more fireplaces, etc etc.
But it begs the question: are we efficiently storing this information?
It Many of the variables store some varient of "large house" with something else mixed in.

We will motivate what PCA does using the following simulated 2-dimensional data set.

```{r}
#| label: pca-example-1
#| echo: false
#| fig-alt: |
#|   Scatter chart. Predictor 1 along the x-axis, predictor 2 along theh y-axis. #|   All the points follow the diagonal with little variation.
set.seed(1234)
whale <- tibble(
  x1 = rnorm(250)
) |>
  mutate(
    x2 = x1 * 0.5 + rnorm(250) / 10
  ) 
  
whale |>
  ggplot(aes(x1, x2)) +
  geom_point() +
  coord_fixed() +
  theme_minimal() +
  labs(
    x = "Predictor 1",
    y = "Predictor 2"
  )
```

There is a clear correlation between these two variables. 
One way of thinking about what PCA does is that it rotates the data set,
in order to maximize the variance along one dimension.
visually we can find that line as seen below.

```{r}
#| label: pca-example-2
#| echo: false
#| fig-alt: |
#|   Scatter chart. Predictor 1 along the x-axis, predictor 2 along theh y-axis. #|   All the points follow the diagonal with little variation. A line is overlaid
#|   along the points.
rot <- prcomp(whale, scale. = FALSE, center = FALSE)$rotation
whale |>
  ggplot(aes(x1, x2)) +
  geom_point() +
  coord_fixed() +
  geom_abline(slope = rot["x2", "PC1"], intercept = 0, color = "#854ac8") +
  lims(
    x = c(-4, 4),
    y = c(-2, 2)
  ) +
  theme_minimal() +
  labs(
    x = "Predictor 1",
    y = "Predictor 2"
  )
```

Rotating the data gives us the following data set after applying PCA.

```{r}
#| label: pca-example-3
#| echo: false
#| fig-alt: |
#|   Scatter chart. Predictor 1 along the x-axis, predictor 2 along theh y-axis. #|   All the points follow the diagonal with little variation. This data is the
#|   same as the data seen in the previous charts except rotation such that
#|   the variance is highest along the x-axis.
whale |>
  recipe(~.) |>
  step_pca(all_predictors()) |>
  prep() |>
  bake(NULL) |>
  ggplot(aes(PC1, PC2)) +
  geom_point() +
  coord_fixed() +
  theme_minimal() +
  lims(
    x = c(-4, 4),
    y = c(-2, 2)
  ) +
  labs(
    x = "Predictor 1",
    y = "Predictor 2"
  )
```

In this adjusted data set,
we would be able to drop the second predictor as the first contains most of the variance in the data set.

We can extend this method to higher dimensional data sets as well.
The first principle component was found by finding the hyperplane that maximized the variance along it,
finding the second principle component is found by calculating the hyperplace that maximized the variance of any planes that are othogonal to the first principle component.
The third is found by finding one that is othogonal to the first and second.
This is repeated until you have a principle component for each variable in the original data set.

In math terms this amounts to finding $\phi_{ij}$ that satisfy

$$
\sum_{j=1}^p  \phi_{ji}^2 = 1
$$

Where $\phi_{ij}$ indicates the value of the loading for the ith input predictor and jth outcome predictor.
We can write out the full list of equations like so. 

$$
\begin{align} 
&Z_1 = \phi_{11} X_1 + \phi_{21} X_2 + ... + \phi_{p1}X_p \\
&Z_2 = \phi_{12} X_1 + \phi_{22} X_2 + ... + \phi_{p2}X_p \\
&... \\
&Z_p = \phi_{1p} X_1 + \phi_{2p} X_2 + ... + \phi_{pp}X_p
\end{align}
$$

Luckily this is this can be found using linear algebra operations, hence why this opperation has the computational speed it has.

The reason why this method works as a feature engineering method,
is that the loadings $\phi_{ij}$ that are estimated can be saved and applied to new data.

The astute reader will notice that the above description technically doesn't reduce dimentions, 
it merely rotates the data in a way that maximizes variance for each variable,
under the constraint that they are all othogonal.
It becomes a dimensionality reduction method by selecting only a certain number of principle compoments.
We are hoping that we can place a cutoff between that selects enough useful components without removing too much.

One downside to PCA is that we are not able to completely stop the loss of signal,
but it is used as we can often vastly decrease the number of columns in our data set.
It is a trade-off between computational speed and performance.
And can be hyperparameter tuned.

In the below chart we have calculated the **cumulative percent variance** for 4 different data sets being processed by PCA.

```{r}
#| label: pca-cumulative-percent-variance
#| echo: false
#| fig-cap: |
#|   4 difference shapes of cumulative percent variance plots.
#| fig-alt: |
#|   Faceted bar chart chart. 4 bar charts in a 2 by 2 grid. Each one represents
#|   a different data set which has had PCA applied to it. 
set.seed(1234)

data_cumpervar <- function(data) {
data |>
  select(where(is.numeric)) |>
  recipe(~.) |>
  step_normalize(all_predictors()) |>
  step_pca(all_predictors()) |>
  prep() |> 
  tidy(2, type = "variance") |>
  filter(terms == "cumulative percent variance")
}
library(scales)

bind_rows(
data_cumpervar(mtcars),
data_cumpervar(ames),
data_cumpervar(Chicago),
data_cumpervar(concrete)
) |>
  ggplot(aes(component, value)) +
  geom_col() +
  facet_wrap(~id, scales = "free_x") +
  labs(
    y = "cumulative percent variance"
  ) +
  theme_minimal() +
  scale_x_continuous(breaks = c(5, seq(0, 100, by = 10)))
```

These charts show how different datasets react differently by having PCA applied.
Remember that we only want to select the top N components,
such that it acts like a dimensionality reduction method.
Both of the left charts shows a fast rise in the cumulative variance,
with a fairly early plateu.
Making a cutoff at the plateu is likely a good choice.
The lower left data set captures 99% of the variance with 27 components,
which amounts 50% decrease in predictors.
On the other hand we have the bottom left data set which achives the same 99& variance with the first 31 components,
which only amounts to 10% decrease in predictors.
A 99% threshold is quite high,
and should be tuned for performance and speed.

It is a common misconception that PCA is a computationally intensive for large data sets,
that is only the case if you apply PCA to generate all the components.
If you instead run your method such that is captures 90% of the variance,
or only the first 10 compoments then your method should only calculate those and stop.
Remember that this method is iterative over the components,
so for optimal computational speed you just need to make sure the settings are correct before running. 

A benefit to using PCA is that the resulting predictors are uncorrelated by definition.
For performance reasons this makes PCA a prime candidate for methods that have a hard time dealing with multicollinearity.
Linear regression being one such model.
Principal Component Regression (PCR) is a term for such a scenario.
It is a model that applies PCA to the predictors before passing them to a linear regression model.
However we don't treat PCR as its own method,
instead thinking of it as a workflow/pipeline with PCA as a preprocessor and linear regression as the model.

Another common misconception is that PCA requires your predictors to be normally distributed.
PCA operates over the correlation matrix which can be done with normality.
That doesn't mean that normally is bad,
just that it isn't required here.

What might have been the cause of this misconception is the fact that PCA is sensitive to scaling (normalization).
This should make sense intuitively since we are optimizing over variances.
If one predictor is age and another is yearly income,
then the first PC would overwhelming be yearly income almost exclusively because of the scale involved.
It is therefore highly recommended to apply any type of [normalization](sec-numeric-normalization).
You should further verify whether the PCA implementation you are using have a bundled normalization preprocessor.

There are a number of limitations,
with the first and foremost being that PCA is only able to extract effects that come from linear combinations of predictors,
in other words,
it won't be able to detect non-linear relationships.
The algorithms used will be determanistic,
however they will only give identical results up to a sign flip for each variable.
This should not be a big issue but it can be surpricing at first if you do not expect it to happen.
PCA  Makes for harder, but not impossible interprebility. 
The [PCA variants](too-many-pca-variants) try to deal with this issue.
This is esspecially true since the base case combines EVERY predictor in the input, 
to produce EVERY predictor in the output.
Another limitation is that PCA doesn't have a native way of dealing with missing data.
However that doens't mean you can't use PCA with data with missing values,
just that you have to be aware of what will happen if you try.
Since PCA is calculated from a correlation matrix,
missing values could simply be excluded during the construction of the correlation matrix.
This will be imperfect but is sometimes done instead of dealing with the missing values first.

Below is an example of the principle compoments in actions.
We took the [MNIST database](https://en.wikipedia.org/wiki/MNIST_database) and performed PCA on the pixel values as predictors.
First We apply it to the entire data set.

```{r}
#| label: pca-mnist
#| echo: false
#| fig-cap: |
#|   PCA aplied to all of MNIST
#| fig-alt: |
#|   Faceted tile chart. Each chart corresponds to a different principal
#|   component. Early components show some shapes that might suggest numbers
#|   such as 3 or 8, later ones show less and less identifiable patterns.
library(keras)

mnist <- keras::dataset_mnist()

set.seed(1234)
mnist$train$x |> dim()

out <- list()

for (x in 1:28) {
  for (y in 1:28) {
    out[[paste0(x, ",", y)]] <- mnist$train$x[, y, x]
  }
}

library(tidymodels)

mnist_tbl <- bind_cols(out)

rec <- recipe(~., data = mnist_tbl[1:1000, ]) |>
  step_pca(all_predictors()) |>
  prep()

rec |>
  tidy(1) |>
  filter(component %in% paste0("PC", 1:4)) |>
  separate(terms, c("x", "y"), ",") |>
  mutate(x = factor(x, 1:28)) |>
  mutate(y = factor(y, 28:1)) |>
  ggplot(aes(x, y, fill = value)) +
  geom_raster() +
  facet_wrap(~component) +
  scale_fill_gradient2() +
  theme(axis.ticks = element_blank(), axis.text = element_blank()) +
  labs(x = NULL, y = NULL, fill = NULL)
```

We clearly see some effects here.
Remember that it isn't important whether something is positive or negative,
just that something is different than something else.
the first PC more or less detectif something is in the middle of the image,
and maybe a faint outline of an 8 or 3.
The second PC more or less looks for a diagonal like as we see in 2s and 7s.
The third PC becomes harder to decipher, but it looks a little like 4s and 9s.
As we go up in PCs it gets harder and harder to find the signal,
which is fine as PCA here isn't trying to make a representation of the data,
it is simply finding the combination of values (pixels in this example) that leads to the highest variance.

If we instead did what we did before but only included data for the number 2 we will see the following principle components.

```{r}
#| label: pca-mnist-2
#| echo: false
#| fig-cap: |
#|   PCA aplied to all 2s in MNIST
#| fig-alt: |
#|   Faceted tile chart. Each chart corresponds to a different principal
#|   component. Each component clearly depicts 2s, with the first one
#|   capturing a normal 2, with each other one showing a shifted or distortion
#|   of a 2.
mnist_tbl$outcome <- mnist$train$y

rec <- recipe(outcome ~., data = mnist_tbl |> filter(outcome == 2)) |>
  step_pca(all_predictors()) |>
  prep()

rec |>
  tidy(1) |>
  filter(component %in% paste0("PC", 1:4)) |>
  separate(terms, c("x", "y"), ",") |>
  mutate(x = factor(x, 1:28)) |>
  mutate(y = factor(y, 28:1)) |>
  ggplot(aes(x, y, fill = value)) +
  geom_raster() +
  facet_wrap(~component) +
  scale_fill_gradient2() +
  theme(axis.ticks = element_blank(), axis.text = element_blank()) +
  labs(x = NULL, y = NULL, fill = NULL)
```

Here we can see that the PCs very much resemple what we would expect a 2 to look like.
And you would honest get a pretty good performance if you just used the first PC as an input into a classifier.

## Pros and Cons

### Pros

### Cons

## R Examples

## Python Examples
